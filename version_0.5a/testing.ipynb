{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e0f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Article:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Summary:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                prompt, _ = temp_prompt.split(end, 1)\n",
    "                prompt = prompt.strip()\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(dataset_name, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(dataset_name, data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "663414e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def combine_data_files(data_files):\n",
    "    \"\"\"\n",
    "    This function reads all the files from the given list of file paths and\n",
    "    combines them into a large DataFrame.\n",
    "\n",
    "    Args:\n",
    "    data_files (list of str): The list of file paths.\n",
    "\n",
    "    Returns:\n",
    "    combined_data (DataFrame): A DataFrame combining all the data from the files.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    data_frames = []\n",
    "\n",
    "    for file in data_files:\n",
    "        # Check if the file exists\n",
    "        if os.path.isfile(file):\n",
    "            # Load the file into a DataFrame and append it to the list\n",
    "            data_frames.append(pd.read_csv(file))\n",
    "        else:\n",
    "            print(f\"The file '{file}' does not exist.\")\n",
    "\n",
    "    # Concatenate all the DataFrames in the list\n",
    "    combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_for_regression(data_file):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Convert the DataFrame to a list of tuples\n",
    "    data = list(data.itertuples(index=False, name=None))\n",
    "\n",
    "    # Initialize lists to store features and labels\n",
    "    feature_list = []\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Remove prefixes\n",
    "    texts, labels = remove_prefix(data)\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    for (prompt, text), label in zip(prompts_and_texts, labels):\n",
    "        # Count POS tags in the text\n",
    "        pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "        # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "        flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "        # Calculate the perplexity of the text and average sentence perplexity\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "        text = tokenizer.decode(text_encoded)\n",
    "        text = text.replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "        text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "        sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in nlp(text).sents]\n",
    "        sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "        avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "            sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "        # Calculate the frequency of uppercase letters\n",
    "        uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "        # Calculate the cosine similarity for the prompt and text\n",
    "        prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "        # Calculate the average cosine similarity for sentences in the text\n",
    "        sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer)\n",
    "        avg_sentence_cosine_similarity = None\n",
    "        if sentence_cosine_similarities:\n",
    "            avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(sentence_cosine_similarities)\n",
    "        else:\n",
    "            print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "        # Prepare a dictionary to append to the feature list\n",
    "        features = {\n",
    "            'ADJ': pos_counts.get('ADJ', 0),\n",
    "            'ADV': pos_counts.get('ADV', 0),\n",
    "            'CONJ': pos_counts.get('CONJ', 0),\n",
    "            'NOUN': pos_counts.get('NOUN', 0),\n",
    "            'NUM': pos_counts.get('NUM', 0),\n",
    "            'VERB': pos_counts.get('VERB', 0),\n",
    "            'COMMA': punctuation_counts.get(',', 0),\n",
    "            'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "            'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "            'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "            'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "            'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "            'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "            'uppercase_freq': uppercase_freq,  # new feature\n",
    "            'flesch_reading_ease': flesch_reading_ease,\n",
    "            'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'text_perplexity': text_perplexity,\n",
    "            'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "            'prompt_text_cosine_similarity': prompt_text_cosine_similarity,  # new feature\n",
    "            'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,  # new feature\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        # Add the feature dictionary to the feature list\n",
    "        feature_list.append(features)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    data_matrix = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists('data_matrix.csv'):\n",
    "        overwrite = input('File data_matrix.csv already exists. Do you want to overwrite it? (y/n): ')\n",
    "        if overwrite.lower() == 'y':\n",
    "            data_matrix.to_csv('data_matrix.csv', index=False)\n",
    "    else:\n",
    "        data_matrix.to_csv('data_matrix.csv', index=False)\n",
    "\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71b34def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No sentence cosine similarities calculated for text: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n"
     ]
    }
   ],
   "source": [
    "data_matrix = prepare_data_for_regression('full_data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9516ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ADJ  ADV  CONJ  NOUN  NUM  VERB  COMMA  FULLSTOP  SPECIAL-  FUNCTION-A  \\\n",
      "0   11    3     0    24    0    10      5         4         0           3   \n",
      "1    6    2     0     7    0     6      2         2         0           1   \n",
      "2    3    0     0     3    0     0      0         1         0           1   \n",
      "3   12    3     0    18    1     7      1         3         2           1   \n",
      "4   12    5     0    34    0    14      8         6         1           0   \n",
      "\n",
      "   ...  uppercase_freq  flesch_reading_ease  flesch_kincaid_grade_level  \\\n",
      "0  ...        0.029173                38.35                        13.9   \n",
      "1  ...        0.025974                52.19                        10.7   \n",
      "2  ...        0.013514                68.77                         6.4   \n",
      "3  ...        0.023641                42.00                        12.5   \n",
      "4  ...        0.013959                28.33                        13.7   \n",
      "\n",
      "   avg_word_length  avg_sentence_length  text_perplexity  \\\n",
      "0         5.278351            26.500000         1.019808   \n",
      "1         5.000000            21.000000         1.084632   \n",
      "2         5.545455            14.000000         1.021262   \n",
      "3         5.415385            25.000000         1.004532   \n",
      "4         6.224299            20.666667         1.083283   \n",
      "\n",
      "   avg_sentence_perplexity  prompt_text_cosine_similarity  \\\n",
      "0                 1.150377                       0.970656   \n",
      "1                 1.053928                       0.971264   \n",
      "2                 1.021262                       0.986438   \n",
      "3                 1.010272                       0.974291   \n",
      "4                 1.078212                       0.965239   \n",
      "\n",
      "   avg_sentence_cosine_similarity  label  \n",
      "0                        0.979159      0  \n",
      "1                        0.977223      0  \n",
      "2                        0.000000      0  \n",
      "3                        0.971269      0  \n",
      "4                        0.976470      0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273577d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
