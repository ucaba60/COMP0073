{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b0cbfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from sklearn import metrics\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2914f13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c95a8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        loss = outputs.loss\n",
    "    return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dac26489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed():\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split='train')\n",
    "\n",
    "    # combine question and long_answer and add label, and create a spaCy Doc for each text\n",
    "    data = [(nlp(f'Question: {q} Answer: {a}'), 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62ef7a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize counters\n",
    "pos_counts = Counter()\n",
    "function_word_counts = Counter()\n",
    "punctuation_counts = Counter()\n",
    "uppercase_counts = Counter()\n",
    "\n",
    "# Initialize variables for sentence length calculation\n",
    "total_tokens = 0\n",
    "total_sentences = 0\n",
    "\n",
    "# Initialize list to store each text's perplexity\n",
    "perplexities = []\n",
    "\n",
    "data = load_pubmed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c818ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc, _ in data:\n",
    "    text = doc.text\n",
    "    perplexities.append(calculate_perplexity(text))\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "        if token.text in {'a', 'in', 'of'}:\n",
    "            function_word_counts[token.text] += 1\n",
    "        if token.is_punct:\n",
    "            punctuation_counts[token.text] += 1\n",
    "        if token.text.isupper():\n",
    "            uppercase_counts[token.text] += 1\n",
    "    total_tokens += len(doc)\n",
    "    total_sentences += len(list(doc.sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc5a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average sentence length\n",
    "avg_sentence_length = total_tokens / total_sentences if total_sentences > 0 else 0\n",
    "\n",
    "# Calculate average perplexity\n",
    "avg_perplexity = sum(perplexities) / len(perplexities) if perplexities else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302f3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Generate a KDE plot with seaborn\n",
    "sns.kdeplot(perplexities, ax=ax)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Perplexity')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('Distribution of Perplexities')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42736e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
