{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d66056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATASETS = ['pubmed_qa', 'writingprompts']\n",
    "DATA_PATH = 'data/writingPrompts'\n",
    "NUM_EXAMPLES = 150\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def process_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts_dataset(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset.\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [process_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [process_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a dataset based on its name.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts_dataset()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses a dataset.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    if dataset == 'writingprompts':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories from the dataset\")  # debug print\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bb24cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Counts the frequency of POS tags, punctuation marks and function words in a given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two dictionaries, where keys are POS tags and punctuation marks\n",
    "           and values are their corresponding count.\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    # Split questions and answers for pubmed_qa dataset\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Strip the 'Answer:' prefix\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer.\n",
    "    Returns a model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "    Calculate the average word length of a list of texts using SpaCy.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    float: The average word length.\n",
    "    \"\"\"\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length of a list of texts using SpaCy.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    float: The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a piece of text.\n",
    "    \"\"\"\n",
    "    # tokenize the input, add special tokens and return tensors\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # if the text is too long, skip it\n",
    "    # this step has the extra effect of removing examples with low-quality/garbage content\n",
    "    if len(input_ids[0]) > 512:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=input_ids)\n",
    "    loss = output.loss\n",
    "    return torch.exp(loss).item()  # perplexity is e^loss\n",
    "\n",
    "\n",
    "def compute_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    texts, labels = zip(*data)\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Stripping the 'Answer: ' string\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n",
    "\n",
    "def plot_perplexities(sentence_perplexities, text_perplexities):\n",
    "    \"\"\"\n",
    "    Plots Kernel Density Estimates of the sentence and text perplexities.\n",
    "\n",
    "    Args:\n",
    "    sentence_perplexities (list of float): The perplexities of the sentences.\n",
    "    text_perplexities (list of float): The perplexities of the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot sentence perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(sentence_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 12)  # Limit x-axis to 12 for sentence perplexity\n",
    "    plt.show()\n",
    "\n",
    "    # Plot text perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(text_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Text Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 10)  # Limit x-axis to 10 for text perplexity\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4f53783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 questions from the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_statisticst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m preprocess_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpubmed_qa\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m statistics \u001b[38;5;241m=\u001b[39m compute_statistics(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpubmed_qa\u001b[39m\u001b[38;5;124m'\u001b[39m,data)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mprint_statisticst\u001b[49m(statistics)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_statisticst' is not defined"
     ]
    }
   ],
   "source": [
    "data = preprocess_data('pubmed_qa')\n",
    "statistics = compute_statistics('pubmed_qa',data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c79968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 914\n",
      "Frequency of adverbs: 205\n",
      "Frequency of conjunctions: 216\n",
      "Frequency of nouns: 1998\n",
      "Frequency of numbers: 40\n",
      "Frequency of pronouns: 142\n",
      "Frequency of verbs: 632\n",
      "Frequency of commas: 202\n",
      "Frequency of fullstops: 308\n",
      "Frequency of special character '-': 82\n",
      "Frequency of function word 'a': 124\n",
      "Frequency of function word 'in': 169\n",
      "Frequency of function word 'of': 283\n",
      "Frequency of function word 'the': 253\n",
      "Average word length: 5.654920832039739\n",
      "Average sentence length: 22.97087378640777\n",
      "Average sentence perplexity: 4.01935589930772\n",
      "Average text perplexity: 2.2276509324709575\n"
     ]
    }
   ],
   "source": [
    "print_statistics(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b10777",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
