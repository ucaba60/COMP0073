{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07e4e4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "\n",
    "DATASETS = ['pubmed_qa', 'writingprompts']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def load_pubmed():\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split='train[:150]')\n",
    "\n",
    "    # combine question and long_answer, and label them as 0\n",
    "    data = [(f'Question: {q} Answer:{a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    tags = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]']\n",
    "    for tag in tags:\n",
    "        prompt = prompt.replace(tag, '')\n",
    "    return prompt\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    text = re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def process_spaces(story):\n",
    "    return story.replace(\n",
    "        ' ,', ',').replace(\n",
    "        ' .', '.').replace(\n",
    "        ' ?', '?').replace(\n",
    "        ' !', '!').replace(\n",
    "        ' ;', ';').replace(\n",
    "        ' \\'', '\\'').replace(\n",
    "        ' â€™ ', '\\'').replace(\n",
    "        ' :', ':').replace(\n",
    "        '<newline>', '\\n').replace(\n",
    "        '`` ', '\"').replace(\n",
    "        ' \\'\\'', '\"').replace(\n",
    "        '\\'\\'', '\"').replace(\n",
    "        '.. ', '... ').replace(\n",
    "        ' )', ')').replace(\n",
    "        '( ', '(').replace(\n",
    "        ' n\\'t', 'n\\'t').replace(\n",
    "        ' i ', ' I ').replace(\n",
    "        ' i\\'', ' I\\'').replace(\n",
    "        '\\\\\\'', '\\'').replace(\n",
    "        '\\n ', '\\n').strip()\n",
    "\n",
    "\n",
    "def load_writingPrompts_dataset():\n",
    "    writing_path = 'data/writingPrompts'\n",
    "\n",
    "    with open(f'{writing_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:178]\n",
    "    with open(f'{writing_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:178]\n",
    "\n",
    "    prompts = [process_prompt(prompt) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "    prompts = [prompt.rstrip() for prompt in prompts]\n",
    "    stories = [process_spaces(story) for story in stories]\n",
    "    joined = [\"Prompt: \" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story and 'NSFW' not in story]\n",
    "\n",
    "    # Label the stories as 0 to indicate they are human-generated\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts_dataset()\n",
    "    else:\n",
    "        print(f\"Dataset name {dataset_name} not recognized.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    if dataset in DATASETS:\n",
    "        data = load_data(dataset)\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    data = list(dict.fromkeys(data))  # deterministic, as opposed to set()\n",
    "\n",
    "    # strip whitespace around each example\n",
    "    data = [(x[0].strip(), x[1]) for x in data]\n",
    "\n",
    "    # remove newlines from each example\n",
    "    data = [(strip_newlines(q), a) for q, a in data]\n",
    "    \n",
    "    if dataset in ['pubmed_qa']:  \n",
    "        print(f\"Loaded and pre-processed {len(data)} answers from the dataset\")  # debug print\n",
    "    # try to keep only examples with > 250 words\n",
    "    if dataset in ['writingprompts']:\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed{len(data)} prompts/stories from the dataset\")  # debug print\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8942be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    \"\"\"\n",
    "    Counts the frequency of POS tags, punctuation marks and function words in a given text.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two dictionaries, where keys are POS tags and punctuation marks \n",
    "           and values are their corresponding count.\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73d7051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_count(dataset_name):\n",
    "    # Load and preprocess the data\n",
    "    data = preprocess_data(dataset_name)\n",
    "    \n",
    "    # Extract texts\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    # Split questions and answers for pubmed_qa dataset\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1] for text in texts]\n",
    "    \n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(*[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    # Print the frequencies\n",
    "    print(f\"Frequency of adjectives: {overall_pos_counts['ADJ']}\")\n",
    "    print(f\"Frequency of adverbs: {overall_pos_counts['ADV']}\")\n",
    "    print(f\"Frequency of conjunctions: {overall_pos_counts['CCONJ']}\")\n",
    "    print(f\"Frequency of nouns: {overall_pos_counts['NOUN']}\")\n",
    "    print(f\"Frequency of numbers: {overall_pos_counts['NUM']}\")\n",
    "    print(f\"Frequency of pronouns: {overall_pos_counts['PRON']}\")\n",
    "    print(f\"Frequency of verbs: {overall_pos_counts['VERB']}\")\n",
    "    print(f\"Frequency of commas: {overall_punctuation_counts[',']}\")\n",
    "    print(f\"Frequency of fullstops: {overall_punctuation_counts['.']}\")\n",
    "    print(f\"Frequency of special character '-': {overall_punctuation_counts['-']}\")\n",
    "    print(f\"Frequency of function word 'a': {overall_function_word_counts['a']}\")\n",
    "    print(f\"Frequency of function word 'in': {overall_function_word_counts['in']}\")\n",
    "    print(f\"Frequency of function word 'of': {overall_function_word_counts['of']}\")\n",
    "    print(f\"Frequency of function word 'the': {overall_function_word_counts['the']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d829d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 answers from the dataset\n",
      "Frequency of adjectives: 914\n",
      "Frequency of adverbs: 205\n",
      "Frequency of conjunctions: 216\n",
      "Frequency of nouns: 1998\n",
      "Frequency of numbers: 40\n",
      "Frequency of pronouns: 142\n",
      "Frequency of verbs: 632\n",
      "Frequency of commas: 202\n",
      "Frequency of fullstops: 308\n",
      "Frequency of special character '-': 82\n",
      "Frequency of function word 'a': 124\n",
      "Frequency of function word 'in': 169\n",
      "Frequency of function word 'of': 283\n",
      "Frequency of function word 'the': 253\n"
     ]
    }
   ],
   "source": [
    "load_and_count('pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76b01cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 prompts/stories from the dataset\n",
      "Frequency of adjectives: 6442\n",
      "Frequency of adverbs: 6118\n",
      "Frequency of conjunctions: 3426\n",
      "Frequency of nouns: 17808\n",
      "Frequency of numbers: 798\n",
      "Frequency of pronouns: 16190\n",
      "Frequency of verbs: 15555\n",
      "Frequency of commas: 5843\n",
      "Frequency of fullstops: 7028\n",
      "Frequency of special character '-': 328\n",
      "Frequency of function word 'a': 2173\n",
      "Frequency of function word 'in': 1208\n",
      "Frequency of function word 'of': 1975\n",
      "Frequency of function word 'the': 4629\n"
     ]
    }
   ],
   "source": [
    "load_and_count('writingprompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa34ff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "90c60aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizerFast\n",
    "from statistics import mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "546fdc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer.\n",
    "    Returns a model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length of a list of texts.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    float: The average sentence length.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "    # Split the texts into sentences\n",
    "    sentences = [sentence for text in texts for sentence in text.split('. ')]\n",
    "\n",
    "    # Tokenize the sentences and calculate their length\n",
    "    sentence_lengths = [len(tokenizer.tokenize(sentence)) for sentence in sentences]\n",
    "\n",
    "    # Calculate and return the average sentence length\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a piece of text.\n",
    "    \"\"\"\n",
    "    # tokenize the input, add special tokens and return tensors\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # if the text is too long, skip it\n",
    "    if len(input_ids[0]) > 512:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=input_ids)\n",
    "    loss = output.loss\n",
    "    return torch.exp(loss).item()  # perplexity is e^loss\n",
    "\n",
    "\n",
    "def calculate_average_perplexities(dataset_name):\n",
    "    # Load and preprocess the data\n",
    "    data = preprocess_data(dataset_name)\n",
    "\n",
    "    # Extract texts\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    # Split questions and answers for pubmed_qa dataset\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1] for text in texts]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1] for text in texts]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Calculate the perplexity for each text\n",
    "    perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "\n",
    "    # Filter out None values\n",
    "    perplexities = [p for p in perplexities if p is not None]\n",
    "    \n",
    "    \n",
    "    # Calculate the average sentence length\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    print(f\"Average sentence length: {average_sentence_length}\")\n",
    "\n",
    "\n",
    "    # Calculate and print the average text perplexity\n",
    "    average_text_perplexity = sum(perplexities) / len(perplexities)\n",
    "    print(f\"Average text perplexity: {average_text_perplexity}\")\n",
    "\n",
    "    # Split the texts into sentences and calculate the perplexity for each sentence\n",
    "    sentences = [sentence for text in texts for sentence in text.split('. ')]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "\n",
    "    # Filter out None values\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "\n",
    "    # Calculate and print the average sentence perplexity\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    print(f\"Average sentence perplexity: {average_sentence_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf919404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 answers from the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length: 23.99361022364217\n",
      "Average text perplexity: 2.2276509324709575\n",
      "Average sentence perplexity: 4.970833402853042\n"
     ]
    }
   ],
   "source": [
    "calculate_average_perplexities('pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb7ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
