{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e79f214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "DATASETS = ['pubmed_qa', 'writingprompts']\n",
    "DATA_PATH = 'data/writingPrompts'\n",
    "NUM_EXAMPLES = 150\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def process_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts_dataset(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset.\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [process_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' ’ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [process_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "    Loads a dataset based on its name.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts_dataset()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "    Preprocesses a dataset.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    if dataset == 'writingprompts':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories from the dataset\")  # debug print\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ac229b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForMaskedLM\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\spacy\\errors.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\spacy\\compat.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Helpers for Python and platform compatibility.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m copy_array\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcPickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\thinc\\__init__.py:5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m registry\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      9\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistry\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     12\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\thinc\\config.py:4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, Promise, VARIABLE_RE\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decorator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mregistry\u001b[39;00m(confection\u001b[38;5;241m.\u001b[39mregistry):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     optimizers: Decorator \u001b[38;5;241m=\u001b[39m catalogue\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthinc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, entry_points\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\thinc\\types.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_cupy, cupy\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_cupy:\n\u001b[0;32m     11\u001b[0m     get_array_module \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mget_array_module\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\thinc\\compat.py:30\u001b[0m\n\u001b[0;32m     26\u001b[0m     has_cupy_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdlpack\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     has_torch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\__init__.py:1239\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m special \u001b[38;5;28;01mas\u001b[39;00m special\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackcompat\u001b[39;00m\n\u001b[1;32m-> 1239\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m onnx \u001b[38;5;28;01mas\u001b[39;00m onnx\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jit \u001b[38;5;28;01mas\u001b[39;00m jit\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg \u001b[38;5;28;01mas\u001b[39;00m linalg\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\__init__.py:12\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _onnx \u001b[38;5;28;01mas\u001b[39;00m _C_onnx\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     _CAFFE2_ATEN_FALLBACK,\n\u001b[0;32m      7\u001b[0m     OperatorExportTypes,\n\u001b[0;32m      8\u001b[0m     TensorProtoDataType,\n\u001b[0;32m      9\u001b[0m     TrainingMode,\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort:skip. Keep the order instead of sorting lexicographically\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     _deprecation,\n\u001b[0;32m     14\u001b[0m     errors,\n\u001b[0;32m     15\u001b[0m     symbolic_caffe2,\n\u001b[0;32m     16\u001b[0m     symbolic_helper,\n\u001b[0;32m     17\u001b[0m     symbolic_opset7,\n\u001b[0;32m     18\u001b[0m     symbolic_opset8,\n\u001b[0;32m     19\u001b[0m     symbolic_opset9,\n\u001b[0;32m     20\u001b[0m     symbolic_opset10,\n\u001b[0;32m     21\u001b[0m     symbolic_opset11,\n\u001b[0;32m     22\u001b[0m     symbolic_opset12,\n\u001b[0;32m     23\u001b[0m     symbolic_opset13,\n\u001b[0;32m     24\u001b[0m     symbolic_opset14,\n\u001b[0;32m     25\u001b[0m     symbolic_opset15,\n\u001b[0;32m     26\u001b[0m     symbolic_opset16,\n\u001b[0;32m     27\u001b[0m     symbolic_opset17,\n\u001b[0;32m     28\u001b[0m     symbolic_opset18,\n\u001b[0;32m     29\u001b[0m     utils,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# TODO(After 1.13 release): Remove the deprecated SymbolicContext\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_exporter_states\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExportTypes, SymbolicContext\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\errors.py:9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _C\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _constants\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diagnostics\n\u001b[0;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnnxExporterError\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnnxExporterWarning\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSymbolicValueError\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m ]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOnnxExporterWarning\u001b[39;00m(\u001b[38;5;167;01mUserWarning\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_diagnostic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     create_export_diagnostic_context,\n\u001b[0;32m      3\u001b[0m     diagnose,\n\u001b[0;32m      4\u001b[0m     engine,\n\u001b[0;32m      5\u001b[0m     export_context,\n\u001b[0;32m      6\u001b[0m     ExportDiagnostic,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_rules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rules\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levels\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\_diagnostic.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m infra\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cpp_backtrace\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cpp_call_stack\u001b[39m(frames_to_skip: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, frames_to_log: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m infra\u001b[38;5;241m.\u001b[39mStack:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\infra\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_infra\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     DiagnosticOptions,\n\u001b[0;32m      3\u001b[0m     Graph,\n\u001b[0;32m      4\u001b[0m     Invocation,\n\u001b[0;32m      5\u001b[0m     Level,\n\u001b[0;32m      6\u001b[0m     levels,\n\u001b[0;32m      7\u001b[0m     Location,\n\u001b[0;32m      8\u001b[0m     Rule,\n\u001b[0;32m      9\u001b[0m     RuleCollection,\n\u001b[0;32m     10\u001b[0m     Stack,\n\u001b[0;32m     11\u001b[0m     StackFrame,\n\u001b[0;32m     12\u001b[0m     Tag,\n\u001b[0;32m     13\u001b[0m     ThreadFlowLocation,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Diagnostic, DiagnosticContext, DiagnosticEngine\n\u001b[0;32m     17\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiagnostic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiagnosticContext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThreadFlowLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\infra\\_infra.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpprint\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FrozenSet, List, Mapping, Optional, Sequence, Tuple\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m formatter, sarif\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mLevel\u001b[39;00m(enum\u001b[38;5;241m.\u001b[39mEnum):\n\u001b[0;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The level of a diagnostic.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    This class is used to represent the level of a diagnostic. The levels are defined\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    - ERROR: A serious problem was found.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\infra\\formatter.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Dict, List, Optional, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _beartype\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sarif\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# A list of types in the SARIF module to support pretty printing.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# This is solely for type annotation for the functions below.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m _SarifClass \u001b[38;5;241m=\u001b[39m Union[\n\u001b[0;32m     13\u001b[0m     sarif\u001b[38;5;241m.\u001b[39mSarifLog,\n\u001b[0;32m     14\u001b[0m     sarif\u001b[38;5;241m.\u001b[39mRun,\n\u001b[0;32m     15\u001b[0m     sarif\u001b[38;5;241m.\u001b[39mReportingDescriptor,\n\u001b[0;32m     16\u001b[0m     sarif\u001b[38;5;241m.\u001b[39mResult,\n\u001b[0;32m     17\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_address\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Address\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_artifact\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Artifact\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_artifact_change\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArtifactChange\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_artifact_content\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     ArtifactContent,\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_artifact_location\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     ArtifactLocation,\n\u001b[0;32m     12\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\onnx\\_internal\\diagnostics\\infra\\sarif\\_artifact_change.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataclasses\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01monnx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_internal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minfra\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msarif\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     _artifact_location,\n\u001b[0;32m     11\u001b[0m     _property_bag,\n\u001b[0;32m     12\u001b[0m     _replacement,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;129m@dataclasses\u001b[39m\u001b[38;5;241m.\u001b[39mdataclass\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mArtifactChange\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A change to a single artifact.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:969\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1091\u001b[0m, in \u001b[0;36mpath_stats\u001b[1;34m(self, path)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def process_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Process the prefixes in the given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset.\n",
    "\n",
    "    Returns:\n",
    "    list: The list of texts after stripping the prefix.\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Strip the 'Answer:' prefix\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Counts the frequency of POS tags, punctuation marks and function words in a given text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two dictionaries, where keys are POS tags and punctuation marks\n",
    "           and values are their corresponding count.\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the Flesch Reading Ease and Flesch-Kincaid Grade Level.\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = process_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Load the model and tokenizer.\n",
    "    Returns a model and tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "    Calculate the average word length of a list of texts using SpaCy.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    float: The average word length.\n",
    "    \"\"\"\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length of a list of texts using SpaCy.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    float: The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Calculate the perplexity of a piece of text.\n",
    "    \"\"\"\n",
    "    # tokenize the input, add special tokens and return tensors\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # if the text is too long, skip it\n",
    "    # this step has the extra effect of removing examples with low-quality/garbage content\n",
    "    if len(input_ids[0]) > 512:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=input_ids)\n",
    "    loss = output.loss\n",
    "    return torch.exp(loss).item()  # perplexity is e^loss\n",
    "\n",
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    texts, labels = process_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n",
    "\n",
    "def plot_perplexities(sentence_perplexities, text_perplexities):\n",
    "    \"\"\"\n",
    "    Plots Kernel Density Estimates of the sentence and text perplexities.\n",
    "\n",
    "    Args:\n",
    "    sentence_perplexities (list of float): The perplexities of the sentences.\n",
    "    text_perplexities (list of float): The perplexities of the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot sentence perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(sentence_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 12)  # Limit x-axis to 12 for sentence perplexity\n",
    "    plt.show()\n",
    "\n",
    "    # Plot text perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(text_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Text Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 10)  # Limit x-axis to 10 for text perplexity\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57ee65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data('pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = summary_statistics('pubmed_qa',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48aed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fcc1b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    }
   ],
   "source": [
    "DATASETS = ['pubmed_qa', 'writingprompts']\n",
    "DATA_PATH = 'data/writingPrompts'\n",
    "NUM_EXAMPLES = 150\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]']\n",
    "\n",
    "\n",
    "data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{NUM_EXAMPLES}]')\n",
    "data = [(f'Summary: {s} Article: {a}', 0) for a, s in zip(data['article'], data['highlights'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8c19632",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(dict.fromkeys(data))\n",
    "data = [(strip_newlines(q).strip(), a) for q, a in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f23ab96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 150\n",
    "\"\"\"\n",
    "Loads the CNN/Daily Mail dataset.\n",
    "\"\"\"\n",
    "data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{NUM_EXAMPLES}]')\n",
    "\n",
    "\n",
    "processed_data = []\n",
    "for a, s in zip(data['article'], data['highlights']):\n",
    "    # remove the string and the '--' from the start of the articles\n",
    "    a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "    # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "    a = a.replace('E-mail to a friend .', '')\n",
    "    s = s.replace('NEW:','')\n",
    "    a = a.replace('Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.','')\n",
    "\n",
    "    # remove whitespace before punctuation marks in both article and summary\n",
    "    a = remove_whitespace_before_punctuations(a)\n",
    "    s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "    processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "    \n",
    "data = processed_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "06287ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(dict.fromkeys(data))\n",
    "data = [(strip_newlines(q).strip(), a) for q, a in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "53193cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday. Young actor says he has no plans to fritter his cash away. Radcliffe's earnings from first five Potter films have been held in trust fund. Article: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest ». There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters.\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "50a256db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 139 prompts/stories from the dataset\n"
     ]
    }
   ],
   "source": [
    "long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "if len(long_data) > 0:\n",
    "    data = long_data\n",
    "print(f\"Loaded and pre-processed {len(data)} prompts/stories from the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "14e61192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a432dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Summary: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday. Young actor says he has no plans to fritter his cash away. Radcliffe\\'s earnings from first five Potter films have been held in trust fund. Article: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\\'s latest ». There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.', 0)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b3d36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
