{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bea36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts'\n",
    "NUM_EXAMPLES = 200\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n",
    "directory = 'Labelled_Data/'\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(dataset_name='gpt'):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str, optional): Name of the preprocessed GPT dataset. Defaults to 'gpt'.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    file_path = 'Labelled_Data/t1_preprocessed.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        return load_gpt()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce7cb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------#\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "def remove_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts if \"Answer:\" in text]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts if \"Story:\" in text]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() for text in texts if \"Article:\" in text]\n",
    "    elif dataset_name == 'gpt':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() if \"Story:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() if \"Article:\" in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "\n",
    "def average_token_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Calculates the average number of tokens in the answers of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of tokens in the answers of a dataset\n",
    "    \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    average_tokens = total_tokens / len(texts)\n",
    "\n",
    "    return average_tokens\n",
    "\n",
    "\n",
    "# PUBMED = 54\n",
    "# WP = 780\n",
    "# CNN = 794\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "   \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a29312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "91270218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "    \n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "    \n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "    \n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b74ef5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_texts(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data for a specified dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Article:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Summary:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        # Split the full_text depending on the dataset\n",
    "        if dataset_name == 'pubmed_qa':\n",
    "            split_text = full_text.split(\"Question:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Answer:\", 1)\n",
    "        elif dataset_name == 'writingprompts':\n",
    "            split_text = full_text.split(\"Prompt:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Story:\", 1)\n",
    "        elif dataset_name == 'cnn_dailymail':\n",
    "            split_text = full_text.split(\"Article:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Summary:\", 1)\n",
    "        elif dataset_name == 'gpt':\n",
    "            # Identify the starting point for each entry in the 'gpt' dataset\n",
    "            for starting_point in starting_points:\n",
    "                if starting_point in full_text:\n",
    "                    split_text = full_text.split(starting_point, 1)\n",
    "                    if len(split_text) == 2:\n",
    "                        _, temp_prompt = split_text\n",
    "                        for end_point in end_points:\n",
    "                            if end_point in temp_prompt:\n",
    "                                prompt, _ = temp_prompt.split(end_point, 1)\n",
    "                                break\n",
    "                    break\n",
    "\n",
    "        prompt = prompt.strip()  # remove leading and trailing whitespaces\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05fd1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_for_dataset(dataset_name, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = preprocess_data(dataset_name)\n",
    "    prompts_and_texts = extract_prompts_and_texts(dataset_name, data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51c30376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i+1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0497208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "       \n",
    "   \"\"\"\n",
    "    \n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    prompts_and_responses = extract_prompts_and_texts(dataset_name, data)\n",
    "    prompt_to_text_cosine_similarities = calculate_cosine_similarities_for_dataset(dataset_name, model, tokenizer)\n",
    "    \n",
    "    sentence_cosine_similarities = []\n",
    "    for _, text in prompts_and_responses:\n",
    "        sentence_cosine_similarities.extend(calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer))\n",
    "\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities,  # and this\n",
    "        'average_prompt_to_text_cosine_similarity': mean(prompt_to_text_cosine_similarities),\n",
    "        'average_sentence_cosine_similarity': mean(sentence_cosine_similarities),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "    print(f\"Average prompt to text cosine simiarity: {statistics['average_prompt_to_text_cosine_similarity']}\")\n",
    "    print(f\"Average sentence cosine simiarity: {statistics['average_sentence_cosine_similarity']}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32ee4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 200 entries from the dataset pubmed_qa\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_data('pubmed_qa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e2ec1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 200 entries from the dataset pubmed_qa\n"
     ]
    }
   ],
   "source": [
    "x = summary_statistics('pubmed_qa',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cf250d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 1189\n",
      "Frequency of adverbs: 265\n",
      "Frequency of conjunctions: 288\n",
      "Frequency of nouns: 2642\n",
      "Frequency of numbers: 58\n",
      "Frequency of pronouns: 199\n",
      "Frequency of verbs: 851\n",
      "Frequency of commas: 264\n",
      "Frequency of fullstops: 402\n",
      "Frequency of special character '-': 112\n",
      "Frequency of function word 'a': 156\n",
      "Frequency of function word 'in': 236\n",
      "Frequency of function word 'of': 372\n",
      "Frequency of function word 'the': 342\n",
      "Average Flesch Reading Ease: 30.3669\n",
      "Average Flesch-Kincaid Grade Level: 14.3805\n",
      "Average word length: 5.606309370235722\n",
      "Average sentence length: 23.304455445544555\n",
      "Average sentence perplexity: 1.085525385223993\n",
      "Average text perplexity: 1.0855632001161575\n",
      "Average prompt to text cosine simiarity: 0.9767077764868737\n",
      "Average sentence cosine simiarity: 0.979069270047487\n"
     ]
    }
   ],
   "source": [
    "print_statistics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9682335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
