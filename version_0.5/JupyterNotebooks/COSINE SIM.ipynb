{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bea36d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n",
    "import argparse\n",
    "import glob\n",
    "\n",
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts'\n",
    "NUM_EXAMPLES = 200\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n",
    "directory = 'Labelled_Data/'\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(dataset_name='gpt'):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str, optional): Name of the preprocessed GPT dataset. Defaults to 'gpt'.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    file_path = 'Labelled_Data/t1_preprocessed.csv'\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        return load_gpt()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of respones being fairly short.\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7cb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------#\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "def remove_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts if \"Answer:\" in text]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts if \"Story:\" in text]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() for text in texts if \"Article:\" in text]\n",
    "    elif dataset_name == 'gpt':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() if \"Story:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() if \"Article:\" in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "\n",
    "def average_token_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Calculates the average number of tokens in the answers of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of tokens in the answers of a dataset\n",
    "    \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    average_tokens = total_tokens / len(texts)\n",
    "\n",
    "    return average_tokens\n",
    "\n",
    "\n",
    "# PUBMED = 54\n",
    "# WP = 780\n",
    "# CNN = 794\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "   \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a29312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91270218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "    \n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "    \n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "    \n",
    "    return cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74ef5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_texts(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data for a specified dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Article:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Summary:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        # Split the full_text depending on the dataset\n",
    "        if dataset_name == 'pubmed_qa':\n",
    "            split_text = full_text.split(\"Question:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Answer:\", 1)\n",
    "        elif dataset_name == 'writingprompts':\n",
    "            split_text = full_text.split(\"Prompt:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Story:\", 1)\n",
    "        elif dataset_name == 'cnn_dailymail':\n",
    "            split_text = full_text.split(\"Article:\", 1)\n",
    "            if len(split_text) == 2:\n",
    "                _, temp_prompt = split_text\n",
    "                prompt, _ = temp_prompt.split(\"Summary:\", 1)\n",
    "        elif dataset_name == 'gpt':\n",
    "            # Identify the starting point for each entry in the 'gpt' dataset\n",
    "            for starting_point in starting_points:\n",
    "                if starting_point in full_text:\n",
    "                    split_text = full_text.split(starting_point, 1)\n",
    "                    if len(split_text) == 2:\n",
    "                        _, temp_prompt = split_text\n",
    "                        for end_point in end_points:\n",
    "                            if end_point in temp_prompt:\n",
    "                                prompt, _ = temp_prompt.split(end_point, 1)\n",
    "                                break\n",
    "                    break\n",
    "\n",
    "        prompt = prompt.strip()  # remove leading and trailing whitespaces\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05fd1d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_for_dataset(dataset_name, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts_and_texts = extract_prompts_and_texts(dataset_name, data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c30376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i+1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0497208",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "       \n",
    "   \"\"\"\n",
    "    \n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    prompts_and_responses = extract_prompts_and_texts(dataset_name, data)\n",
    "    prompt_to_text_cosine_similarities = calculate_cosine_similarities_for_dataset(dataset_name, model, tokenizer)\n",
    "    \n",
    "    sentence_cosine_similarities = []\n",
    "    for _, text in prompts_and_responses:\n",
    "        sentence_cosine_similarities.extend(calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer))\n",
    "\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities,  # and this\n",
    "        'average_prompt_to_text_cosine_similarity': mean(prompt_to_text_cosine_similarities),\n",
    "        'average_sentence_cosine_similarity': mean(sentence_cosine_similarities),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "    print(f\"Average prompt to text cosine simiarity: {statistics['average_prompt_to_text_cosine_similarity']}\")\n",
    "    print(f\"Average sentence cosine simiarity: {statistics['average_sentence_cosine_similarity']}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32ee4e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 200 entries from the dataset pubmed_qa\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_data('pubmed_qa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ec1959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = summary_statistics('pubmed_qa',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf250d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_statistics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "726183e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 1189\n",
      "Frequency of adverbs: 265\n",
      "Frequency of conjunctions: 288\n",
      "Frequency of nouns: 2642\n",
      "Frequency of numbers: 58\n",
      "Frequency of pronouns: 199\n",
      "Frequency of verbs: 851\n",
      "Frequency of commas: 264\n",
      "Frequency of fullstops: 402\n",
      "Frequency of special character '-': 112\n",
      "Frequency of function word 'a': 156\n",
      "Frequency of function word 'in': 236\n",
      "Frequency of function word 'of': 372\n",
      "Frequency of function word 'the': 342\n",
      "Average Flesch Reading Ease: 30.3669\n",
      "Average Flesch-Kincaid Grade Level: 14.3805\n",
      "Average word length: 5.606309370235722\n",
      "Average sentence length: 23.304455445544555\n",
      "Average sentence perplexity: 1.085525385223993\n",
      "Average text perplexity: 1.0855632001161575\n",
      "Average prompt to text cosine simiarity: 0.9767077764868737\n",
      "Average sentence cosine simiarity: 0.979069270047487\n"
     ]
    }
   ],
   "source": [
    "print_statistics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6045bbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data, dataset_name):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store features and labels\n",
    "    feature_list = []\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Remove prefixes\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "    prompts_and_texts = extract_prompts_and_texts(dataset_name, data)\n",
    "\n",
    "    for (prompt, text), label in zip(prompts_and_texts, labels):\n",
    "        # Count POS tags in the text\n",
    "        pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "        # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "        flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "        # Calculate the perplexity of the text and average sentence perplexity\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "        text = tokenizer.decode(text_encoded)\n",
    "        text = text.replace('<s>', '').replace('</s>', '')\n",
    "\n",
    "\n",
    "        text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "        sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in nlp(text).sents]\n",
    "        sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "        avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "            sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "        # Calculate the frequency of uppercase letters\n",
    "        uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "        \n",
    "        # Calculate the cosine similarity for the prompt and text\n",
    "        prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        \n",
    "        # Calculate the average cosine similarity for sentences in the text\n",
    "        sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer)\n",
    "        avg_sentence_cosine_similarity = None\n",
    "        if sentence_cosine_similarities:\n",
    "            avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(sentence_cosine_similarities)\n",
    "        else:\n",
    "            print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "\n",
    "        # Prepare a dictionary to append to the feature list\n",
    "        features = {\n",
    "            'ADJ': pos_counts.get('ADJ', 0),\n",
    "            'ADV': pos_counts.get('ADV', 0),\n",
    "            'CONJ': pos_counts.get('CONJ', 0),\n",
    "            'NOUN': pos_counts.get('NOUN', 0),\n",
    "            'NUM': pos_counts.get('NUM', 0),\n",
    "            'VERB': pos_counts.get('VERB', 0),\n",
    "            'COMMA': punctuation_counts.get(',', 0),\n",
    "            'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "            'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "            'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "            'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "            'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "            'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "            'uppercase_freq': uppercase_freq,  # new feature\n",
    "            'flesch_reading_ease': flesch_reading_ease,\n",
    "            'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'avg_sentence_length': avg_sentence_length,\n",
    "            'text_perplexity': text_perplexity,\n",
    "            'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "            'prompt_text_cosine_similarity': prompt_text_cosine_similarity,  # new feature\n",
    "            'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,  # new feature\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "        # Add the feature dictionary to the feature list\n",
    "        feature_list.append(features)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    data_matrix = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "    return data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59b6c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No sentence cosine similarities calculated for text: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n",
      "WARNING: No sentence cosine similarities calculated for text: DBE appears to be equally safe and effective when performed in the community setting as compared to a tertiary referral center with a comparable yield, efficacy, and complication rate.\n",
      "WARNING: No sentence cosine similarities calculated for text: Genetic variants identified in the present study may be insufficient to promote early carotid atherosclerosis.\n",
      "WARNING: No sentence cosine similarities calculated for text: Opioid PCT is a feasible and acceptable therapeutic method to reduce refractory breathlessness in palliative care patients.\n",
      "WARNING: No sentence cosine similarities calculated for text: Findings suggest that cluster analyses may be useful in identifying groups for targeted health messages.\n",
      "WARNING: No sentence cosine similarities calculated for text: RPN in a porcine model is feasible and could be very useful for teaching and practicing retroperitoneoscopy.\n",
      "WARNING: No sentence cosine similarities calculated for text: Present audit shows that the process of implementation of labor analgesia was quick, successful and safe, notwithstanding the identification of one cluster of women with suboptimal response to epidural analgesia that need to be further studies, overall pregnant womens'adhesion to labor analgesia was satisfactory.\n",
      "WARNING: No sentence cosine similarities calculated for text: HER2 immunoreactivity might have a limited prognostic value for advanced urothelial carcinoma patients with adjuvant M-VEC.\n",
      "WARNING: No sentence cosine similarities calculated for text: Halofantrine has mild to moderate pathological effects on cochlea histology, and can be considered an ototoxic drug.\n",
      "WARNING: No sentence cosine similarities calculated for text: The results of this study cast doubt on the suggested advantage of HBO in reducing patient mortality and morbidity when used as adjuvant therapy for NF.\n",
      "WARNING: No sentence cosine similarities calculated for text: A positive Hawkins sign rules out that the fractured talus has developed avascular necrosis, but its absence does not confirm it.\n",
      "WARNING: No sentence cosine similarities calculated for text: The findings showed that acupuncture of voice-related acupoints could bring about improvement in vocal function and healing of vocal fold lesions.\n",
      "WARNING: No sentence cosine similarities calculated for text: W-d HCCs were clinically demonstrated not to be early cancer, because there was no significant difference in disease free survival between the patients with w-d and l-d HCCs.\n",
      "WARNING: No sentence cosine similarities calculated for text: In comparison with its accuracy in non-DM patients, the accuracy of PET in cervical cancer patients with mild to moderate DM was not significantly reduced.\n",
      "WARNING: No sentence cosine similarities calculated for text: Patients who previously received radiotherapy for primary rectal cancer treatment have worse oncologic outcomes than those who had not received radiotherapy after pelvic exenteration for locally recurrent rectal cancer.\n",
      "WARNING: No sentence cosine similarities calculated for text: An increase of 5% in circumference measurements identified the most potential lymphedema cases compared with an academic trial.\n",
      "WARNING: No sentence cosine similarities calculated for text: In our cohort, selective screening of patients aged>70 years, with carotid bruit, a history of cerebrovascular disease, diabetes mellitus or PVD would have reduced the screening load by 40%, with trivial impact on surgical management or neurological outcomes.\n",
      "WARNING: No sentence cosine similarities calculated for text: Mesocolon invasion should be included in T4 for the staging of gastric cancer.\n",
      "WARNING: No sentence cosine similarities calculated for text: Font influenced pregnant women's ratings of intervention complexity.\n",
      "WARNING: No sentence cosine similarities calculated for text: We found no evidence that IBR compromised the delivery of adjuvant chemotherapy, although there was a significant incidence of implant infection.\n",
      "WARNING: No sentence cosine similarities calculated for text: The low presence of HPV DNA in pterygia does not support the hypothesis that HPV is involved in the development of pterygia in Denmark.\n",
      "WARNING: No sentence cosine similarities calculated for text: IADL disability is a useful addition to the diagnostic process in a memory clinic setting, indicating who is at higher risk of developing dementia at 1- and 2-year follow-up.\n",
      "WARNING: No sentence cosine similarities calculated for text: In the present study, the use of an endoscope during VPS procedures did not increase the risk of surgical infection.\n",
      "WARNING: No sentence cosine similarities calculated for text: These findings indicate that a relationship between multiple sclerosis and streptococcal infections may exist, but to acquire a better understanding of the role of group A streptococci in the pathogenesis of multiple sclerosis, more studies with animal models are necessary.\n",
      "WARNING: No sentence cosine similarities calculated for text: Rescue AC may apply to only 18% of cases, and we identified subsets of more likely candidates.\n",
      "WARNING: No sentence cosine similarities calculated for text: Patients with a history of ACD are at an increased risk of having recurrent preterm birth and cervical shortening in a subsequent pregnancy compared with women with prior preterm birth associated PPROM or PTL.\n",
      "WARNING: No sentence cosine similarities calculated for text: This study demonstrates that a significant eosinophilic inflammation is present across all categories of asthma, and that paucigranulocytic asthma may be seen as a low grade inflammatory disease.\n",
      "WARNING: No sentence cosine similarities calculated for text: HIV/STD control measures appear to have slowed the HIV/AIDS epidemic in Jamaica, however a significant minority of persons continue to have unprotected sex in high risk situations.\n",
      "WARNING: No sentence cosine similarities calculated for text: The elevated risk of death after CABG surgery known previously to be associated with CDM seems also to be shared by a group of similar size that includes patients with IFG and undiagnosed DM.\n",
      "WARNING: No sentence cosine similarities calculated for text: Upfront evaluation of kit mutation status may help us in delineating separate treatment strategies for potentially biologically different tumours and assessing the correct timing of surgery for this subset of GIST.\n",
      "WARNING: No sentence cosine similarities calculated for text: Our data, derived from patients with coronary artery disease, support the hypothesis regarding a possible preventive effect of bezafibrate on the development of colon cancer.\n",
      "WARNING: No sentence cosine similarities calculated for text: The use of contaminated products with antibiotic prophylaxis may be safe in terms of the first day of fever, duration of fever, neutrophil, platelet engraftment and duration of hospitalization.\n",
      "WARNING: No sentence cosine similarities calculated for text: The rabbit is a good model to be used in training of surgery, with a low morbi-mortality, able to be anesthetized intramuscularly, with no need of pre-operative fasting and does not present hypoglycemia even with the extended fasting period.\n",
      "WARNING: No sentence cosine similarities calculated for text: We conclude that beta1Gly49 homozygosity and TACC haplotype of ADRB2 gene, both loss-of-function genetic variations, may predispose to TTN.\n",
      "WARNING: No sentence cosine similarities calculated for text: The shaving of the incision site immediately before spinal surgery may increase the rate of postoperative infection.\n",
      "WARNING: No sentence cosine similarities calculated for text: The results of this study do not support the hypothesis that there is a direct link between atmospheric pressure values and abdominal aortic aneurysm ruptures.\n",
      "WARNING: No sentence cosine similarities calculated for text: Most people in France are influenced by situational factors when deciding if a physician should breach confidentiality to protect the spouse of a patient infected with STD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No sentence cosine similarities calculated for text: This clinical trial evaluated the possible reduction in intervention-related myocardial damage that was attributable to remote postischemic conditioning.\n",
      "WARNING: No sentence cosine similarities calculated for text: The leukocyte count at presentation can be used as an adjunct in the evaluation of the severity of injury in blunt trauma patients.\n",
      "WARNING: No sentence cosine similarities calculated for text: a-tDCS could be useful in identifying residual connectivity markers in clinically-defined UWS, who may lack of purposeful behavior as a result of a motor-output failure.\n",
      "WARNING: No sentence cosine similarities calculated for text: Tracheobronchoscopy is a useful and safe procedure and should be recommended in tertiary centers for babies with EA before surgical repair.\n",
      "WARNING: No sentence cosine similarities calculated for text: Otolith organs input influences the axis of horizontal semicircular canal ocular reflex; therefore, the plane of compensatory eye movements induced by the horizontal canal stimulation is not always parallel to the canal.\n",
      "WARNING: No sentence cosine similarities calculated for text: It seems quite admissible and justified to encourage medical students to officiate as examiners in undergraduate emergency medicine OSCE formative testing, but not necessarily in summative assessment evaluations.\n",
      "WARNING: No sentence cosine similarities calculated for text: Evaluation of astrocytomas utilizing antibody to vWF and confocal microscopy aids in the grading of these neoplasms.\n",
      "WARNING: No sentence cosine similarities calculated for text: In this in vitro cadaver study, the tricompartmental replacement significantly changed knee kinematics while the unicompartmental replacement preserved normal knee kinematics.\n",
      "WARNING: No sentence cosine similarities calculated for text: Fetal gender does not affect the risk of having an ED visit for asthma during pregnancy, and it is not associated with adverse pregnancy outcomes among women who had an asthma-related ED during pregnancy.\n",
      "WARNING: No sentence cosine similarities calculated for text: Post-mastectomy radiotherapy should be discussed for a sub-group of node-negative patients with predictors factors of local failure such as age<or = 40 years and larger tumour size.\n",
      "WARNING: No sentence cosine similarities calculated for text: (1) There is little overlap between regions of CFAEs during AF and regions of SRF measured in the time domain or the frequency domain, (2) the majority of SRF appears to occur in regions with wave-front collision, (3) the distribution of SRF is similar in patients with AF and normal controls, suggesting that this may not have an important role in AF maintenance and may not be a suitable ablation target.\n",
      "WARNING: No sentence cosine similarities calculated for text: Premature births from rural mothers have a higher risk of stillbirth and mortality in neonatal intensive care than urban infants.\n",
      "WARNING: No sentence cosine similarities calculated for text: Better healing, shorter AT, and equal strength were achieved with ethyl-2-cyanoacrylate compared with polyglactin 910 sutures in intestinal anastomosis in the experimental setting.\n",
      "WARNING: No sentence cosine similarities calculated for text: Cancellous bone plays an important role in primary stability of mini-implants in the presence or absence of cortical bone.\n",
      "WARNING: No sentence cosine similarities calculated for text: On the whole, the data confirm an absence of a July effect in patients undergoing major cancer surgery.\n",
      "WARNING: No sentence cosine similarities calculated for text: A routinely inserted ENBD tube did not improve the clinical course, despite patients having to endure increased procedure time and discomfort, and the insertion would therefore be unnecessary.\n",
      "WARNING: No sentence cosine similarities calculated for text: This study that shows that adequate femoral tunnel lengths can be safely created without knee hyperflex - ion using flexible instruments via an anteromedial portal.\n",
      "WARNING: No sentence cosine similarities calculated for text: Percutaneous ethanol injection without aspiration of ethanol-mixed fluid seems to be the preferable method of treatment of benign cystic thyroid nodules from the perspective of both the physician and the patient.\n",
      "WARNING: No sentence cosine similarities calculated for text: The association between plasma glucose levels and CVD risk is mainly explained by insulin resistance, which raises the question of whether glucose lowering per se without changes in the processes that underlie hyperglycemia should be the sole clinical paradigm in the treatment of type 2 diabetes or its prevention.\n",
      "WARNING: No sentence cosine similarities calculated for text: Analyzing the prevalence of 11 chronic conditions by using Medicare claims data provides a monitoring tool that can guide health care providers and policy makers in devising strategies to address chronic conditions and rising health care costs.\n",
      "WARNING: No sentence cosine similarities calculated for text: Completion of the obstetric manual of the PEP improved the knowledge of the midwives but no alteration in practice was detected.\n",
      "WARNING: No sentence cosine similarities calculated for text: While music did not significantly reduce cortisol, less profound spikes in UFC levels were observed but that, given the limitations of the research, this observation could have occurred merely by chance.\n",
      "WARNING: No sentence cosine similarities calculated for text: Using a targeted meta-analytic approach, it is possible to demonstrate that reducing spasticity in the arm is associated with a significant improvement in arm function.\n",
      "WARNING: No sentence cosine similarities calculated for text: The Holmium:YAG laser is an ideal intracorporeal lithotripter for ureteral calculi, with a high success rate and low morbidity.\n",
      "WARNING: No sentence cosine similarities calculated for text: These data suggest that CIN and VAIN may have some common features in certain cases, i.e., if an HPV infection is proved.\n",
      "WARNING: No sentence cosine similarities calculated for text: General practitioners should consider using patients' first names more often, particularly with younger patients.\n",
      "WARNING: No sentence cosine similarities calculated for text: These results indicate that prophylactic treatment with edaravone prevents I/R-induced ovarian damage during pneumoperitoneum in an experimental rat model.\n"
     ]
    }
   ],
   "source": [
    "y = prepare_data_for_regression(data,'pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cae9dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94907900",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
