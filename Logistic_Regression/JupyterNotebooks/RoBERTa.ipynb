{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4ac8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n",
    "\n",
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail']\n",
    "DATA_PATH = '../data/writingPrompts'\n",
    "NUM_EXAMPLES = 150\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' ’ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories[summaries/articles] from the dataset\")  # debug\n",
    "        # print\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "        Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of data to be converted to CSV.\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            directory (str, optional): Name of the directory to save the CSV file. Defaults to 'Labelled_Data'.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)\n",
    "\n",
    "\n",
    "def combine_datasets(datasets=DATASETS, extract_prompts=False, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Combines data from multiple datasets into a single dataset. If specified, extracts prompts based on dataset names,\n",
    "    and saves the result to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory: Where the file will be saved\n",
    "        datasets (list, optional): List of datasets to combine. Defaults to DATASETS.\n",
    "        extract_prompts (bool, optional): Whether to extract prompts from the combined data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the combined data\n",
    "    combined_data = []\n",
    "\n",
    "    # If specified, also store the extracted prompts\n",
    "    extracted_prompts = [] if extract_prompts else None\n",
    "\n",
    "    # Load and preprocess data from each dataset\n",
    "    for dataset in datasets:\n",
    "        data = preprocess_data(dataset)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "        # If specified, extract prompts\n",
    "        if extract_prompts:\n",
    "            extracted_prompts.extend(extract_prompt(data, dataset))\n",
    "\n",
    "    # Shuffle the combined data to ensure a mix of data from all datasets\n",
    "    # random.shuffle(combined_data)\n",
    "    # random.shuffle(extracted_prompts) if extract_prompts else None\n",
    "\n",
    "    # Save the combined data to a CSV file\n",
    "    convert_to_csv(combined_data, 'combined')\n",
    "\n",
    "    # If specified, save the extracted prompts to a CSV file\n",
    "    if extract_prompts:\n",
    "        df = pd.DataFrame(extracted_prompts, columns=['text'])\n",
    "        df.to_csv(f'{directory}/prompts.csv', index=False)\n",
    "\n",
    "\n",
    "def extract_prompt(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts the prompts from a preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): Preprocessed data.\n",
    "        dataset_name (str): Name of the dataset the data is from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        prompts = [text.split('Answer:')[0] + 'Answer:' for text, label in data]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        # Split the text into article and summary, then only append the summary\n",
    "        prompts = [\n",
    "            'Write a news article based on the following summary: ' + text.split('Summary:')[1].split('Article:')[\n",
    "                0].strip() for text, label in data]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        prompts = [text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:' for text, label in\n",
    "                   data]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def token_count(csv_files):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Load prompts from CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        prompts = df['text'].tolist()\n",
    "\n",
    "        # Initialize a counter for total tokens\n",
    "        total_tokens = 0\n",
    "\n",
    "        for prompt in prompts:\n",
    "            num_tokens = len(encoding.encode(prompt))\n",
    "            total_tokens += num_tokens\n",
    "\n",
    "        print(f\"File '{csv_file}' has {total_tokens} tokens.\")\n",
    "\n",
    "        # Estimate cost\n",
    "        if csv_file == 'Labelled_Data/prompts.csv':\n",
    "            cost = (total_tokens / 1000) * 0.003\n",
    "            print(f\"Estimated cost for '{csv_file}' is ${cost:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d393a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4be66aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def remove_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Strip the 'Answer:' prefix'\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() for text in texts]  # Stripping the 'Article: ' string\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def average_token_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Calculates the average number of tokens in the answers of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of tokens in the answers of a dataset\n",
    "    \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    average_tokens = total_tokens / len(texts)\n",
    "\n",
    "    return average_tokens\n",
    "\n",
    "#PUBMED = 54\n",
    "#WP = 780\n",
    "#CNN = 794\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def prepare_data_for_regression(data, dataset_name):\n",
    "    \"\"\"\n",
    "       This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "       Args:\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       feature_matrix (DataFrame): A DataFrame where each row represents a text and each column represents a feature.\n",
    "       label_vector (Series): A Series where each element is the label of a text.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store features and labels\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Remove prefixes\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Count POS tags in the text\n",
    "        pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "        # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "        flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "        # Calculate the perplexity of the text and average sentence perplexity\n",
    "        text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "        sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in nlp(text).sents]\n",
    "        sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "        avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "            sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "        # Prepare a dictionary to append to the feature list\n",
    "        features = {**pos_counts, **punctuation_counts, **function_word_counts,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length, 'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity, 'avg_sentence_perplexity': avg_sentence_perplexity}\n",
    "\n",
    "        # Add the feature dictionary and the label to their respective lists\n",
    "        feature_list.append(features)\n",
    "        label_list.append(label)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    feature_matrix = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "    # Convert the list of labels into a Series\n",
    "    label_vector = pd.Series(label_list)\n",
    "\n",
    "    return feature_matrix, label_vector\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "#     model_name = 'allenai/scibert_scivocab_uncased'\n",
    "#     model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text if it's too long for the model\n",
    "        input_ids = input_ids[:, :model.config.max_position_embeddings]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Exception in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "   \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n",
    "\n",
    "def plot_perplexities(sentence_perplexities, text_perplexities):\n",
    "    \"\"\"\n",
    "    Plots Kernel Density Estimates of the sentence and text perplexities.\n",
    "\n",
    "    Args:\n",
    "    sentence_perplexities (list of float): The perplexities of the sentences.\n",
    "    text_perplexities (list of float): The perplexities of the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot sentence perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(sentence_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 12)  # Limit x-axis to 12 for sentence perplexity\n",
    "    plt.show()\n",
    "\n",
    "    # Plot text perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(text_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Text Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 10)  # Limit x-axis to 10 for text perplexity\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b4f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372b31fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6578ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 questions from the dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043152dd1755425ba06c9e2892452503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atana\\anaconda3\\envs\\MScProject\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\atana\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225226855a174566bb820973b49e60cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e25346dc3446be99105a54fb3e55bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd0a8543be84db5a459e86304324787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess_data('pubmed_qa')\n",
    "\n",
    "x = summary_statistics('pubmed_qa', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cc52d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 914\n",
      "Frequency of adverbs: 205\n",
      "Frequency of conjunctions: 216\n",
      "Frequency of nouns: 1998\n",
      "Frequency of numbers: 40\n",
      "Frequency of pronouns: 142\n",
      "Frequency of verbs: 632\n",
      "Frequency of commas: 202\n",
      "Frequency of fullstops: 308\n",
      "Frequency of special character '-': 82\n",
      "Frequency of function word 'a': 124\n",
      "Frequency of function word 'in': 169\n",
      "Frequency of function word 'of': 283\n",
      "Frequency of function word 'the': 253\n",
      "Average Flesch Reading Ease: 29.3832\n",
      "Average Flesch-Kincaid Grade Level: 14.416\n",
      "Average word length: 5.654920832039739\n",
      "Average sentence length: 22.97087378640777\n",
      "Average sentence perplexity: 1.084736425899765\n",
      "Average text perplexity: 1.0907774806022643\n"
     ]
    }
   ],
   "source": [
    "print_statistics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b9610e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 125 prompts/stories[summaries/articles] from the dataset\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e27b86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = summary_statistics('pubmed_qa', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48d99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 914\n",
      "Frequency of adverbs: 205\n",
      "Frequency of conjunctions: 216\n",
      "Frequency of nouns: 1998\n",
      "Frequency of numbers: 40\n",
      "Frequency of pronouns: 142\n",
      "Frequency of verbs: 632\n",
      "Frequency of commas: 202\n",
      "Frequency of fullstops: 308\n",
      "Frequency of special character '-': 82\n",
      "Frequency of function word 'a': 124\n",
      "Frequency of function word 'in': 169\n",
      "Frequency of function word 'of': 283\n",
      "Frequency of function word 'the': 253\n",
      "Average Flesch Reading Ease: 29.3832\n",
      "Average Flesch-Kincaid Grade Level: 14.416\n",
      "Average word length: 5.654920832039739\n",
      "Average sentence length: 22.97087378640777\n",
      "Average sentence perplexity: 1.084736425899765\n",
      "Average text perplexity: 1.0907774806022643\n"
     ]
    }
   ],
   "source": [
    "print_statistics(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453f711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "302f2373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 125 prompts/stories[summaries/articles] from the dataset\n"
     ]
    }
   ],
   "source": [
    "data2 = preprocess_data('writingprompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7994f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n",
      "Exception in calculate_perplexity: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "x2 = summary_statistics('writingprompts', data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6627ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of adjectives: 4857\n",
      "Frequency of adverbs: 4771\n",
      "Frequency of conjunctions: 2615\n",
      "Frequency of nouns: 13559\n",
      "Frequency of numbers: 558\n",
      "Frequency of pronouns: 12770\n",
      "Frequency of verbs: 12037\n",
      "Frequency of commas: 4604\n",
      "Frequency of fullstops: 5440\n",
      "Frequency of special character '-': 272\n",
      "Frequency of function word 'a': 1640\n",
      "Frequency of function word 'in': 899\n",
      "Frequency of function word 'of': 1524\n",
      "Frequency of function word 'the': 3500\n",
      "Average Flesch Reading Ease: 75.1044\n",
      "Average Flesch-Kincaid Grade Level: 8.3568\n",
      "Average word length: 4.122463713048104\n",
      "Average sentence length: 14.347865714719733\n",
      "Average sentence perplexity: 1.1123606655964569\n",
      "Average text perplexity: 1.096551609666724\n"
     ]
    }
   ],
   "source": [
    "print_statistics(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e34e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
