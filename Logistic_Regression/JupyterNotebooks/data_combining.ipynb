{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c1848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail']\n",
    "DATA_PATH = '../data/writingPrompts'\n",
    "NUM_EXAMPLES = 150\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def process_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [process_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [process_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories[summaries/articles] from the dataset\")  # debug\n",
    "        # print\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "        Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of data to be converted to CSV.\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            directory (str, optional): Name of the directory to save the CSV file. Defaults to 'Labelled_Data'.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)\n",
    "\n",
    "\n",
    "def combine_datasets(datasets=DATASETS, extract_prompts=False, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Combines data from multiple datasets into a single dataset. If specified, extracts prompts based on dataset names,\n",
    "    and saves the result to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory: Where the file will be saved\n",
    "        datasets (list, optional): List of datasets to combine. Defaults to DATASETS.\n",
    "        extract_prompts (bool, optional): Whether to extract prompts from the combined data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the combined data\n",
    "    combined_data = []\n",
    "\n",
    "    # If specified, also store the extracted prompts\n",
    "    extracted_prompts = [] if extract_prompts else None\n",
    "\n",
    "    # Load and preprocess data from each dataset\n",
    "    for dataset in datasets:\n",
    "        data = preprocess_data(dataset)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "        # If specified, extract prompts\n",
    "        if extract_prompts:\n",
    "            extracted_prompts.extend(extract_prompt(data, dataset))\n",
    "\n",
    "    # Shuffle the combined data to ensure a mix of data from all datasets\n",
    "    # random.shuffle(combined_data)\n",
    "    # random.shuffle(extracted_prompts) if extract_prompts else None\n",
    "\n",
    "    # Save the combined data to a CSV file\n",
    "    convert_to_csv(combined_data, 'combined')\n",
    "\n",
    "    # If specified, save the extracted prompts to a CSV file\n",
    "    if extract_prompts:\n",
    "        df = pd.DataFrame(extracted_prompts, columns=['text'])\n",
    "        df.to_csv(f'{directory}/prompts.csv', index=False)\n",
    "\n",
    "\n",
    "def extract_prompt(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts the prompts from a preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): Preprocessed data.\n",
    "        dataset_name (str): Name of the dataset the data is from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        prompts = [text.split('Answer:')[0] + 'Answer:' for text, label in data]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        # Split the text into article and summary, then only append the summary\n",
    "        prompts = [\n",
    "            'Write a news article based on the following summary: ' + text.split('Summary:')[1].split('Article:')[\n",
    "                0].strip() for text, label in data]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        prompts = [text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:' for text, label in data]\n",
    "    return prompts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 questions from the dataset\n",
      "Loaded and pre-processed 125 prompts/stories[summaries/articles] from the dataset\n"
     ]
    }
   ],
   "source": [
    "combine_datasets(extract_prompts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b87d6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual length: 414\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# After combining the datasets into a single DataFrame\n",
    "combined_df = pd.read_csv(\"Labelled_Data/prompts.csv\")\n",
    "\n",
    "\n",
    "# Check if combined_df contains all entries from the datasets\n",
    "actual_length = len(combined_df)\n",
    "\n",
    "print(f\"Actual length: {actual_length}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11fb66ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 150 questions from the dataset\n"
     ]
    }
   ],
   "source": [
    "x= preprocess_data('pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebc87a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Syncope during bathing in infants, a pediatric form of water-induced urticaria? Answer: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n"
     ]
    }
   ],
   "source": [
    "print(x[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c8df73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
