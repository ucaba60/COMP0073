{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034321ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "\n",
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n",
    "\n",
    "\n",
    "def remove_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Strip the 'Answer:' prefix'\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() for text in texts]  # Stripping the 'Article: ' string\n",
    "    elif dataset_name == 'gpt':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() if \"Story:\" in text else text for text in texts]\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() if \"Article:\" in text else text for text in texts]\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def average_token_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Calculates the average number of tokens in the answers of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of tokens in the answers of a dataset\n",
    "    \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    average_tokens = total_tokens / len(texts)\n",
    "\n",
    "    return average_tokens\n",
    "\n",
    "\n",
    "# PUBMED = 54\n",
    "# WP = 780\n",
    "# CNN = 794\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def prepare_data_for_regression(data, dataset_name):\n",
    "    \"\"\"\n",
    "       This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "       Args:\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       feature_matrix (DataFrame): A DataFrame where each row represents a text and each column represents a feature.\n",
    "       label_vector (Series): A Series where each element is the label of a text.\n",
    "    \"\"\"\n",
    "    # Initialize lists to store features and labels\n",
    "    feature_list = []\n",
    "    label_list = []\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Remove prefixes\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    for text, label in zip(texts, labels):\n",
    "        # Count POS tags in the text\n",
    "        pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "        # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "        flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "        # Calculate the average word length\n",
    "        avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "        # Calculate the average sentence length\n",
    "        avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "        # Calculate the perplexity of the text and average sentence perplexity\n",
    "        text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "        sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in nlp(text).sents]\n",
    "        sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "        avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "            sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "        # Prepare a dictionary to append to the feature list\n",
    "        features = {**pos_counts, **punctuation_counts, **function_word_counts,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length, 'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity, 'avg_sentence_perplexity': avg_sentence_perplexity}\n",
    "\n",
    "        # Add the feature dictionary and the label to their respective lists\n",
    "        feature_list.append(features)\n",
    "        label_list.append(label)\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    feature_matrix = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "    # Convert the list of labels into a Series\n",
    "    label_vector = pd.Series(label_list)\n",
    "\n",
    "    return feature_matrix, label_vector\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text if it's too long for the model\n",
    "        input_ids = input_ids[:, :model.config.max_position_embeddings]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Tokens > 512, ignoring... in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "   \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }\n",
    "\n",
    "\n",
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n",
    "\n",
    "\n",
    "def plot_perplexities(sentence_perplexities, text_perplexities):\n",
    "    \"\"\"\n",
    "    Plots Kernel Density Estimates of the sentence and text perplexities.\n",
    "\n",
    "    Args:\n",
    "    sentence_perplexities (list of float): The perplexities of the sentences.\n",
    "    text_perplexities (list of float): The perplexities of the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot sentence perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(sentence_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 12)  # Limit x-axis to 12 for sentence perplexity\n",
    "    plt.show()\n",
    "\n",
    "    # Plot text perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(text_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Text Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 10)  # Limit x-axis to 10 for text perplexity\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4966e4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n",
    "\n",
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail','gpt']\n",
    "DATA_PATH = '../data/writingPrompts'\n",
    "NUM_EXAMPLES = 200\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n",
    "\n",
    "\n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n",
    "\n",
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' ’ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories[summaries/articles] from the dataset\")  # debug\n",
    "        # print\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "        Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of data to be converted to CSV.\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            directory (str, optional): Name of the directory to save the CSV file. Defaults to 'Labelled_Data'.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)\n",
    "\n",
    "\n",
    "def combine_datasets(datasets=DATASETS, extract_prompts=False, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Combines data from multiple datasets into a single dataset. If specified, extracts prompts based on dataset names,\n",
    "    and saves the result to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory: Where the file will be saved\n",
    "        datasets (list, optional): List of datasets to combine. Defaults to DATASETS.\n",
    "        extract_prompts (bool, optional): Whether to extract prompts from the combined data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the combined data\n",
    "    combined_data = []\n",
    "\n",
    "    # If specified, also store the extracted prompts\n",
    "    extracted_prompts = [] if extract_prompts else None\n",
    "\n",
    "    # Load and preprocess data from each dataset\n",
    "    for dataset in datasets:\n",
    "        data = preprocess_data(dataset)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "        # If specified, extract prompts\n",
    "        if extract_prompts:\n",
    "            extracted_prompts.extend(extract_prompt(data, dataset))\n",
    "\n",
    "    # Shuffle the combined data to ensure a mix of data from all datasets\n",
    "    # random.shuffle(combined_data)\n",
    "    # random.shuffle(extracted_prompts) if extract_prompts else None\n",
    "\n",
    "    # Save the combined data to a CSV file\n",
    "    convert_to_csv(combined_data, 'combined_human')\n",
    "\n",
    "    # If specified, save the extracted prompts to a CSV file\n",
    "    if extract_prompts:\n",
    "        df = pd.DataFrame(extracted_prompts, columns=['text'])\n",
    "        df.to_csv(f'{directory}/prompts.csv', index=False)\n",
    "\n",
    "\n",
    "def extract_prompt(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts the prompts from a preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): Preprocessed data.\n",
    "        dataset_name (str): Name of the dataset the data is from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        prompts = [text.split('Answer:')[0] + 'Answer:' for text, label in data]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        # Split the text into article and summary, then only append the summary\n",
    "        prompts = [\n",
    "            'Write a news article based on the following summary: ' + text.split('Summary:')[1].split('Article:')[\n",
    "                0].strip() for text, label in data]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        prompts = [text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:' for text, label in\n",
    "                   data]\n",
    "    return prompts\n",
    "\n",
    "\n",
    "def token_count(csv_files):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_files (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Load prompts from CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        prompts = df['text'].tolist()\n",
    "\n",
    "        # Initialize a counter for total tokens\n",
    "        total_tokens = 0\n",
    "\n",
    "        for prompt in prompts:\n",
    "            num_tokens = len(encoding.encode(prompt))\n",
    "            total_tokens += num_tokens\n",
    "\n",
    "        print(f\"File '{csv_file}' has {total_tokens} tokens.\")\n",
    "\n",
    "        # Estimate cost\n",
    "        if csv_file == 'Labelled_Data/prompts.csv':\n",
    "            cost = (total_tokens / 1000) * 0.003\n",
    "            print(f\"Estimated cost for '{csv_file}' is ${cost:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c26548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "854c34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Labelled_Data/t1_preprocessed.csv\")\n",
    "\n",
    "# Create a list of tuples\n",
    "data_gpt = [(row['Text'], row['Label']) for index, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c3baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_gpt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c53990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 200 questions from the dataset\n"
     ]
    }
   ],
   "source": [
    "data_pub = preprocess_data('pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b689331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Question: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death? Answer: Results depicted mitochondrial dynamics in vivo as PCD progresses within the lace plant, and highlight the correlation of this organelle with other organelles during developmental PCD. To the best of our knowledge, this is the first report of mitochondria and chloroplasts moving on transvacuolar strands to form a ring structure surrounding the nucleus during developmental PCD. Also, for the first time, we have shown the feasibility for the use of CsA in a whole plant system. Overall, our findings implicate the mitochondria as playing a critical and early role in developmentally regulated PCD in the lace plant.', 0)\n"
     ]
    }
   ],
   "source": [
    "print(data_pub[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e0fde21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 170 prompts/stories[summaries/articles] from the dataset\n"
     ]
    }
   ],
   "source": [
    "data_write = preprocess_data('writingprompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7771f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Prompt: Every person in the world undergoes a `` goodness \\'\\' test. It \\'s designed to give a score from 1 to 200, where 1 is pure evil, and 200 is an angel in human body. Then the world is divided into 200 zones, where people can live among their own kind. Story: Clancy Marguerian, 154, private first class of the 150+ army, sits in his foxhole. Tired cold, wet and hungry, the only thing preventing him from laying down his rifle and walking towards the enemy lines in surrender is the knowledge that however bad he has it here, life as a 50-100 POW is surely much worse. He\\'s fighting to keep his eyes open and his rifle ready when the mortar shells start landing near him. He hunkers lower. After a few minutes under the barrage, Marguerian hears hurried footsteps, a grunt, and a thud as a soldier leaps into the foxhole. The man\\'s uniform is tan, he must be a 50-100. The two men snarl and grab at eachother, grappling in the small foxhole. Abruptly, their faces come together. \"Clancy?\" \"Rob?\" Rob Hall, 97, Corporal in the 50-100 army grins, as the situation turns from life or death struggle, to a meeting of two college friends. He lets go of Marguerian\\'s collar. \"Holy shit Clancy, you\\'re the last person I expected to see here\" \"Yeah\" \"Shit man, I didn\\'t think I\\'d ever see\\'Mr. volunteers every saturday morning at the food shelf\\', not after The Reorganization at least\" \"Yeah Rob, it is something isn\\'t it\" \"Man, I\\'m sorry I tried to kill you there, hey, I heard you guys were out of food, here, you can share my dinner\" Clancy marvels, even after all this: The Reorganization, the coalitions, the war, Rob is still his old, chatty self. The two men sit, Rob chatting away, Clancy forcing out pleasantries. They pass Rob\\'s rations between them. \"Clancy my man, I heard a group of terrorist 5\\'s took have formed some kind of cult, and they\\'re rallying all the < 50 in their own coalition\" \"Oh yeah?\" \"Yeah, I mean, that sucks and everything, cause those are some scary dudes, but I heard that there\\'s going to be a truce between our countries in a few days, why don\\'t we just hang out here, pretty soon we won\\'t even be enemies anymore!\" \"Yeah, Rob, that sounds like a plan\" \"Man, I\\'m so glad I found you again, in a few days, this war will be over, and things will be cool between us and, hey, remember Sarah? I heard she\\'s a 151, maybe I\\'ll look her up, I\\'ll be sure to visit you too once I can get a pass to sector 150-155, it\\'ll probably be tough though, even before the war, you had to do sooo much paperwork to be allowed to visit, I wonder if passes will even be reinstated after the truce ends, hey, did I ever tell you about the time...\" Rob babbles as he dozes off, grinning up at Clancy. When Clancy is sure that his friend is asleep, he slits Rob\\'s throat with his bayonet. Clancy climbs out of the foxhole, and stumbles his way back to battalion HQ.', 0)\n"
     ]
    }
   ],
   "source": [
    "print(data_write[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74337606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 184 prompts/stories[summaries/articles] from the dataset\n"
     ]
    }
   ],
   "source": [
    "data_cnn = preprocess_data('cnn_dailymail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8887d86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Summary: Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 Monday. Young actor says he has no plans to fritter his cash away. Radcliffe\\'s earnings from first five Potter films have been held in trust fund. Article: Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\\'s latest ». There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters.', 0)\n"
     ]
    }
   ],
   "source": [
    "print(data_cnn[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0688e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pub,label_pub = prepare_data_for_regression(data_pub,'pubmed_qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c294b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8635f186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (658 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "feature_write,label_write = prepare_data_for_regression(data_write,'writingprompts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b6088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "062f71ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (527 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "feature_cnn,label_cnn = prepare_data_for_regression(data_cnn,'cnn_dailymail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e4ed75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b589da4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (645 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n",
      "Tokens > 512, ignoring... in calculate_perplexity: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "feature_gpt,label_gpt = prepare_data_for_regression(data_gpt,'gpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde63991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fb8cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.concat([feature_pub, feature_write, feature_cnn,feature_gpt], ignore_index=True)\n",
    "target_vector = pd.concat([label_pub, label_write, label_cnn,label_gpt], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68118914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e6327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5702d2d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m logreg \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m) \u001b[38;5;66;03m# increase max_iter if the model doesn't converge\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mlogreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Use the model to make predictions on the testing data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logreg\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1196\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_matrix, target_vector, test_size=0.2, random_state=0)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression(max_iter=1000) # increase max_iter if the model doesn't converge\n",
    "\n",
    "# Train the model on the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Use the model to make predictions on the testing data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "# Creating the confusion matrix\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix: \\n', confusion_matrix)\n",
    "\n",
    "# Creating the classification report\n",
    "classification_report = metrics.classification_report(y_test, y_pred)\n",
    "print('Classification Report: \\n', classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e01d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
