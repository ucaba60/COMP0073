{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02998489",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary Classification) of AI/Human-Generated Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382880bf",
   "metadata": {},
   "source": [
    "## First Goal: Gather Appropriate Labelled Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fccd25",
   "metadata": {},
   "source": [
    "### Choice of Datasets: PubMed (Scientific), WritingPrompts(Fiction), CNN_DailyMail(News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58c23de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Dependencies\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c074473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail']\n",
    "DATA_PATH = '../data/writingPrompts' \n",
    "NUM_EXAMPLES = 150 #Limiting to 150 samples MAX from each dataset due to resource constraints\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]'] #Need to remove those from WritingPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "263321ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions \n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6e7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to load each dataset, because their format is different.\n",
    "#Goal is to have the three datasets in the fromat: [Prompt] [Response]\n",
    "#Where [Prompt] will be used on GPT and Response is the ground truth\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4aa394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb5162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b007aea",
   "metadata": {},
   "source": [
    "## The preprocessing function called to get the data in the format we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ccc5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories[summaries/articles] from the dataset\")  # debug\n",
    "        # print\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47129a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "        Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of data to be converted to CSV.\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            directory (str, optional): Name of the directory to save the CSV file. Defaults to 'Labelled_Data'.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b736f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want the prompts only in order to feed them to LLM .\n",
    "def extract_prompt(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts the prompts from a preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): Preprocessed data.\n",
    "        dataset_name (str): Name of the dataset the data is from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        prompts = [text.split('Answer:')[0] + 'Answer:' for text, label in data]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        # Split the text into article and summary, then only append the summary\n",
    "        prompts = [\n",
    "            'Write a news article based on the following summary: ' + text.split('Summary:')[1].split('Article:')[\n",
    "                0].strip() for text, label in data]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        prompts = [text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:' for text, label in\n",
    "                   data]\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd4fffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commbines the three datasets into a single .csv file.\n",
    "#Additional Parameter for extraction of [Prompts] only\n",
    "def combine_datasets(datasets=DATASETS, extract_prompts=False, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Combines data from multiple datasets into a single dataset. If specified, extracts prompts based on dataset names,\n",
    "    and saves the result to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory: Where the file will be saved\n",
    "        datasets (list, optional): List of datasets to combine. Defaults to DATASETS.\n",
    "        extract_prompts (bool, optional): Whether to extract prompts from the combined data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the combined data\n",
    "    combined_data = []\n",
    "\n",
    "    # If specified, also store the extracted prompts\n",
    "    extracted_prompts = [] if extract_prompts else None\n",
    "\n",
    "    # Load and preprocess data from each dataset\n",
    "    for dataset in datasets:\n",
    "        data = preprocess_data(dataset)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "        # If specified, extract prompts\n",
    "        if extract_prompts:\n",
    "            extracted_prompts.extend(extract_prompt(data, dataset))\n",
    "\n",
    "    # Shuffle the combined data to ensure a mix of data from all datasets\n",
    "    # random.shuffle(combined_data)\n",
    "    # random.shuffle(extracted_prompts) if extract_prompts else None\n",
    "\n",
    "    # Save the combined data to a CSV file\n",
    "    convert_to_csv(combined_data, 'combined')\n",
    "\n",
    "    # If specified, save the extracted prompts to a CSV file\n",
    "    if extract_prompts:\n",
    "        df = pd.DataFrame(extracted_prompts, columns=['text'])\n",
    "        df.to_csv(f'{directory}/prompts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66a8689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get estimated token count using TIKTOKEN, provided by OpenAI for accurate token count\n",
    "def token_count(csv_files):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Load prompts from CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        prompts = df['text'].tolist()\n",
    "\n",
    "        # Initialize a counter for total tokens\n",
    "        total_tokens = 0\n",
    "\n",
    "        for prompt in prompts:\n",
    "            num_tokens = len(encoding.encode(prompt))\n",
    "            total_tokens += num_tokens\n",
    "\n",
    "        print(f\"File '{csv_file}' has {total_tokens} tokens.\")\n",
    "\n",
    "        # Estimate cost\n",
    "        if csv_file == 'Labelled_Data/prompts.csv':\n",
    "            cost = (total_tokens / 1000) * 0.003\n",
    "            print(f\"Estimated cost for '{csv_file}' is ${cost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c92de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
