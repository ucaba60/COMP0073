{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e746851",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary Classification) of AI/Human-Generated Text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e894379",
   "metadata": {},
   "source": [
    "## First Goal: Gather Appropriate Labelled Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d9db1b",
   "metadata": {},
   "source": [
    "### Choice of Datasets: PubMed (Scientific), WritingPrompts(Fiction), CNN_DailyMail(News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5039eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Dependencies\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b93eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail']\n",
    "DATA_PATH = '../data/writingPrompts' \n",
    "NUM_EXAMPLES = 150 #Limiting to 150 samples MAX from each dataset due to resource constraints\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]'] #Need to remove those from WritingPrompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2f6a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions \n",
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cacfa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to load each dataset, because their format is different.\n",
    "#Goal is to have the three datasets in the fromat: [Prompt] [Response]\n",
    "#Where [Prompt] will be used on GPT and Response is the ground truth\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12782669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "267ec0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_name):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada5d8ac",
   "metadata": {},
   "source": [
    "## The preprocessing function called to get the data in the format we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9dcc63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "    if dataset == 'pubmed_qa':\n",
    "        print(f\"Loaded and pre-processed {len(data)} questions from the dataset\")  # debug print\n",
    "\n",
    "    # Getting long-enough prompts, can do the same for the articles as well\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} prompts/stories[summaries/articles] from the dataset\")  # debug\n",
    "        # print\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db30c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_csv(data, dataset_name, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "        Converts the data to a DataFrame and saves it to a CSV file in the specified directory.\n",
    "\n",
    "        Args:\n",
    "            data (list): List of data to be converted to CSV.\n",
    "            dataset_name (str): Name of the dataset.\n",
    "            directory (str, optional): Name of the directory to save the CSV file. Defaults to 'Labelled_Data'.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Convert data to DataFrame\n",
    "    df = pd.DataFrame(data, columns=['text', 'label'])\n",
    "\n",
    "    # Write DataFrame to CSVv\n",
    "    df.to_csv(f'{directory}/{dataset_name}_Human_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Want the prompts only in order to feed them to LLM .\n",
    "def extract_prompt(data, dataset_name):\n",
    "    \"\"\"\n",
    "    Extracts the prompts from a preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        data (list): Preprocessed data.\n",
    "        dataset_name (str): Name of the dataset the data is from.\n",
    "\n",
    "    Returns:\n",
    "        list: List of extracted prompts.\n",
    "    \"\"\"\n",
    "    prompts = []\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        prompts = [text.split('Answer:')[0] + 'Answer:' for text, label in data]\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        # Split the text into article and summary, then only append the summary\n",
    "        prompts = [\n",
    "            'Write a news article based on the following summary: ' + text.split('Summary:')[1].split('Article:')[\n",
    "                0].strip() for text, label in data]\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        prompts = [text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:' for text, label in\n",
    "                   data]\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a107c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commbines the three datasets into a single .csv file.\n",
    "#Additional Parameter for extraction of [Prompts] only\n",
    "def combine_datasets(datasets=DATASETS, extract_prompts=False, directory='Labelled_Data'):\n",
    "    \"\"\"\n",
    "    Combines data from multiple datasets into a single dataset. If specified, extracts prompts based on dataset names,\n",
    "    and saves the result to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        directory: Where the file will be saved\n",
    "        datasets (list, optional): List of datasets to combine. Defaults to DATASETS.\n",
    "        extract_prompts (bool, optional): Whether to extract prompts from the combined data. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize a list to store the combined data\n",
    "    combined_data = []\n",
    "\n",
    "    # If specified, also store the extracted prompts\n",
    "    extracted_prompts = [] if extract_prompts else None\n",
    "\n",
    "    # Load and preprocess data from each dataset\n",
    "    for dataset in datasets:\n",
    "        data = preprocess_data(dataset)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "        # If specified, extract prompts\n",
    "        if extract_prompts:\n",
    "            extracted_prompts.extend(extract_prompt(data, dataset))\n",
    "\n",
    "    # Shuffle the combined data to ensure a mix of data from all datasets\n",
    "    # random.shuffle(combined_data)\n",
    "    # random.shuffle(extracted_prompts) if extract_prompts else None\n",
    "\n",
    "    # Save the combined data to a CSV file\n",
    "    convert_to_csv(combined_data, 'combined')\n",
    "\n",
    "    # If specified, save the extracted prompts to a CSV file\n",
    "    if extract_prompts:\n",
    "        df = pd.DataFrame(extracted_prompts, columns=['text'])\n",
    "        df.to_csv(f'{directory}/prompts.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015ebc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get estimated token count using TIKTOKEN, provided by OpenAI for accurate token count\n",
    "def token_count(csv_files):\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in a CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_file (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        # Load prompts from CSV file\n",
    "        df = pd.read_csv(csv_file)\n",
    "        prompts = df['text'].tolist()\n",
    "\n",
    "        # Initialize a counter for total tokens\n",
    "        total_tokens = 0\n",
    "\n",
    "        for prompt in prompts:\n",
    "            num_tokens = len(encoding.encode(prompt))\n",
    "            total_tokens += num_tokens\n",
    "\n",
    "        print(f\"File '{csv_file}' has {total_tokens} tokens.\")\n",
    "\n",
    "        # Estimate cost\n",
    "        if csv_file == 'Labelled_Data/prompts.csv':\n",
    "            cost = (total_tokens / 1000) * 0.003\n",
    "            print(f\"Estimated cost for '{csv_file}' is ${cost:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0ab0c",
   "metadata": {},
   "source": [
    "# Now that functions for pre-processing data are defined can call them to get the data in identical format [Prompt/Response]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d835d",
   "metadata": {},
   "source": [
    "## Now want to produce Feature Matrix and Target Vector\n",
    "### To do that need a way to calculate all the dependent variables (POS-Tags, Perplexities)\n",
    "### Would also be good practice to proudce summary statistics for each dataset, to gain more insight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dea7b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from statistics import mean\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2009c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4524646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(dataset_name, data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    dataset_name (str): The name of the dataset.\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        texts = [text.split(\"Answer:\", 1)[1].strip() for text in texts]  # Strip the 'Answer:' prefix'\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        texts = [text.split(\"Story:\", 1)[1].strip() for text in texts]  # Stripping the 'Story: ' string\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        texts = [text.split(\"Article:\", 1)[1].strip() for text in texts]  # Stripping the 'Article: ' string\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e3daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to get a general idea of how long GPT responses should be \n",
    "def average_token_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "    Calculates the average number of tokens in the answers of a dataset.\n",
    "\n",
    "    Returns:\n",
    "        float: Average number of tokens in the answers of a dataset.\n",
    "    \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for text in texts:\n",
    "        num_tokens = len(encoding.encode(text))\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    average_tokens = total_tokens / len(texts)\n",
    "\n",
    "    return average_tokens\n",
    "\n",
    "#PUBMED = 54\n",
    "#WP = 780\n",
    "#CNN = 794\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbf396fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNTING THE POS-TAGS USING SPACEY - library made specifically for this\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07906518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also incorporated readability scores\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67bd5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "875d5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    # CHEKCED\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac699d5f",
   "metadata": {},
   "source": [
    "# Probably function I am most uncertain of - calculating perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "132fedb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "    # tokenize the input, add special tokens and return tensors\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # if the text is too long, skip it\n",
    "    # this step has the extra effect of removing examples with low-quality/garbage content\n",
    "    if len(input_ids[0]) > 512:\n",
    "        return None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids, labels=input_ids)\n",
    "    loss = output.loss\n",
    "    return torch.exp(loss).item()  # perplexity is e^loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2354a6a",
   "metadata": {},
   "source": [
    "# To produce some summary statistics now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73d75296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e096ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_statistics(dataset_name, data):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "       Calculates various summary statistics for a dataset.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (dict): The data from the dataset.\n",
    "\n",
    "       Returns:\n",
    "       dict: A dictionary containing various summary statistics of the data.\n",
    "   \"\"\"\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    model, tokenizer = load_model()\n",
    "    overall_pos_counts, overall_punctuation_counts, overall_function_word_counts = load_and_count(dataset_name, data)\n",
    "    readability_scores = [calculate_readability_scores(text) for text in texts]\n",
    "    average_flesch_reading_ease = mean(score[0] for score in readability_scores)\n",
    "    average_flesch_kincaid_grade_level = mean(score[1] for score in readability_scores)\n",
    "    average_word_length = calculate_average_word_length(texts)\n",
    "    average_sentence_length = calculate_average_sentence_length(texts)\n",
    "    text_perplexities = [calculate_perplexity(text, model, tokenizer) for text in texts]\n",
    "    text_perplexities = [p for p in text_perplexities if p is not None]\n",
    "    average_text_perplexity = sum(text_perplexities) / len(text_perplexities)\n",
    "    sentences = [sentence.text for text in texts for sentence in nlp(text).sents]\n",
    "    sentence_perplexities = [calculate_perplexity(sentence, model, tokenizer) for sentence in sentences]\n",
    "    sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "    average_sentence_perplexity = sum(sentence_perplexities) / len(sentence_perplexities)\n",
    "    return {\n",
    "        'pos_freqs': overall_pos_counts,\n",
    "        'punctuation_freqs': overall_punctuation_counts,\n",
    "        'function_word_freqs': overall_function_word_counts,\n",
    "        'average_word_length': average_word_length,\n",
    "        'average_flesch_reading_ease': average_flesch_reading_ease,\n",
    "        'average_flesch_kincaid_grade_level': average_flesch_kincaid_grade_level,\n",
    "        'average_sentence_length': average_sentence_length,\n",
    "        'average_text_perplexity': average_text_perplexity,\n",
    "        'average_sentence_perplexity': average_sentence_perplexity,\n",
    "        'sentence_perplexities': sentence_perplexities,  # added this\n",
    "        'text_perplexities': text_perplexities  # and this\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88e44b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(statistics):\n",
    "    # CHECKED\n",
    "    pos_freqs = statistics['pos_freqs']\n",
    "    punctuation_freqs = statistics['punctuation_freqs']\n",
    "    function_word_freqs = statistics['function_word_freqs']\n",
    "\n",
    "    print(f\"Frequency of adjectives: {pos_freqs.get('ADJ', 0)}\")\n",
    "    print(f\"Frequency of adverbs: {pos_freqs.get('ADV', 0)}\")\n",
    "    print(f\"Frequency of conjunctions: {pos_freqs.get('CCONJ', 0)}\")\n",
    "    print(f\"Frequency of nouns: {pos_freqs.get('NOUN', 0)}\")\n",
    "    print(f\"Frequency of numbers: {pos_freqs.get('NUM', 0)}\")\n",
    "    print(f\"Frequency of pronouns: {pos_freqs.get('PRON', 0)}\")\n",
    "    print(f\"Frequency of verbs: {pos_freqs.get('VERB', 0)}\")\n",
    "    print(f\"Frequency of commas: {punctuation_freqs.get(',', 0)}\")\n",
    "    print(f\"Frequency of fullstops: {punctuation_freqs.get('.', 0)}\")\n",
    "    print(f\"Frequency of special character '-': {punctuation_freqs.get('-', 0)}\")\n",
    "    print(f\"Frequency of function word 'a': {function_word_freqs.get('a', 0)}\")\n",
    "    print(f\"Frequency of function word 'in': {function_word_freqs.get('in', 0)}\")\n",
    "    print(f\"Frequency of function word 'of': {function_word_freqs.get('of', 0)}\")\n",
    "    print(f\"Frequency of function word 'the': {function_word_freqs.get('the', 0)}\")\n",
    "    print(f\"Average Flesch Reading Ease: {statistics['average_flesch_reading_ease']}\")\n",
    "    print(f\"Average Flesch-Kincaid Grade Level: {statistics['average_flesch_kincaid_grade_level']}\")\n",
    "    print(f\"Average word length: {statistics['average_word_length']}\")\n",
    "    print(f\"Average sentence length: {statistics['average_sentence_length']}\")\n",
    "    print(f\"Average sentence perplexity: {statistics['average_sentence_perplexity']}\")\n",
    "    print(f\"Average text perplexity: {statistics['average_text_perplexity']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a5748",
   "metadata": {},
   "source": [
    "A paper compared the perplexity distributions ,also GPT-ZERO metions perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "150b3c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perplexities(sentence_perplexities, text_perplexities):\n",
    "    \"\"\"\n",
    "    Plots Kernel Density Estimates of the sentence and text perplexities.\n",
    "\n",
    "    Args:\n",
    "    sentence_perplexities (list of float): The perplexities of the sentences.\n",
    "    text_perplexities (list of float): The perplexities of the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Plot sentence perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(sentence_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Sentence Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 12)  # Limit x-axis to 12 for sentence perplexity\n",
    "    plt.show()\n",
    "\n",
    "    # Plot text perplexities\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.kdeplot(text_perplexities, color='skyblue', fill=True)\n",
    "    plt.title('Density Plot of Text Perplexities')\n",
    "    plt.xlabel('Perplexity')\n",
    "    plt.xlim(0, 10)  # Limit x-axis to 10 for text perplexity\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a828ca60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
