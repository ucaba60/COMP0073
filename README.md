# COMP0073
Summer Project Code


The application aims to classify a given piece of text and its corresponding title as written either by human or a language model. This is achieved through machine-learning binary classifiers – Logistic Regression, Support Vector Machine, Random Forest and an Ensemble of the three.

The application can be used as follows (note the numbers on the picture above):

1. This is where the user would input the title of the text. This is done in order to estimate the relation between the title and the long text. (cosine similarity).

2. This is where the user would input the main body of text.

3. This is where the user would select a machine learning model to use. Note: the Ensemble method has yielded the best results during testing/validation and Logistic Regression – the worst.

4. This is where the training corpus would be selected. The idea behind this is that the input text may be generated by models other than GPT-3.5-turbo. In general use-cases pick gpt-3.5-turbo.

5. This is the output window, giving the model classification in addition to the log probabilities and training corpus utilised.

The machine learning models were trained using data from 3 datasets – PubMedQA, WritingPrompts & CNN_Dailymail. The datasets are in the format ‘Question:Answer’. A total of 825 samples were extracted from the datasets. Afterwards the ‘Questions’ were used as prompts in order to generate language model responses (for gpt-3.5-turbo, gpt2-large, gpt-1jx separately). In total there are 1650 training/validation samples (half human, half AI- generated).

Afterwards, linguistic features were extracted from the texts as follows:

· Part-of-Speech tags – using Spacy library the frequency of ADJ,ADV,CONJ etc. were extracted.

· Perplexity – using RoBERTa the perplexity of each text was extracted. Literature suggests that human perplexity is > AI-perplexity.

· Cosine Similarity – once again vectorised using RoBERTa. Literature suggests that human text is more focused on the topic/title and AI-text answers in a more general way.

· TF-IDF scores – measure how ‘important’ certain words are for a text. This was computed for the top 10 words with largest difference in TF-IDF scores between human and AI-text in addition the words conclusion and it’s synonyms were added.

· Sentence length, average word length, uppercase frequency – standard across literature.

· Reading Ease – how ‘easy’ a text is to comprehend. Literature suggests AI-text is more readable.

After the extraction of the features the data was labelled, in order to identify what is human-written and what is GPT-generated. Then the above-mentioned machine-learning models were trained and tested using Grid-Search to find the hyperparameters that would yield the best results (accuracy, F1-score, recall,precision).

Overall, Ensemble & SVMs perform the best. The trained models are then sabed as .pkl files for ease-of-use.

After the title and long-text have been inputted by the users they are processed, and the above features are extracted. Then they are fed to the pre-trained machine learning models for classification.


                                                          **  Supplementary Explanation of Code and File Structure**

The code is separated into .py files depending on the functionality. There are also folders where data is stored in a .csv format. Generally, running interface.py starts the application.

Here is a brief summary of what each .py file does and what each folder contains:

Datasets.py – this is functions related to extracting, processing and combining data from the human datasets (PubMedQA, WritingPrompts, CNN_Dailymail) in the format ‘Question:Answer’.

§ This requires the exitance of the data folder, where WritingPrompts data is located.

§ This creates a file prompts.csv in the folder extracted_data.

§ This creates a file combined_human_data.csv containing all human text in extracted_data.

Llm_sample.py – functions related to using the ‘Questions’ as prompts for various language models (GPT-3.5-trubo).

· This creates the file gpt-3.5-turbo_repsonses.csv in extracted_data. This is the raw responses.

· This creates the file gpt-3.5-turbo_responses_preprocessed in extracted_data. These are the responses with some pre-processing.

· This creates the gpt-3.5-turbo_and_human_data.csv in extracted_data. This contains the pre-processed texts from both human and AI sources. This is the file from which features are extracted.

Feature_extraction.py – these are the main functions that extract the features (POS-tags, cosine similarity etc.) from the texts.

Training_matrix.py – this creates a data_matrix.csv file which is the result of extracting all the features. These files are used for training.

ML_models.py – training and fine-tuning ML models.

· The models are saved in the folder model_data/[model name]

Interface.py – the function that uses Gradio to start the application.

Folders:

Extracted_data: Contains the prompts, the AI-responses, the human text and the combined text.

Model_data: Contains .pkl files for each machine learning model.
