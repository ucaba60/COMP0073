{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e4af2d",
   "metadata": {},
   "source": [
    "# Supplementary Python Scrips "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe19ac",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the full collection of Python scripts used in the dissertaiton\n",
    "\n",
    "Explanations are provided in the form of in-code comments, docstrings as well as supplementary text, where deemed necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3db08",
   "metadata": {},
   "source": [
    "## 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af365c2e",
   "metadata": {},
   "source": [
    "The datasets used in throughout this dissertation were selected on the basis that they provide a form of a 'question' and 'response' which can easily be extracted. The 'questions' were used to prompt relevant LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19236946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575b401",
   "metadata": {},
   "source": [
    "Need to download the WritingPrompts data from [here](https://www.kaggle.com/datasets/ratthachat/writing-prompts). Save the data into a directory: <b>data/writingPrompts </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e882f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts' #This is required to load the writingPrompts dataset, as it is not part of the 'datasets' library, \n",
    "NUM_EXAMPLES = 300 #Number of initial samples from each dataset, note below, the actual number of samples is ~825 due to filtering\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14d3411",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23302ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d30776",
   "metadata": {},
   "source": [
    "Functions to load the relevant dataset(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7370ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(file_name):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name, gpt_filename=None):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "           gpt_filename (str, optional): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        if gpt_filename is None:\n",
    "            raise ValueError(\"A filename must be provided to load the GPT dataset.\")\n",
    "        return load_gpt(gpt_filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d56a28",
   "metadata": {},
   "source": [
    "Pre-processing data to ensure homogeneity of the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cffbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of the responses being fairly short.\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save(gpt_dataset=None, gpt_dataset_path=None, output_folder='extracted_data'):\n",
    "    \"\"\"\n",
    "    Preprocesses the datasets, combines them, and saves the result to a .csv file.\n",
    "    Optional argument gpt_dataset allows preprocessing the GPT dataset and combining it with existing datasets.\n",
    "\n",
    "    Args:\n",
    "        gpt_dataset (str, optional): Name of the GPT dataset csv file (without the .csv extension).\n",
    "        gpt_dataset_path (str, optional): Path to the GPT dataset.\n",
    "        output_folder: folder where the extracted data will be saved\n",
    "\n",
    "    Returns:\n",
    "        None, saves the combined data to a .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if gpt_dataset:\n",
    "        # Load and preprocess the GPT dataset\n",
    "        gpt_data_path = os.path.join(gpt_dataset_path, gpt_dataset)\n",
    "        gpt_data = load_data('gpt', gpt_data_path)\n",
    "        gpt_data = list(dict.fromkeys(gpt_data))\n",
    "        gpt_data = [(strip_newlines(q).strip(), a) for q, a in gpt_data]\n",
    "\n",
    "        # Load the already preprocessed data from the other datasets\n",
    "        combined_df = pd.read_csv(os.path.join(output_folder, 'combined_human_data.csv'))\n",
    "        combined_data = list(zip(combined_df['Text'], combined_df['Label']))\n",
    "\n",
    "        # Combine the data\n",
    "        combined_data += gpt_data\n",
    "\n",
    "        model_name = gpt_dataset.split('_')[0]  # Extract model name from gpt_dataset\n",
    "\n",
    "        output_file = f'{model_name}_and_human_data.csv'\n",
    "\n",
    "    else:\n",
    "        # Preprocess all the datasets\n",
    "        pubmed_data = preprocess_data('pubmed_qa')\n",
    "        writingprompts_data = preprocess_data('writingprompts')\n",
    "        cnn_daily_mail_data = preprocess_data('cnn_dailymail')\n",
    "\n",
    "        combined_data = pubmed_data + writingprompts_data + cnn_daily_mail_data\n",
    "\n",
    "        output_file = 'combined_human_data.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        overwrite = input(f\"'{output_file_path}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(f\"Not overwriting existing file '{output_file_path}'. Exiting...\")\n",
    "            return\n",
    "\n",
    "    # Save the combined data to a .csv file\n",
    "    df = pd.DataFrame(combined_data, columns=['Text', 'Label'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined dataset saved to '{output_file_path}' with {len(combined_data)} entries.\")\n",
    "\n",
    "\n",
    "# preprocess_and_save(output_folder = 'extracted_data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b72bddf",
   "metadata": {},
   "source": [
    "### Firstly, load and pre-process PubMed,WritingPrompts and CNN_Dailymail - the 'human' data. Data is storred in folder 'extracted_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f49993",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_and_save(output_folder = 'extracted_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b682f2",
   "metadata": {},
   "source": [
    "### Now, using the combined dataset , extracting the 'questions' to be used as prompts for the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd45f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_save(file_folder_path):\n",
    "    \"\"\"\n",
    "    Extracts prompts from the combined dataset and saves them to a .csv file.\n",
    "\n",
    "    Args:\n",
    "        file_folder_path (str): The path to the folder where the combined_source_data.csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        None, saves the prompts to a .csv file.\n",
    "    \"\"\"\n",
    "    # Load the combined dataset\n",
    "    combined_data_file = os.path.join(file_folder_path, 'combined_source_data.csv')\n",
    "    df = pd.read_csv(combined_data_file)\n",
    "    combined_data = list(zip(df['Text'], df['Label']))\n",
    "\n",
    "    # Extract prompts from the combined data\n",
    "    prompts = []\n",
    "    for full_text, _ in combined_data:\n",
    "        if 'Question:' in full_text and 'Answer:' in full_text:\n",
    "            prompts.append(full_text.split('Answer:')[0] + 'Answer:')\n",
    "        elif 'Summary:' in full_text and 'Article:' in full_text:\n",
    "            prompts.append('Write a news article based on the following summary: ' +\n",
    "                           full_text.split('Summary:')[1].split('Article:')[0].strip())\n",
    "        elif 'Prompt:' in full_text and 'Story:' in full_text:\n",
    "            prompts.append(full_text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:')\n",
    "        else:\n",
    "            print(f\"Could not determine dataset for the entry: {full_text}\")\n",
    "\n",
    "    # Save the prompts to a new CSV file\n",
    "    df_prompts = pd.DataFrame(prompts, columns=['Prompt'])\n",
    "    df_prompts.to_csv(os.path.join(file_folder_path, 'prompts.csv'), index=False)\n",
    "    print(f\"Prompts extracted and saved to '{os.path.join(file_folder_path, 'prompts.csv')}' with {len(df_prompts)}\"\n",
    "          f\" entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_prompts_and_save(\"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95370275",
   "metadata": {},
   "source": [
    "### Prompting LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c86d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532d15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "openai.api_key = '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' #Insert your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca526e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt3_responses(prompt_csv_path, response_folder_path, model=\"gpt-3.5-turbo\", temperature=1):\n",
    "    \"\"\"\n",
    "    Generate GPT-3 responses for a list of prompts saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model (str, optional): The ID of the model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Determines the randomness of the AI's output. Defaults to 1, as per OpenAI docs.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Initialize the starting point\n",
    "    start = 0\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # If so, get the number of completed prompts from the file\n",
    "        with open(response_csv_path, \"r\", newline=\"\", encoding='utf-8') as file:\n",
    "            start = sum(1 for row in csv.reader(file)) - 1  # Subtract 1 for the header\n",
    "\n",
    "    # Process the remaining prompts in batches\n",
    "    for i in range(start, len(prompts), BATCH_SIZE):\n",
    "        batch = prompts[i:i + BATCH_SIZE]\n",
    "        responses = []\n",
    "\n",
    "        for prompt in batch:\n",
    "            # Generate the response\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,  # as per OpenAI Documentation default temperature value is 1,\n",
    "                # which is the case here as well, this line is intended for future developments.\n",
    "            )\n",
    "\n",
    "            # Append the response to the list\n",
    "            responses.append('<<RESP>> ' + response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "        # Save the responses to a new DataFrame\n",
    "        response_df = pd.DataFrame({\n",
    "            'Prompt': batch,\n",
    "            'Response': responses\n",
    "        })\n",
    "\n",
    "        # Write the DataFrame to the CSV file, appending if it already exists\n",
    "        if os.path.exists(response_csv_path):\n",
    "            response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "        print(f\"Batch {i // BATCH_SIZE + 1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec975eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gpt3_responses('extracted_data/prompts.csv', 'extracted_data', temperature=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1060a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2_responses(prompt_csv_path, response_folder_path, model_name):\n",
    "    \"\"\"\n",
    "    Generate responses for a list of prompts saved in a csv file using a GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model_name (str): Name of the GPT-2 model to use (for example, \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\").\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "    # Define acceptable models\n",
    "    acceptable_models = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "\n",
    "    if model_name not in acceptable_models:\n",
    "        raise ValueError(f\"Invalid model name. Acceptable models are: {', '.join(acceptable_models)}\")\n",
    "\n",
    "    # Load the GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model_name}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # Load the existing responses\n",
    "        existing_responses_df = pd.read_csv(response_csv_path)\n",
    "\n",
    "        # Determine the starting point based on the number of existing responses\n",
    "        start = len(existing_responses_df)\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    for i in range(start, len(prompts)):\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompts[i], return_tensors=\"pt\")\n",
    "\n",
    "        # Generate a response\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "            do_sample=True,\n",
    "            max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "        )\n",
    "\n",
    "        # Calculate the number of tokens in the prompt\n",
    "        prompt_length = input_ids.shape[-1]\n",
    "\n",
    "        # Decode only the response, excluding the prompt\n",
    "        response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        # Save the prompt and response to a DataFrame\n",
    "        response_df = pd.DataFrame({\n",
    "            'Prompt': [prompts[i]],\n",
    "            'Response': [response]\n",
    "        })\n",
    "\n",
    "        # Append the DataFrame to the CSV file\n",
    "        if os.path.exists(response_csv_path):\n",
    "            response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "        print(f\"Prompt {i + 1} of {len(prompts)} processed\")\n",
    "\n",
    "    print(f\"All prompts processed. Responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18252a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gpt2_responses(\"extracted_data/prompts.csv\", \"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b620b5",
   "metadata": {},
   "source": [
    "Function above sometimes generates empty responses, hence a function to check an re-generate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a78e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_responses(response_csv_path):\n",
    "    \"\"\"\n",
    "    Check the csv file containing generated responses for any NaN values.\n",
    "    If any are found, regenerate the responses using the provided model.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, updates the csv file with the regenerated responses.\n",
    "    \"\"\"\n",
    "    # Extract the model name from the filename\n",
    "    model_name = os.path.basename(response_csv_path).split('_')[0]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loaded model {model_name}\")\n",
    "\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Iterate over the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row['Response']):\n",
    "            # Encode the prompt\n",
    "            input_ids = tokenizer.encode(row['Prompt'], return_tensors=\"pt\")\n",
    "\n",
    "            # Generate a response\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "                do_sample=True,\n",
    "                max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "            )\n",
    "\n",
    "            # Calculate the number of tokens in the prompt\n",
    "            prompt_length = input_ids.shape[-1]\n",
    "\n",
    "            # Decode only the response, excluding the prompt\n",
    "            response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "            # Replace the NaN response with the new one\n",
    "            df.at[i, 'Response'] = response\n",
    "\n",
    "            # Save the DataFrame back to the CSV file\n",
    "            df.to_csv(response_csv_path, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"Regenerated response for prompt {i + 1} of {len(df)}. Updated responses saved to {response_csv_path}.\")\n",
    "\n",
    "    print(f\"All NaN responses regenerated. Updated responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regenerate_responses('extracted_data/gpt2-large_responses.csv')\n",
    "\n",
    "\n",
    "#\n",
    "# df = pd.read_csv(\"extracted_data/gpt2-large_responses.csv\")\n",
    "# nan_rows = df[df.isna().any(axis=1)]\n",
    "# print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(response_csv_path):\n",
    "    \"\"\"\n",
    "    Load 'Prompt' and 'Response' from the generated responses csv file, remove the '<<RESP>>' string,\n",
    "    adjust the format to match the original datasets, add a label 1 to every instance,\n",
    "    and save to a new csv file.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the combined text and labels.\n",
    "    \"\"\"\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Remove the '<<RESP>>' string from each response\n",
    "    df['Response'] = df['Response'].str.replace('<<RESP>> ', '')\n",
    "\n",
    "    # Combine the prompt and the response in a new column 'Text' with adjustments for specific prompts\n",
    "    df['Text'] = df.apply(\n",
    "        lambda row: (\n",
    "            'Prompt: ' + row['Prompt'].replace(' Continue the story:', '') + ' Story: ' + row['Response']\n",
    "            if row['Prompt'].endswith('Continue the story:')\n",
    "            else (\n",
    "                'Summary: ' + row['Prompt'].replace('Write a news article based on the following summary: ',\n",
    "                                                    '') + ' Article: ' + row['Response']\n",
    "                if row['Prompt'].startswith('Write a news article based on the following summary:')\n",
    "                else row['Prompt'] + ' ' + row['Response']\n",
    "            )\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Add a new column 'Label' with value 1 to each instance\n",
    "    df['Label'] = 1\n",
    "\n",
    "    # Keep only the 'Text' and 'Label' columns\n",
    "    df = df[['Text', 'Label']]\n",
    "\n",
    "    # Construct the output file path based on the response file path\n",
    "    base_path, extension = os.path.splitext(response_csv_path)\n",
    "    output_csv_path = f\"{base_path}_preprocessed{extension}\"\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.isfile(output_csv_path):\n",
    "        overwrite = input(f\"{output_csv_path} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_and_combine('extracted_data/gpt2_t1_responses.csv')\n",
    "# extract_and_combine(\"extracted_data/gpt2-large_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa3d957",
   "metadata": {},
   "source": [
    "Calling preprocess_and_save once again with additional argument the 'gpt' dataset , which will preprocess the gpt responses and append them to the human-labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f2e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_and_save(gpt_dataset='gpt2-large_responses_preprocessed.csv', gpt_dataset_path='extracted_data',\n",
    "#                     output_folder='extracted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9139420",
   "metadata": {},
   "source": [
    "## Now that a file containing labelled human and AI data is generated the following chapter will define helper functions to extract relevant featuers from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfd157",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f08330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f03757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b05e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        # this step has the extra effect of removing examples with low-quality/garbage content (DetectGPT)\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Summary:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        full_text = full_text.strip()  # Remove leading/trailing white spaces\n",
    "        text = text.strip()\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            start = start.strip()\n",
    "            end = end.strip()\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                if end in temp_prompt: # Check if end is present in temp_prompt before splitting\n",
    "                    prompt, _ = temp_prompt.split(end, 1)\n",
    "                    prompt = prompt.strip()\n",
    "                else:\n",
    "                    print(f\"WARNING: Unable to find the end string '{end}' in temp_prompt for full text: {full_text} and text: {text}\")\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for full text: {full_text} and text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efaabe",
   "metadata": {},
   "source": [
    "Now, a function to create a data-matrix, with columns being the feautres and each row - an observation from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13470690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f797929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29c1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features = {\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                    'label': label\n",
    "                }\n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3089eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prepare_data_for_regression('extracted_data/full_data_gpt2.csv', save_file='data_matrix_gpt2.csv')\n",
    "# prepare_data_for_regression(\"extracted_data/gpt2-large_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18c61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc16824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63123dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
