{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffa9373",
   "metadata": {},
   "source": [
    "# Supplementary Python Scrips "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fa5d0",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the full collection of Python scripts used in the dissertaiton\n",
    "\n",
    "Explanations are provided in the form of in-code comments, docstrings as well as supplementary text, where deemed necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbaafc",
   "metadata": {},
   "source": [
    "## 1. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee43bb3",
   "metadata": {},
   "source": [
    "The datasets used in throughout this dissertation were selected on the basis that they provide a form of a 'question' and 'response' which can easily be extracted. The 'questions' were used to prompt relevant LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ba189a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AtanasM\\AppData\\Local\\anaconda3\\envs\\MScProject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325e1f6",
   "metadata": {},
   "source": [
    "Need to download the WritingPrompts data from [here](https://www.kaggle.com/datasets/ratthachat/writing-prompts). Save the data into a directory: <b>data/writingPrompts </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8e800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts' #This is required to load the writingPrompts dataset, as it is not part of the 'datasets' library, \n",
    "NUM_EXAMPLES = 300 #Number of initial samples from each dataset, note below, the actual number of samples is ~825 due to filtering\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97653fc7",
   "metadata": {},
   "source": [
    "Helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ac8d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568f5c11",
   "metadata": {},
   "source": [
    "Functions to load the relevant dataset(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e03b32c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(file_name):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name, gpt_filename=None):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "           gpt_filename (str, optional): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        if gpt_filename is None:\n",
    "            raise ValueError(\"A filename must be provided to load the GPT dataset.\")\n",
    "        return load_gpt(gpt_filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cff0018",
   "metadata": {},
   "source": [
    "Pre-processing data to ensure homogeneity of the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9129c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of the responses being fairly short.\n",
    "    # This is consistent with most research approaches concering these datasets ()\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save(gpt_dataset=None, gpt_dataset_path=None, output_folder='extracted_data'):\n",
    "    \"\"\"\n",
    "    Preprocesses the datasets, combines them, and saves the result to a .csv file.\n",
    "    Optional argument gpt_dataset allows preprocessing the GPT dataset and combining it with existing datasets.\n",
    "\n",
    "    Args:\n",
    "        gpt_dataset (str, optional): Name of the GPT dataset csv file (without the .csv extension).\n",
    "        gpt_dataset_path (str, optional): Path to the GPT dataset.\n",
    "        output_folder: folder where the extracted data will be saved\n",
    "\n",
    "    Returns:\n",
    "        None, saves the combined data to a .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if gpt_dataset:\n",
    "        # Load and preprocess the GPT dataset\n",
    "        gpt_data_path = os.path.join(gpt_dataset_path, gpt_dataset)\n",
    "        gpt_data = load_data('gpt', gpt_data_path)\n",
    "        gpt_data = list(dict.fromkeys(gpt_data))\n",
    "        gpt_data = [(strip_newlines(q).strip(), a) for q, a in gpt_data]\n",
    "\n",
    "        # Load the already preprocessed data from the other datasets\n",
    "        combined_df = pd.read_csv(os.path.join(output_folder, 'combined_human_data.csv'))\n",
    "        combined_data = list(zip(combined_df['Text'], combined_df['Label']))\n",
    "\n",
    "        # Combine the data\n",
    "        combined_data += gpt_data\n",
    "\n",
    "        model_name = gpt_dataset.split('_')[0]  # Extract model name from gpt_dataset\n",
    "\n",
    "        output_file = f'{model_name}_and_human_data.csv'\n",
    "\n",
    "    else:\n",
    "        # Preprocess all the datasets\n",
    "        pubmed_data = preprocess_data('pubmed_qa')\n",
    "        writingprompts_data = preprocess_data('writingprompts')\n",
    "        cnn_daily_mail_data = preprocess_data('cnn_dailymail')\n",
    "\n",
    "        combined_data = pubmed_data + writingprompts_data + cnn_daily_mail_data\n",
    "\n",
    "        output_file = 'combined_human_data.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        overwrite = input(f\"'{output_file_path}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(f\"Not overwriting existing file '{output_file_path}'. Exiting...\")\n",
    "            return\n",
    "\n",
    "    # Save the combined data to a .csv file\n",
    "    df = pd.DataFrame(combined_data, columns=['Text', 'Label'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined dataset saved to '{output_file_path}' with {len(combined_data)} entries.\")\n",
    "\n",
    "\n",
    "# preprocess_and_save(output_folder = 'extracted_data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0479798a",
   "metadata": {},
   "source": [
    "### Firstly, load and pre-process PubMed,WritingPrompts and CNN_Dailymail - the 'human' data. Data is storred in folder 'extracted_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b9bf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 300 entries from the dataset pubmed_qa\n",
      "Loaded and pre-processed 249 entries from the dataset writingprompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 276 entries from the dataset cnn_dailymail\n",
      "Combined dataset saved to 'extracted_data\\combined_human_data.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(output_folder = 'extracted_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6e1b0",
   "metadata": {},
   "source": [
    "Folder <b>extracted_data</b> contains combined_human_data -> .csv file with label 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996a751",
   "metadata": {},
   "source": [
    "### Now, using the combined dataset , extracting the 'questions' to be used as prompts for the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f17a3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_save(file_folder_path):\n",
    "    \"\"\"\n",
    "    Extracts prompts from the combined dataset and saves them to a .csv file.\n",
    "\n",
    "    Args:\n",
    "        file_folder_path (str): The path to the folder where the combined_source_data.csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        None, saves the prompts to a .csv file.\n",
    "    \"\"\"\n",
    "    # Load the combined dataset\n",
    "    combined_data_file = os.path.join(file_folder_path, 'combined_human_data.csv')\n",
    "    df = pd.read_csv(combined_data_file)\n",
    "    combined_data = list(zip(df['Text'], df['Label']))\n",
    "\n",
    "    # Extract prompts from the combined data\n",
    "    prompts = []\n",
    "    for full_text, _ in combined_data:\n",
    "        if 'Question:' in full_text and 'Answer:' in full_text:\n",
    "            prompts.append(full_text.split('Answer:')[0] + 'Write an abstract for a scientific paper that answers the Question:')\n",
    "        elif 'Summary:' in full_text and 'Article:' in full_text:\n",
    "            prompts.append('Write a news article based on the following summary: ' +\n",
    "                           full_text.split('Summary:')[1].split('Article:')[0].strip())\n",
    "        elif 'Prompt:' in full_text and 'Story:' in full_text:\n",
    "            prompts.append(full_text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:')\n",
    "        else:\n",
    "            print(f\"Could not determine dataset for the entry: {full_text}\")\n",
    "\n",
    "    # Save the prompts to a new CSV file\n",
    "    df_prompts = pd.DataFrame(prompts, columns=['Prompt'])\n",
    "    df_prompts.to_csv(os.path.join(file_folder_path, 'prompts.csv'), index=False)\n",
    "    print(f\"Prompts extracted and saved to '{os.path.join(file_folder_path, 'prompts.csv')}' with {len(df_prompts)}\"\n",
    "          f\" entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abe37513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts extracted and saved to 'extracted_data\\prompts.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "extract_prompts_and_save(\"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6586fa6b",
   "metadata": {},
   "source": [
    "### Prompting LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c36b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "131f4b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "openai.api_key = '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' #Insert your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8507104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt3_responses(prompt_csv_path, response_folder_path, model=\"gpt-3.5-turbo\", temperature=1):\n",
    "    \"\"\"\n",
    "    Generate GPT-3 responses for a list of prompts saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model (str, optional): The ID of the model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Determines the randomness of the AI's output. Defaults to 1, as per OpenAI docs.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Initialize the starting point\n",
    "    start = 0\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # If so, get the number of completed prompts from the file\n",
    "        with open(response_csv_path, \"r\", newline=\"\", encoding='utf-8') as file:\n",
    "            start = sum(1 for row in csv.reader(file)) - 1  # Subtract 1 for the header\n",
    "\n",
    "    while start < len(prompts):\n",
    "        try:\n",
    "            # Process the remaining prompts in batches\n",
    "            for i in range(start, len(prompts), BATCH_SIZE):\n",
    "                batch = prompts[i:i + BATCH_SIZE]\n",
    "                responses = []\n",
    "\n",
    "                for prompt in batch:\n",
    "                    # Generate the response\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "\n",
    "                    # Append the response to the list\n",
    "                    responses.append('<<RESP>> ' + response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "                # Save the responses to a new DataFrame\n",
    "                response_df = pd.DataFrame({\n",
    "                    'Prompt': batch,\n",
    "                    'Response': responses\n",
    "                })\n",
    "\n",
    "                # Write the DataFrame to the CSV file, appending if it already exists\n",
    "                if os.path.exists(response_csv_path):\n",
    "                    response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "                print(f\"Batch {i // BATCH_SIZE + 1} completed\")\n",
    "                start = i + BATCH_SIZE\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Sleeping for 10 seconds before retrying...\")\n",
    "            time.sleep(10)  # wait for 10 seconds before retrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "894ac6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 23 completed\n",
      "Batch 24 completed\n",
      "Batch 25 completed\n",
      "Batch 26 completed\n",
      "Batch 27 completed\n",
      "Batch 28 completed\n",
      "Batch 29 completed\n",
      "Batch 30 completed\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "generate_gpt3_responses('extracted_data/prompts.csv', 'extracted_data', temperature=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00ab688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2_responses(prompt_csv_path, response_folder_path, model_name):\n",
    "    \"\"\"\n",
    "    Generate responses for a list of prompts saved in a csv file using a GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model_name (str): Name of the GPT-2 model to use (for example, \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\").\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "    # Define acceptable models\n",
    "    acceptable_models = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "\n",
    "    if model_name not in acceptable_models:\n",
    "        raise ValueError(f\"Invalid model name. Acceptable models are: {', '.join(acceptable_models)}\")\n",
    "\n",
    "    # Load the GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model_name}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # Load the existing responses\n",
    "        existing_responses_df = pd.read_csv(response_csv_path)\n",
    "\n",
    "        # Determine the starting point based on the number of existing responses\n",
    "        start = len(existing_responses_df)\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    for i in range(start, len(prompts)):\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompts[i], return_tensors=\"pt\")\n",
    "\n",
    "        # Generate a response\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "            do_sample=True,\n",
    "            max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "        )\n",
    "\n",
    "        # Calculate the number of tokens in the prompt\n",
    "        prompt_length = input_ids.shape[-1]\n",
    "\n",
    "        # Decode only the response, excluding the prompt\n",
    "        response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        # Save the prompt and response to a DataFrame\n",
    "        response_df = pd.DataFrame({\n",
    "            'Prompt': [prompts[i]],\n",
    "            'Response': [response]\n",
    "        })\n",
    "\n",
    "        # Append the DataFrame to the CSV file\n",
    "        if os.path.exists(response_csv_path):\n",
    "            response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "        print(f\"Prompt {i + 1} of {len(prompts)} processed\")\n",
    "\n",
    "    print(f\"All prompts processed. Responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6507d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gpt2_responses(\"extracted_data/prompts.csv\", \"extracted_data\",model_name='gpt2-large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877d0c4",
   "metadata": {},
   "source": [
    "Function above sometimes generates empty responses, hence a function to check an re-generate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c41cdcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_responses(response_csv_path):\n",
    "    \"\"\"\n",
    "    Check the csv file containing generated responses for any NaN values.\n",
    "    If any are found, regenerate the responses using the provided model.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, updates the csv file with the regenerated responses.\n",
    "    \"\"\"\n",
    "    # Extract the model name from the filename\n",
    "    model_name = os.path.basename(response_csv_path).split('_')[0]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loaded model {model_name}\")\n",
    "\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Iterate over the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row['Response']):\n",
    "            # Encode the prompt\n",
    "            input_ids = tokenizer.encode(row['Prompt'], return_tensors=\"pt\")\n",
    "\n",
    "            # Generate a response\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "                do_sample=True,\n",
    "                max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "            )\n",
    "\n",
    "            # Calculate the number of tokens in the prompt\n",
    "            prompt_length = input_ids.shape[-1]\n",
    "\n",
    "            # Decode only the response, excluding the prompt\n",
    "            response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "            # Replace the NaN response with the new one\n",
    "            df.at[i, 'Response'] = response\n",
    "\n",
    "            # Save the DataFrame back to the CSV file\n",
    "            df.to_csv(response_csv_path, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"Regenerated response for prompt {i + 1} of {len(df)}. Updated responses saved to {response_csv_path}.\")\n",
    "\n",
    "    print(f\"All NaN responses regenerated. Updated responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regenerate_responses('extracted_data/gpt2-large_responses.csv')\n",
    "\n",
    "\n",
    "#\n",
    "# df = pd.read_csv(\"extracted_data/gpt2-large_responses.csv\")\n",
    "# nan_rows = df[df.isna().any(axis=1)]\n",
    "# print(nan_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76e5f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(response_csv_path):\n",
    "    \"\"\"\n",
    "    Load 'Prompt' and 'Response' from the generated responses csv file, remove the '<<RESP>>' string,\n",
    "    adjust the format to match the original datasets, add a label 1 to every instance,\n",
    "    and save to a new csv file.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the combined text and labels.\n",
    "    \"\"\"\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Remove the '<<RESP>>' string from each response\n",
    "    df['Response'] = df['Response'].str.replace('<<RESP>> ', '')\n",
    "\n",
    "    # Replace the specific string in the prompt\n",
    "    df['Prompt'] = df['Prompt'].str.replace(\n",
    "        'Write an abstract for a scientific paper that answers the Question:', 'Answer:')\n",
    "\n",
    "    # Combine the prompt and the response in a new column 'Text' with adjustments for specific prompts\n",
    "    df['Text'] = df.apply(\n",
    "        lambda row: (\n",
    "            'Prompt: ' + row['Prompt'].replace(' Continue the story:', '') + ' Story: ' + row['Response']\n",
    "            if row['Prompt'].endswith('Continue the story:')\n",
    "            else (\n",
    "                'Summary: ' + row['Prompt'].replace('Write a news article based on the following summary: ',\n",
    "                                                    '') + ' Article: ' + row['Response']\n",
    "                if row['Prompt'].startswith('Write a news article based on the following summary:')\n",
    "                else row['Prompt'] + ' ' + row['Response']\n",
    "            )\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Remove 'Title:' and/or 'Abstract:' if they appear after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer: (Title:|Abstract:)', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer:.*Abstract:', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears in the text\n",
    "    df['Text'] = df['Text'].str.replace('Abstract:', '', regex=False)\n",
    "\n",
    "    # Add a new column 'Label' with value 1 to each instance\n",
    "    df['Label'] = 1\n",
    "\n",
    "    # Keep only the 'Text' and 'Label' columns\n",
    "    df = df[['Text', 'Label']]\n",
    "\n",
    "    # Construct the output file path based on the response file path\n",
    "    base_path, extension = os.path.splitext(response_csv_path)\n",
    "    output_csv_path = f\"{base_path}_preprocessed{extension}\"\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.isfile(output_csv_path):\n",
    "        overwrite = input(f\"{output_csv_path} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de14243f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_data/gpt-3.5-turbo_responses_preprocessed.csv already exists. Do you want to overwrite it? (y/n): y\n"
     ]
    }
   ],
   "source": [
    "extract_and_combine(\"extracted_data/gpt-3.5-turbo_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854f3cc2",
   "metadata": {},
   "source": [
    "Calling preprocess_and_save once again with additional argument the 'gpt' dataset , which will preprocess the gpt responses and append them to the human-labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0493cd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'extracted_data\\gpt-3.5-turbo_and_human_data.csv' already exists. Do you want to overwrite it? (y/n): y\n",
      "Combined dataset saved to 'extracted_data\\gpt-3.5-turbo_and_human_data.csv' with 1125 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(gpt_dataset='gpt-3.5-turbo_responses_preprocessed.csv', gpt_dataset_path='extracted_data',\n",
    "                     output_folder='extracted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb57aa65",
   "metadata": {},
   "source": [
    "## Now that a file containing labelled human and AI data is generated the following chapter will define helper functions to extract relevant featuers from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd9ac5",
   "metadata": {},
   "source": [
    "# 2. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ea927bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70a41957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3aeb35a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    \n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        # this step has the extra effect of removing examples with low-quality/garbage content (DetectGPT)\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Summary:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        full_text = full_text.strip()  # Remove leading/trailing white spaces\n",
    "        text = text.strip()\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            start = start.strip()\n",
    "            end = end.strip()\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                if end in temp_prompt: # Check if end is present in temp_prompt before splitting\n",
    "                    prompt, _ = temp_prompt.split(end, 1)\n",
    "                    prompt = prompt.strip()\n",
    "                else:\n",
    "                    print(f\"WARNING: Unable to find the end string '{end}' in temp_prompt for full text: {full_text} and text: {text}\")\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for full text: {full_text} and text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b3940",
   "metadata": {},
   "source": [
    "Now, a function to create a data-matrix, with columns being the feautres and each row - an observation from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52822e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f9fa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b94ad888",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features = {\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                    'label': label\n",
    "                }\n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb9bf9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1\n",
      "Processed row 2\n",
      "WARNING: No sentence cosine similarities calculated for text: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n",
      "Processed row 3\n",
      "Processed row 4\n",
      "Processed row 5\n",
      "WARNING: No sentence cosine similarities calculated for text: DBE appears to be equally safe and effective when performed in the community setting as compared to a tertiary referral center with a comparable yield, efficacy, and complication rate.\n",
      "Processed row 6\n",
      "Processed row 7\n",
      "Processed row 8\n",
      "WARNING: No sentence cosine similarities calculated for text: Genetic variants identified in the present study may be insufficient to promote early carotid atherosclerosis.\n",
      "Processed row 9\n",
      "Processed row 10\n",
      "Processed row 11\n",
      "Processed row 12\n",
      "Processed row 13\n",
      "Processed row 14\n",
      "Processed row 15\n",
      "WARNING: No sentence cosine similarities calculated for text: Opioid PCT is a feasible and acceptable therapeutic method to reduce refractory breathlessness in palliative care patients.\n",
      "Processed row 16\n",
      "Processed row 17\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprepare_data_for_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_data/gpt2-large_and_human_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 66\u001b[0m, in \u001b[0;36mprepare_data_for_regression\u001b[1;34m(data_file, save_file, chunk_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 66\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [calculate_perplexity(sentence\u001b[38;5;241m.\u001b[39mtext, model, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     67\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     68\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     69\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     70\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 66\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     67\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     68\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     69\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     70\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 195\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[1;34m(text, model, tokenizer)\u001b[0m\n\u001b[0;32m    192\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, :\u001b[38;5;241m512\u001b[39m]\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    197\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1121\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1119\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(prediction_scores\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1120\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1121\u001b[0m     masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1124\u001b[0m     output \u001b[38;5;241m=\u001b[39m (prediction_scores,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "prepare_data_for_regression(\"extracted_data/gpt2-large_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b085486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e433a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1275345",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
