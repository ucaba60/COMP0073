{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2303fa7",
   "metadata": {},
   "source": [
    "# Python Scripts for Data Generation, Pre-Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749ca01",
   "metadata": {},
   "source": [
    "## 1. Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3470e1c5",
   "metadata": {},
   "source": [
    "<b> Synopsis: </b>\n",
    "This section of code gathers and does some pre-processing for the datasets <b> PubMed, WritingPrompts & CNN_DailyMail </b>.\n",
    "\n",
    "\n",
    "The idea is to have the format: 'question'.... 'response'.... for each of the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69d0aa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f152ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts' #This is required to load the writingPrompts dataset, as it is not part of the 'datasets' library, \n",
    "NUM_EXAMPLES = 300 #Number of initial samples from each dataset, note below, the actual number of samples is ~825 due to filtering\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c07730",
   "metadata": {},
   "source": [
    "Defining some helper functions, see docstrings for more information.\n",
    "\n",
    "These strip any newline characters, remove whitespaces, also remove some strings that are dataset specific. For example, WritingPrompts has the string 'CC' in front of most prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d7a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cac91",
   "metadata": {},
   "source": [
    "### Functions to load the relevant dataset(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1f78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(file_name):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name, gpt_filename=None):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "           gpt_filename (str, optional): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        if gpt_filename is None:\n",
    "            raise ValueError(\"A filename must be provided to load the GPT dataset.\")\n",
    "        return load_gpt(gpt_filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b5d7b",
   "metadata": {},
   "source": [
    "### Functions to ensure the each part of the combined dataset is in the same format (no unnecessary whitespaces etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa717e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of the responses being fairly short.\n",
    "    # This is consistent with most research approaches concering these datasets (DetectGPT paper e.g.)\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save(gpt_dataset=None, gpt_dataset_path=None, output_folder='extracted_data'):\n",
    "    \"\"\"\n",
    "    Preprocesses the datasets, combines them, and saves the result to a .csv file.\n",
    "    Optional argument gpt_dataset allows preprocessing the GPT dataset and combining it with existing datasets.\n",
    "\n",
    "    Args:\n",
    "        gpt_dataset (str, optional): Name of the GPT dataset csv file (without the .csv extension).\n",
    "        gpt_dataset_path (str, optional): Path to the GPT dataset.\n",
    "        output_folder: folder where the extracted data will be saved\n",
    "\n",
    "    Returns:\n",
    "        None, saves the combined data to a .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if gpt_dataset:\n",
    "        # Load and preprocess the GPT dataset\n",
    "        gpt_data_path = os.path.join(gpt_dataset_path, gpt_dataset)\n",
    "        gpt_data = load_data('gpt', gpt_data_path)\n",
    "        gpt_data = list(dict.fromkeys(gpt_data))\n",
    "        gpt_data = [(strip_newlines(q).strip(), a) for q, a in gpt_data]\n",
    "\n",
    "        # Load the already preprocessed data from the other datasets\n",
    "        combined_df = pd.read_csv(os.path.join(output_folder, 'combined_human_data.csv'))\n",
    "        combined_data = list(zip(combined_df['Text'], combined_df['Label']))\n",
    "\n",
    "        # Combine the data\n",
    "        combined_data += gpt_data\n",
    "\n",
    "        model_name = gpt_dataset.split('_')[0]  # Extract model name from gpt_dataset\n",
    "\n",
    "        output_file = f'{model_name}_and_human_data.csv'\n",
    "\n",
    "    else:\n",
    "        # Preprocess all the datasets\n",
    "        pubmed_data = preprocess_data('pubmed_qa')\n",
    "        writingprompts_data = preprocess_data('writingprompts')\n",
    "        cnn_daily_mail_data = preprocess_data('cnn_dailymail')\n",
    "\n",
    "        combined_data = pubmed_data + writingprompts_data + cnn_daily_mail_data\n",
    "\n",
    "        output_file = 'combined_human_data.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        overwrite = input(f\"'{output_file_path}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(f\"Not overwriting existing file '{output_file_path}'. Exiting...\")\n",
    "            return\n",
    "\n",
    "    # Save the combined data to a .csv file\n",
    "    df = pd.DataFrame(combined_data, columns=['Text', 'Label'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined dataset saved to '{output_file_path}' with {len(combined_data)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fff612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_and_save(output_folder = 'extracted_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c1302",
   "metadata": {},
   "source": [
    "### Synopsis : The above functions load up and perform some pre-processing for the three datasets , the function preprocess_and_save creates a .csv file called combined_human_data.\n",
    "\n",
    "### This file contains all the human-generated dataset entries with two columns. First column is the text the second column is the label - 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd40ec4",
   "metadata": {},
   "source": [
    "## 2. Extract Prompts from the combined_human_data.csv, to feed to GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26b8c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_save(file_folder_path):\n",
    "    \"\"\"\n",
    "    Extracts prompts from the combined dataset and saves them to a .csv file.\n",
    "\n",
    "    Args:\n",
    "        file_folder_path (str): The path to the folder where the combined_source_data.csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        None, saves the prompts to a .csv file.\n",
    "    \"\"\"\n",
    "    # Load the combined dataset\n",
    "    combined_data_file = os.path.join(file_folder_path, 'combined_human_data.csv')\n",
    "    df = pd.read_csv(combined_data_file)\n",
    "    combined_data = list(zip(df['Text'], df['Label']))\n",
    "\n",
    "    # Extract prompts from the combined data\n",
    "    prompts = []\n",
    "    for i, (full_text, _) in enumerate(combined_data):\n",
    "        if i < 300:\n",
    "            prompt = full_text.replace('Answer:', 'Write an abstract for a scientific paper that answers the Question:')\n",
    "            prompt = prompt.split('Write an abstract for a scientific paper that answers the Question:')[0] + \\\n",
    "                     'Write an abstract for a scientific paper that answers the Question:'\n",
    "            prompts.append(prompt.strip())\n",
    "        elif 'Summary:' in full_text and 'Article:' in full_text:\n",
    "            prompts.append('Write a news article based on the following summary: ' +\n",
    "                           full_text.split('Summary:')[1].split('Article:')[0].strip())\n",
    "        elif 'Prompt:' in full_text and 'Story:' in full_text:\n",
    "            prompts.append(full_text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:')\n",
    "        else:\n",
    "            print(f\"Could not determine dataset for the entry: {full_text}\")\n",
    "\n",
    "    # Save the prompts to a new CSV file\n",
    "    df_prompts = pd.DataFrame(prompts, columns=['Prompt'])\n",
    "    df_prompts.to_csv(os.path.join(file_folder_path, 'prompts.csv'), index=False)\n",
    "    print(f\"Prompts extracted and saved to '{os.path.join(file_folder_path, 'prompts.csv')}' with {len(df_prompts)}\"\n",
    "          f\" entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86ed0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_prompts_and_save(\"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f7987",
   "metadata": {},
   "source": [
    "### Extract_prompts_and_save creates a .csv file called prompts.csv containing ONLY the promtps in the desired format for GPT-3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c00edb",
   "metadata": {},
   "source": [
    "## 3. Generating GPT 3.5 Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dc7e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import time \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56dca0c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3039788163.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    openai.api_key =  #Insert your API key here.\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "openai.api_key = '1' #Insert your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77bb3d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt3_responses(prompt_csv_path, response_folder_path, model=\"gpt-3.5-turbo\", temperature=1):\n",
    "    \"\"\"\n",
    "    Generate GPT-3 responses for a list of prompts saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model (str, optional): The ID of the model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Determines the randomness of the AI's output. Defaults to 1, as per OpenAI docs.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Initialize the starting point\n",
    "    start = 0\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # If so, get the number of completed prompts from the file\n",
    "        with open(response_csv_path, \"r\", newline=\"\", encoding='utf-8') as file:\n",
    "            start = sum(1 for row in csv.reader(file)) - 1  # Subtract 1 for the header\n",
    "\n",
    "    while start < len(prompts):\n",
    "        try:\n",
    "            # Process the remaining prompts in batches\n",
    "            for i in range(start, len(prompts), BATCH_SIZE):\n",
    "                batch = prompts[i:i + BATCH_SIZE]\n",
    "                responses = []\n",
    "\n",
    "                for prompt in batch:\n",
    "                    # Generate the response\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "\n",
    "                    # Append the response to the list\n",
    "                    responses.append('<<RESP>> ' + response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "                # Save the responses to a new DataFrame\n",
    "                response_df = pd.DataFrame({\n",
    "                    'Prompt': batch,\n",
    "                    'Response': responses\n",
    "                })\n",
    "\n",
    "                # Write the DataFrame to the CSV file, appending if it already exists\n",
    "                if os.path.exists(response_csv_path):\n",
    "                    response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "                print(f\"Batch {i // BATCH_SIZE + 1} completed\")\n",
    "                start = i + BATCH_SIZE\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Sleeping for 10 seconds before retrying...\")\n",
    "            time.sleep(10)  # wait for 10 seconds before retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef3364fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate_gpt3_responses('extracted_data/prompts.csv', 'extracted_data', temperature=1) #temeprature is arbirtary this is the default value as per OpenAI docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b70cb",
   "metadata": {},
   "source": [
    "### The above function uses the prompts extracted and saves GPT 3.5 -turbo responses in a .csv file called gpt_3.5_turbo_responses.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ce3d2",
   "metadata": {},
   "source": [
    "### Now , formatting the GPT-3.5-turbo responses data so it is the same format as our human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03bc41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(response_csv_path):\n",
    "    \"\"\"\n",
    "    Load 'Prompt' and 'Response' from the generated responses csv file, remove the '<<RESP>>' string,\n",
    "    adjust the format to match the original datasets, add a label 1 to every instance,\n",
    "    and save to a new csv file.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the combined text and labels.\n",
    "    \"\"\"\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Remove the '<<RESP>>' string from each response\n",
    "    df['Response'] = df['Response'].str.replace('<<RESP>> ', '')\n",
    "\n",
    "    # Replace the specific string in the prompt\n",
    "    df['Prompt'] = df['Prompt'].str.replace(\n",
    "        'Write an abstract for a scientific paper that answers the Question:', 'Answer:')\n",
    "\n",
    "    # Combine the prompt and the response in a new column 'Text' with adjustments for specific prompts\n",
    "    df['Text'] = df.apply(\n",
    "        lambda row: (\n",
    "            'Prompt: ' + row['Prompt'].replace(' Continue the story:', '') + ' Story: ' + row['Response']\n",
    "            if row['Prompt'].endswith('Continue the story:')\n",
    "            else (\n",
    "                'Summary: ' + row['Prompt'].replace('Write a news article based on the following summary: ',\n",
    "                                                    '') + ' Article: ' + row['Response']\n",
    "                if row['Prompt'].startswith('Write a news article based on the following summary:')\n",
    "                else row['Prompt'] + ' ' + row['Response']\n",
    "            )\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Remove 'Title:' and/or 'Abstract:' if they appear after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer: (Title:|Abstract:)', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer:.*Abstract:', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears in the text\n",
    "    df['Text'] = df['Text'].str.replace('Abstract:', '', regex=False)\n",
    "\n",
    "    # Add a new column 'Label' with value 1 to each instance\n",
    "    df['Label'] = 1\n",
    "\n",
    "    # Keep only the 'Text' and 'Label' columns\n",
    "    df = df[['Text', 'Label']]\n",
    "    \n",
    "    # Print the number of entries pre-processed\n",
    "    num_entries = len(df)\n",
    "    print(f\"Number of entries pre-processed: {num_entries}\")\n",
    "\n",
    "    # Construct the output file path based on the response file path\n",
    "    base_path, extension = os.path.splitext(response_csv_path)\n",
    "    output_csv_path = f\"{base_path}_preprocessed{extension}\"\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.isfile(output_csv_path):\n",
    "        overwrite = input(f\"{output_csv_path} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c09498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract_and_combine(\"extracted_data/gpt-3.5-turbo_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91609e3d",
   "metadata": {},
   "source": [
    "### The above function formats the responses from GPT-3.5 so that they can be compared to the human-generated strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "332b95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_and_save(gpt_dataset='gpt-3.5-turbo_responses_preprocessed.csv', gpt_dataset_path='extracted_data',\n",
    "                     #output_folder='extracted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23d4534",
   "metadata": {},
   "source": [
    "### Calling preprocess_and_save with the additional argument gpt_dataset will append the GPT-3.5 responses to the previously generated humand responses, so we have one file that has both gpt and human responses with labels 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a657597",
   "metadata": {},
   "source": [
    "### 4. Extracting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ecac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e99b42",
   "metadata": {},
   "source": [
    "Note: need to run:\n",
    "\n",
    "<b>\n",
    "pip install -U pip setuptools wheel\n",
    "\n",
    "\n",
    "pip install -U spacy\n",
    "    \n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "</b>\n",
    "\n",
    "to get the below to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1215d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40c517",
   "metadata": {},
   "source": [
    "### The below functions extract features from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3cead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97da30d",
   "metadata": {},
   "source": [
    "### The above just removes the Prefixes that are present in every entry in the data -'Answer:' , 'Story' 'Article'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7249c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    \n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4c11c",
   "metadata": {},
   "source": [
    "### POS-tags are gathered using <b>Spacy</b>, which is a NLP library made to extract those specifically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "797c86d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffda62",
   "metadata": {},
   "source": [
    "### The Readability Scores are computed using the <b> textstat </b> library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4af17e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2fea1",
   "metadata": {},
   "source": [
    "### Fairly Straightforward code to calculate average lenght of sentences and words using SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        # this step has the extra effect of removing examples with low-quality/garbage content (DetectGPT)\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93051b37",
   "metadata": {},
   "source": [
    "\n",
    "The <b> perplexity </b> is computed by tokenizing the text using <b> RoBERTa </b> and then calculating perplexity as the exponential of the loss. Essentially the model predicts the next token and we take the cross-entropy loss between the prediciton and the actual next token.\n",
    "\n",
    "Here only the first 512 tokens are considered, as was done in DetectGPT paper, mainly because that's the maximum for RoBERTa and because it removes examples with low quality text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f13b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(normalized_embeddings1.numpy(), normalized_embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Summary:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        full_text = full_text.strip()  # Remove leading/trailing white spaces\n",
    "        text = text.strip()\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            start = start.strip()\n",
    "            end = end.strip()\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                if end in temp_prompt: # Check if end is present in temp_prompt before splitting\n",
    "                    prompt, _ = temp_prompt.split(end, 1)\n",
    "                    prompt = prompt.strip()\n",
    "                else:\n",
    "                    print(f\"WARNING: Unable to find the end string '{end}' in temp_prompt for full text: {full_text} and text: {text}\")\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for full text: {full_text} and text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f92b2f6",
   "metadata": {},
   "source": [
    "\n",
    "### The <b> cosine similarity </b> is computed by tokenizing the texts with <b> RoBERTa </b> and then generating word embeddings by passing the encoded tokens to the model , the word embeddings are converted to arrays , normalised and then their cosines are computed. The <b> cosine </b> function computes the cosine distance between the two arrays (vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba4795",
   "metadata": {},
   "source": [
    "### Synopsis:\n",
    "### The features extracted are POS -tags, Readability Scores, Average Word Lenght, Average Sentence Lenght, Text Perplexity, Cosine Similarity (for sentences in each text and between the response and the prompt). The below focuses on TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65609acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70fb8c32",
   "metadata": {},
   "source": [
    "### Now I want to get the top 20 words with highest average difference in TF-IDF scores accross both human and AI-generated text as well as all synonyms of 'conclude'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80296f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "import seaborn as sns\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5905d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_tfidf(text):\n",
    "    \"\"\"\n",
    "    This function pre-processes the text by lowercasing all words, removing punctuation,\n",
    "    removing stop words and lemmatizing the words.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be pre-processed.\n",
    "\n",
    "    Returns:\n",
    "    text (str): The pre-processed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # join words back into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62aa9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference_tfidf_words(data_file, n_top_words=10):\n",
    "    \"\"\"\n",
    "    This function reads the input data, focuses only on the texts (responses),\n",
    "    and computes the top n words with the largest average difference in TF-IDF scores\n",
    "    between human-labelled text and AI-generated text.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    n_top_words (int): The number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    diff_words (DataFrame): DataFrame with top n words and their average difference in TF-IDF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_file)\n",
    "    data_tuples = [tuple(x) for x in data.values]\n",
    "    _, texts = zip(*extract_prompts_and_texts(data_tuples))\n",
    "    \n",
    "    # preprocess texts\n",
    "    texts = [prepare_for_tfidf(text) for text in texts]\n",
    "\n",
    "    # split data into human and AI generated\n",
    "    human_texts = [text for text, label in zip(texts, data['Label']) if label == 0]\n",
    "    ai_texts = [text for text, label in zip(texts, data['Label']) if label == 1]\n",
    "\n",
    "    # create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # compute TF-IDF for human texts\n",
    "    human_tfidf = vectorizer.fit_transform(human_texts)\n",
    "    feature_names_human = vectorizer.get_feature_names_out()\n",
    "    human_words_tfidf = dict(zip(feature_names_human, human_tfidf.sum(axis=0).tolist()[0]))\n",
    "\n",
    "    # compute TF-IDF for AI texts\n",
    "    ai_tfidf = vectorizer.fit_transform(ai_texts)\n",
    "    feature_names_ai = vectorizer.get_feature_names_out()\n",
    "    ai_words_tfidf = dict(zip(feature_names_ai, ai_tfidf.sum(axis=0).tolist()[0]))\n",
    "\n",
    "    # compute the difference in TF-IDF scores\n",
    "    diff_words_tfidf = {word: human_words_tfidf.get(word, 0) - ai_words_tfidf.get(word, 0) \n",
    "                        for word in set(feature_names_human).union(feature_names_ai)}\n",
    "    \n",
    "    diff_words = pd.DataFrame(diff_words_tfidf.items(), columns=['word', 'tfidf_difference']).nlargest(n_top_words, 'tfidf_difference')\n",
    "\n",
    "    return diff_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1585532",
   "metadata": {},
   "source": [
    "### This function finds what the top 20 words with largest average difference in TF-IDF scores between human and AI- generated text are. I want to have those words and compute the TF-IDF score for each for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "017fcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_tfidf_words(data_file, n_top_words=10):\n",
    "    \"\"\"\n",
    "    This function reads the input data, computes the top n words with the largest average difference in \n",
    "    TF-IDF scores between human-labelled text and AI-generated text, and plots the results.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    n_top_words (int): The number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # get the top words with the largest difference in tf-idf\n",
    "    diff_words = compute_difference_tfidf_words(data_file, n_top_words)\n",
    "\n",
    "    # plot the results using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    palette = sns.color_palette(\"coolwarm\", n_top_words)  # using coolwarm palette, you can use any palette of your liking\n",
    "    sns.barplot(x=diff_words['tfidf_difference'][::-1], y=diff_words['word'][::-1], palette=palette)  # reverse order to have largest bar at top\n",
    "    plt.xlabel('TF-IDF Difference')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title('Top {} Words with Largest Average Difference in TF-IDF Scores between Human and AI-Generated Text'.format(n_top_words))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74005c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_diff_tfidf_words(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd9afd",
   "metadata": {},
   "source": [
    "# 6. Finally one function that reads a .csv file and computes all the features , using the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98375e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "        \n",
    "    # Calculate the top 10 words with the highest difference in TF-IDF scores and the vectorizer\n",
    "#     diff_words = compute_difference_tfidf_words(data_file, n_top_words=10)\n",
    "    top_words = vocabulary = ['said', 'like', 'im', 'get', 'told', 'dont', 'say', 'know', 'think', 'look', 'conclusion','summarise','summarize','finale','overall','sum','end','summary','conclude']\n",
    "    # Combine top_words and synonyms into one list\n",
    "    all_words =list(set(top_words))\n",
    "    \n",
    "    # Create a TF-IDF vectorizer with the top 10 words as vocabulary\n",
    "    vectorizer = TfidfVectorizer(vocabulary=all_words)\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                features = {}  # Initialize the features dictionary here\n",
    "                \n",
    "                \n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "                \n",
    "                # Transform the text into TF-IDF scores\n",
    "                tfidf_scores = vectorizer.fit_transform([text]).toarray()\n",
    "                    \n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features.update({\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                })\n",
    "                \n",
    "                # If the TF-IDF scores array is not empty, zip the scores with the words to create a dictionary\n",
    "                # and update the features dictionary with this new dictionary\n",
    "                if tfidf_scores.size > 0:\n",
    "                    word_scores = {f\"tf_idfs_{word}\": score for word, score in zip(all_words, tfidf_scores[0])}\n",
    "                    features.update(word_scores)\n",
    "                else:  # If the TF-IDF scores array is empty, assign 0 to each word's score\n",
    "                    word_scores = {f\"tf_idfs_{word}\": 0 for word in all_words}\n",
    "                    features.update(word_scores)\n",
    "                    \n",
    "                    \n",
    "                features['label'] = label\n",
    "\n",
    "           \n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f7347",
   "metadata": {},
   "source": [
    "### Note: I extracted the top 20 words with largest difference in TF-IDF scores and also added synonyms to the words ' conclude' 'summarise' , since those did not appear in the top-20 list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c74a2",
   "metadata": {},
   "source": [
    "# 7. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06eba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74f077fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv(\"data_matrix_gpt-3.5-turbo.csv\")\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop('label', axis=1)\n",
    "y = data['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling data, as Logistic Regression is sensitive to feature scales\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the model\n",
    "clf = LogisticRegression()  # Adjust max_iter as needed\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'solver': ['newton-cg', 'lbfgs', 'sag'], \n",
    "        'penalty': ['l2', 'none'], \n",
    "        'C': np.logspace(-4, 4, 20),\n",
    "        'max_iter' : [5000, 10000, 20000]  # increased max_iter values\n",
    "    },\n",
    "    {\n",
    "        'solver': ['liblinear', 'saga'], \n",
    "        'penalty': ['l1', 'l2'], \n",
    "        'C': np.logspace(-4, 4, 20),\n",
    "        'max_iter' : [5000, 10000, 20000]  # increased max_iter values\n",
    "    },\n",
    "    {\n",
    "        'solver': ['saga'], \n",
    "        'penalty': ['elasticnet'], \n",
    "        'C': np.logspace(-4, 4, 20),\n",
    "        'l1_ratio': np.linspace(0, 1, 10), # Only applicable for 'elasticnet'\n",
    "        'max_iter' : [5000, 10000, 20000]  # increased max_iter values\n",
    "    },\n",
    "]\n",
    "\n",
    "# Specify the number of combinations to sample\n",
    "n_iter_search = 50  # Adjust this value as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae57eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(clf, param_distributions=param_grid, n_iter=n_iter_search, cv=5, verbose=0, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "model_best = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab441a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_best.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b80cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficients(model, X):\n",
    "    cols = X.columns\n",
    "\n",
    "    # For binary classification\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        coef = model.coef_[0]\n",
    "        sns.barplot(x=cols, y=coef)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Feature Coefficients')\n",
    "\n",
    "    # For multi-class classification\n",
    "    else:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        coef = model.coef_\n",
    "        sns.heatmap(coef, xticklabels=cols, cmap=\"RdBu_r\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title('Feature Coefficients')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d76cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ROC-AUC curve\n",
    "def plot_roc_auc_curve(y_test, y_scores):\n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b8ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the functions\n",
    "plot_confusion_matrix(y_test, y_pred)\n",
    "plot_coefficients(model_best, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9400d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = model_best.predict_proba(X_test)[:, 1]  # scores = proba of positive class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d45364",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_auc_curve(y_test, y_scores)  # you should compute y_scores first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707d0bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b609ad87",
   "metadata": {},
   "source": [
    "## The above is a Logistic Regression model and it's results, I have used RandomGridSearch to find the best hyperparamters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f69d94e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
