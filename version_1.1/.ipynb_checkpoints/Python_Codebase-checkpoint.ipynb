{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaab8d0c",
   "metadata": {},
   "source": [
    "# Python Scripts for Data Generation, Pre-Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527a0e1",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "The datasets used in this work were selected on the basis that they provide a from of 'question' and 'response', which can easily be extracted. The 'questions' were used to prompt the LLM models (GPT-3.5-turbo, GPT2, GPT-J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdcea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4ebb8",
   "metadata": {},
   "source": [
    "Need to download the WritingPrompts data from [here](https://www.kaggle.com/datasets/ratthachat/writing-prompts). Save the data into a directory: <b>data/writingPrompts </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8122ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts' #This is required to load the writingPrompts dataset, as it is not part of the 'datasets' library, \n",
    "NUM_EXAMPLES = 300 #Number of initial samples from each dataset, note below, the actual number of samples is ~825 due to filtering\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c918e0",
   "metadata": {},
   "source": [
    "Defining some helper functions, see docstrings for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff5c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9423d5e",
   "metadata": {},
   "source": [
    "Functions to load the relevant dataset(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0906df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(file_name):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name, gpt_filename=None):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "           gpt_filename (str, optional): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        if gpt_filename is None:\n",
    "            raise ValueError(\"A filename must be provided to load the GPT dataset.\")\n",
    "        return load_gpt(gpt_filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca534d31",
   "metadata": {},
   "source": [
    "Functions to ensure the each part of the combined dataset is in the same format (no unnecessary whitespaces etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18cc99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of the responses being fairly short.\n",
    "    # This is consistent with most research approaches concering these datasets (DetectGPT paper e.g.)\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save(gpt_dataset=None, gpt_dataset_path=None, output_folder='extracted_data'):\n",
    "    \"\"\"\n",
    "    Preprocesses the datasets, combines them, and saves the result to a .csv file.\n",
    "    Optional argument gpt_dataset allows preprocessing the GPT dataset and combining it with existing datasets.\n",
    "\n",
    "    Args:\n",
    "        gpt_dataset (str, optional): Name of the GPT dataset csv file (without the .csv extension).\n",
    "        gpt_dataset_path (str, optional): Path to the GPT dataset.\n",
    "        output_folder: folder where the extracted data will be saved\n",
    "\n",
    "    Returns:\n",
    "        None, saves the combined data to a .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if gpt_dataset:\n",
    "        # Load and preprocess the GPT dataset\n",
    "        gpt_data_path = os.path.join(gpt_dataset_path, gpt_dataset)\n",
    "        gpt_data = load_data('gpt', gpt_data_path)\n",
    "        gpt_data = list(dict.fromkeys(gpt_data))\n",
    "        gpt_data = [(strip_newlines(q).strip(), a) for q, a in gpt_data]\n",
    "\n",
    "        # Load the already preprocessed data from the other datasets\n",
    "        combined_df = pd.read_csv(os.path.join(output_folder, 'combined_human_data.csv'))\n",
    "        combined_data = list(zip(combined_df['Text'], combined_df['Label']))\n",
    "\n",
    "        # Combine the data\n",
    "        combined_data += gpt_data\n",
    "\n",
    "        model_name = gpt_dataset.split('_')[0]  # Extract model name from gpt_dataset\n",
    "\n",
    "        output_file = f'{model_name}_and_human_data.csv'\n",
    "\n",
    "    else:\n",
    "        # Preprocess all the datasets\n",
    "        pubmed_data = preprocess_data('pubmed_qa')\n",
    "        writingprompts_data = preprocess_data('writingprompts')\n",
    "        cnn_daily_mail_data = preprocess_data('cnn_dailymail')\n",
    "\n",
    "        combined_data = pubmed_data + writingprompts_data + cnn_daily_mail_data\n",
    "\n",
    "        output_file = 'combined_human_data.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        overwrite = input(f\"'{output_file_path}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(f\"Not overwriting existing file '{output_file_path}'. Exiting...\")\n",
    "            return\n",
    "\n",
    "    # Save the combined data to a .csv file\n",
    "    df = pd.DataFrame(combined_data, columns=['Text', 'Label'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined dataset saved to '{output_file_path}' with {len(combined_data)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15524da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 300 entries from the dataset pubmed_qa\n",
      "Loaded and pre-processed 249 entries from the dataset writingprompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 276 entries from the dataset cnn_dailymail\n",
      "'extracted_data\\combined_human_data.csv' already exists. Do you want to overwrite it? (y/n): y\n",
      "Combined dataset saved to 'extracted_data\\combined_human_data.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(output_folder = 'extracted_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3da4f6",
   "metadata": {},
   "source": [
    "Now we can load and pre-process <b> PubMed, WritingPrompts and CNN_DailyMail </b> the labelled 'human' data. A file called <b> combined_human_data.csv</b> will be storred in the data folder called <b>extracted_data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074feb5",
   "metadata": {},
   "source": [
    "Using the <b> combined_human_data.csv </b> the 'questions' will be extracted , to be used as prompts for the LLMs.\n",
    "Note: <b> Because the answers from PubMed are essentially abstracts from papers the prompt has been altered </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90d1c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_save(file_folder_path):\n",
    "    \"\"\"\n",
    "    Extracts prompts from the combined dataset and saves them to a .csv file.\n",
    "\n",
    "    Args:\n",
    "        file_folder_path (str): The path to the folder where the combined_source_data.csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        None, saves the prompts to a .csv file.\n",
    "    \"\"\"\n",
    "    # Load the combined dataset\n",
    "    combined_data_file = os.path.join(file_folder_path, 'combined_human_data.csv')\n",
    "    df = pd.read_csv(combined_data_file)\n",
    "    combined_data = list(zip(df['Text'], df['Label']))\n",
    "\n",
    "    # Extract prompts from the combined data\n",
    "    prompts = []\n",
    "    for i, (full_text, _) in enumerate(combined_data):\n",
    "        if i < 300:\n",
    "            prompt = full_text.replace('Answer:', 'Write an abstract for a scientific paper that answers the Question:')\n",
    "            prompt = prompt.split('Write an abstract for a scientific paper that answers the Question:')[0] + \\\n",
    "                     'Write an abstract for a scientific paper that answers the Question:'\n",
    "            prompts.append(prompt.strip())\n",
    "        elif 'Summary:' in full_text and 'Article:' in full_text:\n",
    "            prompts.append('Write a news article based on the following summary: ' +\n",
    "                           full_text.split('Summary:')[1].split('Article:')[0].strip())\n",
    "        elif 'Prompt:' in full_text and 'Story:' in full_text:\n",
    "            prompts.append(full_text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:')\n",
    "        else:\n",
    "            print(f\"Could not determine dataset for the entry: {full_text}\")\n",
    "\n",
    "    # Save the prompts to a new CSV file\n",
    "    df_prompts = pd.DataFrame(prompts, columns=['Prompt'])\n",
    "    df_prompts.to_csv(os.path.join(file_folder_path, 'prompts.csv'), index=False)\n",
    "    print(f\"Prompts extracted and saved to '{os.path.join(file_folder_path, 'prompts.csv')}' with {len(df_prompts)}\"\n",
    "          f\" entries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8770c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts extracted and saved to 'extracted_data\\prompts.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "extract_prompts_and_save(\"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dfdb3",
   "metadata": {},
   "source": [
    "Creates a file called <b>prompts.csv</b> that contains all prompts to be used in the LLMs. 825 entries, consistent with before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad2026",
   "metadata": {},
   "source": [
    "## Generating GPT-3.5/LLMs responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7308415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import time \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "54f33313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "openai.api_key = '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' #Insert your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "980b20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt3_responses(prompt_csv_path, response_folder_path, model=\"gpt-3.5-turbo\", temperature=1):\n",
    "    \"\"\"\n",
    "    Generate GPT-3 responses for a list of prompts saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model (str, optional): The ID of the model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Determines the randomness of the AI's output. Defaults to 1, as per OpenAI docs.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Initialize the starting point\n",
    "    start = 0\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # If so, get the number of completed prompts from the file\n",
    "        with open(response_csv_path, \"r\", newline=\"\", encoding='utf-8') as file:\n",
    "            start = sum(1 for row in csv.reader(file)) - 1  # Subtract 1 for the header\n",
    "\n",
    "    while start < len(prompts):\n",
    "        try:\n",
    "            # Process the remaining prompts in batches\n",
    "            for i in range(start, len(prompts), BATCH_SIZE):\n",
    "                batch = prompts[i:i + BATCH_SIZE]\n",
    "                responses = []\n",
    "\n",
    "                for prompt in batch:\n",
    "                    # Generate the response\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "\n",
    "                    # Append the response to the list\n",
    "                    responses.append('<<RESP>> ' + response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "                # Save the responses to a new DataFrame\n",
    "                response_df = pd.DataFrame({\n",
    "                    'Prompt': batch,\n",
    "                    'Response': responses\n",
    "                })\n",
    "\n",
    "                # Write the DataFrame to the CSV file, appending if it already exists\n",
    "                if os.path.exists(response_csv_path):\n",
    "                    response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "                print(f\"Batch {i // BATCH_SIZE + 1} completed\")\n",
    "                start = i + BATCH_SIZE\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Sleeping for 10 seconds before retrying...\")\n",
    "            time.sleep(10)  # wait for 10 seconds before retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbdf6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_gpt3_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted_data/prompts.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 40\u001b[0m, in \u001b[0;36mgenerate_gpt3_responses\u001b[1;34m(prompt_csv_path, response_folder_path, model, temperature)\u001b[0m\n\u001b[0;32m     36\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Generate the response\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Append the response to the list\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<<RESP>> \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_gpt3_responses('extracted_data/prompts.csv', 'extracted_data', temperature=1) #temeprature is arbirtary this is the default value as per OpenAI docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79885a8c",
   "metadata": {},
   "source": [
    "Responses will be saved in <b>got-3.5-turbo_responses.csv </b> file. However they must be further processed to ensure compatability with existing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41271afd",
   "metadata": {},
   "source": [
    "Doing the same thing for gpt-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2_responses(prompt_csv_path, response_folder_path, model_name):\n",
    "    \"\"\"\n",
    "    Generate responses for a list of prompts saved in a csv file using a GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model_name (str): Name of the GPT-2 model to use (for example, \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\").\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "    # Define acceptable models\n",
    "    acceptable_models = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "\n",
    "    if model_name not in acceptable_models:\n",
    "        raise ValueError(f\"Invalid model name. Acceptable models are: {', '.join(acceptable_models)}\")\n",
    "\n",
    "    # Load the GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model_name}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # Load the existing responses\n",
    "        existing_responses_df = pd.read_csv(response_csv_path)\n",
    "\n",
    "        # Determine the starting point based on the number of existing responses\n",
    "        start = len(existing_responses_df)\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    for i in range(start, len(prompts)):\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompts[i], return_tensors=\"pt\")\n",
    "\n",
    "        # Generate a response\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "            do_sample=True,\n",
    "            max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "        )\n",
    "\n",
    "        # Calculate the number of tokens in the prompt\n",
    "        prompt_length = input_ids.shape[-1]\n",
    "\n",
    "        # Decode only the response, excluding the prompt\n",
    "        response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        # Save the prompt and response to a DataFrame\n",
    "        response_df = pd.DataFrame({\n",
    "            'Prompt': [prompts[i]],\n",
    "            'Response': [response]\n",
    "        })\n",
    "\n",
    "        # Append the DataFrame to the CSV file\n",
    "        if os.path.exists(response_csv_path):\n",
    "            response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "        print(f\"Prompt {i + 1} of {len(prompts)} processed\")\n",
    "\n",
    "    print(f\"All prompts processed. Responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99484560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gpt2_responses(\"extracted_data/prompts.csv\", \"extracted_data\",model_name='gpt2-large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e7597",
   "metadata": {},
   "source": [
    "Function above sometimes generates empty responses, hence a function to check an re-generate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_responses(response_csv_path):\n",
    "    \"\"\"\n",
    "    Check the csv file containing generated responses for any NaN values.\n",
    "    If any are found, regenerate the responses using the provided model.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, updates the csv file with the regenerated responses.\n",
    "    \"\"\"\n",
    "    # Extract the model name from the filename\n",
    "    model_name = os.path.basename(response_csv_path).split('_')[0]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loaded model {model_name}\")\n",
    "\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Iterate over the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row['Response']):\n",
    "            # Encode the prompt\n",
    "            input_ids = tokenizer.encode(row['Prompt'], return_tensors=\"pt\")\n",
    "\n",
    "            # Generate a response\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "                do_sample=True,\n",
    "                max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "            )\n",
    "\n",
    "            # Calculate the number of tokens in the prompt\n",
    "            prompt_length = input_ids.shape[-1]\n",
    "\n",
    "            # Decode only the response, excluding the prompt\n",
    "            response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "            # Replace the NaN response with the new one\n",
    "            df.at[i, 'Response'] = response\n",
    "\n",
    "            # Save the DataFrame back to the CSV file\n",
    "            df.to_csv(response_csv_path, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"Regenerated response for prompt {i + 1} of {len(df)}. Updated responses saved to {response_csv_path}.\")\n",
    "\n",
    "    print(f\"All NaN responses regenerated. Updated responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422a421",
   "metadata": {},
   "source": [
    "Now we will have a file <b> gpt2-large_responses.csv </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if no NaN responses:\n",
    "# regenerate_responses('extracted_data/gpt2-large_responses.csv')\n",
    "\n",
    "\n",
    "#\n",
    "# df = pd.read_csv(\"extracted_data/gpt2-large_responses.csv\")\n",
    "# nan_rows = df[df.isna().any(axis=1)]\n",
    "# print(nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d61e8d",
   "metadata": {},
   "source": [
    "Now , formatting the GPT-3.5-turbo responses data so it is the same format as our human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "485b19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(response_csv_path):\n",
    "    \"\"\"\n",
    "    Load 'Prompt' and 'Response' from the generated responses csv file, remove the '<<RESP>>' string,\n",
    "    adjust the format to match the original datasets, add a label 1 to every instance,\n",
    "    and save to a new csv file.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the combined text and labels.\n",
    "    \"\"\"\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Remove the '<<RESP>>' string from each response\n",
    "    df['Response'] = df['Response'].str.replace('<<RESP>> ', '')\n",
    "\n",
    "    # Replace the specific string in the prompt\n",
    "    df['Prompt'] = df['Prompt'].str.replace(\n",
    "        'Write an abstract for a scientific paper that answers the Question:', 'Answer:')\n",
    "\n",
    "    # Combine the prompt and the response in a new column 'Text' with adjustments for specific prompts\n",
    "    df['Text'] = df.apply(\n",
    "        lambda row: (\n",
    "            'Prompt: ' + row['Prompt'].replace(' Continue the story:', '') + ' Story: ' + row['Response']\n",
    "            if row['Prompt'].endswith('Continue the story:')\n",
    "            else (\n",
    "                'Summary: ' + row['Prompt'].replace('Write a news article based on the following summary: ',\n",
    "                                                    '') + ' Article: ' + row['Response']\n",
    "                if row['Prompt'].startswith('Write a news article based on the following summary:')\n",
    "                else row['Prompt'] + ' ' + row['Response']\n",
    "            )\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Remove 'Title:' and/or 'Abstract:' if they appear after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer: (Title:|Abstract:)', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer:.*Abstract:', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears in the text\n",
    "    df['Text'] = df['Text'].str.replace('Abstract:', '', regex=False)\n",
    "\n",
    "    # Add a new column 'Label' with value 1 to each instance\n",
    "    df['Label'] = 1\n",
    "\n",
    "    # Keep only the 'Text' and 'Label' columns\n",
    "    df = df[['Text', 'Label']]\n",
    "    \n",
    "    # Print the number of entries pre-processed\n",
    "    num_entries = len(df)\n",
    "    print(f\"Number of entries pre-processed: {num_entries}\")\n",
    "\n",
    "    # Construct the output file path based on the response file path\n",
    "    base_path, extension = os.path.splitext(response_csv_path)\n",
    "    output_csv_path = f\"{base_path}_preprocessed{extension}\"\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.isfile(output_csv_path):\n",
    "        overwrite = input(f\"{output_csv_path} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e0317b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries pre-processed: 825\n",
      "extracted_data/gpt-3.5-turbo_responses_preprocessed.csv already exists. Do you want to overwrite it? (y/n): y\n"
     ]
    }
   ],
   "source": [
    "extract_and_combine(\"extracted_data/gpt-3.5-turbo_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17595",
   "metadata": {},
   "source": [
    "Calling preprocess_and_save once again with additional argument the <b> 'gpt' dataset </b> , which will perform additional preprocessing to the gpt responses and append them to the human-labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe9da800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to 'extracted_data\\gpt-3.5-turbo_and_human_data.csv' with 1650 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(gpt_dataset='gpt-3.5-turbo_responses_preprocessed.csv', gpt_dataset_path='extracted_data',\n",
    "                     output_folder='extracted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71069358",
   "metadata": {},
   "source": [
    "825 human + 825 gpt = 1650 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfb312",
   "metadata": {},
   "source": [
    "## Now there exists <b> gpt-3.5-turbo_and_human_data.csv </b> which contains our observations and labels for gpt-generated and human-generated data. The below is feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef19280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27dcdf",
   "metadata": {},
   "source": [
    "Note: need to run:\n",
    "\n",
    "\n",
    "pip install -U pip setuptools wheel\n",
    "\n",
    "\n",
    "pip install -U spacy\n",
    "\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "to get the below to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b0ebec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b2453",
   "metadata": {},
   "source": [
    "The below are functions to extract features from the combined data, please refer to the docstrings for explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a9c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    \n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        # this step has the extra effect of removing examples with low-quality/garbage content (DetectGPT)\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(embeddings1.numpy(), embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Summary:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        full_text = full_text.strip()  # Remove leading/trailing white spaces\n",
    "        text = text.strip()\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            start = start.strip()\n",
    "            end = end.strip()\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                if end in temp_prompt: # Check if end is present in temp_prompt before splitting\n",
    "                    prompt, _ = temp_prompt.split(end, 1)\n",
    "                    prompt = prompt.strip()\n",
    "                else:\n",
    "                    print(f\"WARNING: Unable to find the end string '{end}' in temp_prompt for full text: {full_text} and text: {text}\")\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for full text: {full_text} and text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f8ffe",
   "metadata": {},
   "source": [
    "## Now a function to create a data-matrix which has columns with each feature and each row is an observation from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd28f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features = {\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                    'label': label\n",
    "                }\n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca4de5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 956\n",
      "Processed row 957\n",
      "Processed row 958\n",
      "Processed row 959\n",
      "Processed row 960\n",
      "Processed row 961\n",
      "Processed row 962\n",
      "Processed row 963\n",
      "Processed row 964\n",
      "Processed row 965\n",
      "Processed row 966\n",
      "Processed row 967\n",
      "Processed row 968\n",
      "Processed row 969\n",
      "Processed row 970\n",
      "Processed row 971\n",
      "Processed row 972\n",
      "Processed row 973\n",
      "Processed row 974\n",
      "Processed row 975\n",
      "Processed row 976\n",
      "Processed row 977\n",
      "Processed row 978\n",
      "Processed row 979\n",
      "Processed row 980\n",
      "Processed row 981\n",
      "Processed row 982\n",
      "Processed row 983\n",
      "Processed row 984\n",
      "Processed row 985\n",
      "Processed row 986\n",
      "Processed row 987\n",
      "Processed row 988\n",
      "Processed row 989\n",
      "Processed row 990\n",
      "Processed row 991\n",
      "Processed row 992\n",
      "Processed row 993\n",
      "Processed row 994\n",
      "Processed row 995\n",
      "Processed row 996\n",
      "Processed row 997\n",
      "Processed row 998\n",
      "Processed row 999\n",
      "Processed row 1000\n",
      "Processed row 1001\n",
      "Processed row 1002\n",
      "Processed row 1003\n",
      "Processed row 1004\n",
      "Processed row 1005\n",
      "Processed row 1006\n",
      "Processed row 1007\n",
      "Processed row 1008\n",
      "Processed row 1009\n",
      "Processed row 1010\n",
      "Processed row 1011\n",
      "Processed row 1012\n",
      "Processed row 1013\n",
      "Processed row 1014\n",
      "Processed row 1015\n",
      "Processed row 1016\n",
      "Processed row 1017\n",
      "Processed row 1018\n",
      "Processed row 1019\n",
      "Processed row 1020\n",
      "Processed row 1021\n",
      "Processed row 1022\n",
      "Processed row 1023\n",
      "Processed row 1024\n",
      "Processed row 1025\n",
      "Processed row 1026\n",
      "Processed row 1027\n",
      "Processed row 1028\n",
      "Processed row 1029\n",
      "Processed row 1030\n",
      "Processed row 1031\n",
      "Processed row 1032\n",
      "Processed row 1033\n",
      "Processed row 1034\n",
      "Processed row 1035\n",
      "Processed row 1036\n",
      "Processed row 1037\n",
      "Processed row 1038\n",
      "Processed row 1039\n",
      "Processed row 1040\n",
      "Processed row 1041\n",
      "Processed row 1042\n",
      "Processed row 1043\n",
      "Processed row 1044\n",
      "Processed row 1045\n",
      "Processed row 1046\n",
      "Processed row 1047\n",
      "Processed row 1048\n",
      "Processed row 1049\n",
      "Processed row 1050\n",
      "Processed row 1051\n",
      "Processed row 1052\n",
      "Processed row 1053\n",
      "Processed row 1054\n",
      "Processed row 1055\n",
      "Processed row 1056\n",
      "Processed row 1057\n",
      "Processed row 1058\n",
      "Processed row 1059\n",
      "Processed row 1060\n",
      "Processed row 1061\n",
      "Processed row 1062\n",
      "Processed row 1063\n",
      "Processed row 1064\n",
      "Processed row 1065\n",
      "Processed row 1066\n",
      "Processed row 1067\n",
      "Processed row 1068\n",
      "Processed row 1069\n",
      "Processed row 1070\n",
      "Processed row 1071\n",
      "Processed row 1072\n",
      "Processed row 1073\n",
      "Processed row 1074\n",
      "Processed row 1075\n",
      "Processed row 1076\n",
      "Processed row 1077\n",
      "Processed row 1078\n",
      "Processed row 1079\n",
      "Processed row 1080\n",
      "Processed row 1081\n",
      "Processed row 1082\n",
      "Processed row 1083\n",
      "Processed row 1084\n",
      "Processed row 1085\n",
      "Processed row 1086\n",
      "Processed row 1087\n",
      "Processed row 1088\n",
      "Processed row 1089\n",
      "Processed row 1090\n",
      "Processed row 1091\n",
      "Processed row 1092\n",
      "Processed row 1093\n",
      "Processed row 1094\n",
      "Processed row 1095\n",
      "Processed row 1096\n",
      "Processed row 1097\n",
      "Processed row 1098\n",
      "Processed row 1099\n",
      "Processed row 1100\n",
      "Processed row 1101\n",
      "Processed row 1102\n",
      "Processed row 1103\n",
      "Processed row 1104\n",
      "Processed row 1105\n",
      "Processed row 1106\n",
      "Processed row 1107\n",
      "Processed row 1108\n",
      "Processed row 1109\n",
      "Processed row 1110\n",
      "Processed row 1111\n",
      "Processed row 1112\n",
      "Processed row 1113\n",
      "Processed row 1114\n",
      "Processed row 1115\n",
      "Processed row 1116\n",
      "Processed row 1117\n",
      "Processed row 1118\n",
      "Processed row 1119\n",
      "Processed row 1120\n",
      "Processed row 1121\n",
      "Processed row 1122\n",
      "Processed row 1123\n",
      "Processed row 1124\n",
      "Processed row 1125\n",
      "Processed row 1126\n",
      "Processed row 1127\n",
      "Processed row 1128\n",
      "Processed row 1129\n",
      "Processed row 1130\n",
      "Processed row 1131\n",
      "Processed row 1132\n",
      "Processed row 1133\n",
      "Processed row 1134\n",
      "Processed row 1135\n",
      "Processed row 1136\n",
      "Processed row 1137\n",
      "Processed row 1138\n",
      "Processed row 1139\n",
      "Processed row 1140\n",
      "Processed row 1141\n",
      "Processed row 1142\n",
      "Processed row 1143\n",
      "Processed row 1144\n",
      "Processed row 1145\n",
      "Processed row 1146\n",
      "Processed row 1147\n",
      "Processed row 1148\n",
      "Processed row 1149\n",
      "Processed row 1150\n",
      "Processed row 1151\n",
      "Processed row 1152\n",
      "Processed row 1153\n",
      "Processed row 1154\n",
      "Processed row 1155\n",
      "Processed row 1156\n",
      "Processed row 1157\n",
      "Processed row 1158\n",
      "Processed row 1159\n",
      "Processed row 1160\n",
      "Processed row 1161\n",
      "Processed row 1162\n",
      "Processed row 1163\n",
      "Processed row 1164\n",
      "Processed row 1165\n",
      "Processed row 1166\n",
      "Processed row 1167\n",
      "Processed row 1168\n",
      "Processed row 1169\n",
      "Processed row 1170\n",
      "Processed row 1171\n",
      "Processed row 1172\n",
      "Processed row 1173\n",
      "Processed row 1174\n",
      "Processed row 1175\n",
      "Processed row 1176\n",
      "Processed row 1177\n",
      "Processed row 1178\n",
      "Processed row 1179\n",
      "Processed row 1180\n",
      "Processed row 1181\n",
      "Processed row 1182\n",
      "Processed row 1183\n",
      "Processed row 1184\n",
      "Processed row 1185\n",
      "Processed row 1186\n",
      "Processed row 1187\n",
      "Processed row 1188\n",
      "Processed row 1189\n",
      "Processed row 1190\n",
      "Processed row 1191\n",
      "Processed row 1192\n",
      "Processed row 1193\n",
      "Processed row 1194\n",
      "Processed row 1195\n",
      "Processed row 1196\n",
      "Processed row 1197\n",
      "Processed row 1198\n",
      "Processed row 1199\n",
      "Processed row 1200\n",
      "Processed row 1201\n",
      "Processed row 1202\n",
      "Processed row 1203\n",
      "Processed row 1204\n",
      "Processed row 1205\n",
      "Processed row 1206\n",
      "Processed row 1207\n",
      "Processed row 1208\n",
      "Processed row 1209\n",
      "Processed row 1210\n",
      "Processed row 1211\n",
      "Processed row 1212\n",
      "Processed row 1213\n",
      "Processed row 1214\n",
      "Processed row 1215\n",
      "Processed row 1216\n",
      "Processed row 1217\n",
      "Processed row 1218\n",
      "Processed row 1219\n",
      "Processed row 1220\n",
      "Processed row 1221\n",
      "Processed row 1222\n",
      "Processed row 1223\n",
      "Processed row 1224\n",
      "Processed row 1225\n",
      "Processed row 1226\n",
      "Processed row 1227\n",
      "Processed row 1228\n",
      "Processed row 1229\n",
      "Processed row 1230\n",
      "Processed row 1231\n",
      "Processed row 1232\n",
      "Processed row 1233\n",
      "Processed row 1234\n",
      "Processed row 1235\n",
      "Processed row 1236\n",
      "Processed row 1237\n",
      "Processed row 1238\n",
      "Processed row 1239\n",
      "Processed row 1240\n",
      "Processed row 1241\n",
      "Processed row 1242\n",
      "Processed row 1243\n",
      "Processed row 1244\n",
      "Processed row 1245\n",
      "Processed row 1246\n",
      "Processed row 1247\n",
      "Processed row 1248\n",
      "Processed row 1249\n",
      "Processed row 1250\n",
      "Processed row 1251\n",
      "Processed row 1252\n",
      "Processed row 1253\n",
      "Processed row 1254\n",
      "Processed row 1255\n",
      "Processed row 1256\n",
      "Processed row 1257\n",
      "Processed row 1258\n",
      "Processed row 1259\n",
      "Processed row 1260\n",
      "Processed row 1261\n",
      "Processed row 1262\n",
      "Processed row 1263\n",
      "Processed row 1264\n",
      "Processed row 1265\n",
      "Processed row 1266\n",
      "Processed row 1267\n",
      "Processed row 1268\n",
      "Processed row 1269\n",
      "Processed row 1270\n",
      "Processed row 1271\n",
      "Processed row 1272\n",
      "Processed row 1273\n",
      "Processed row 1274\n",
      "Processed row 1275\n",
      "Processed row 1276\n",
      "Processed row 1277\n",
      "Processed row 1278\n",
      "Processed row 1279\n",
      "Processed row 1280\n",
      "Processed row 1281\n",
      "Processed row 1282\n",
      "Processed row 1283\n",
      "Processed row 1284\n",
      "Processed row 1285\n",
      "Processed row 1286\n",
      "Processed row 1287\n",
      "Processed row 1288\n",
      "Processed row 1289\n",
      "Processed row 1290\n",
      "Processed row 1291\n",
      "Processed row 1292\n",
      "Processed row 1293\n",
      "Processed row 1294\n",
      "Processed row 1295\n",
      "Processed row 1296\n",
      "Processed row 1297\n",
      "Processed row 1298\n",
      "Processed row 1299\n",
      "Processed row 1300\n",
      "Processed row 1301\n",
      "Processed row 1302\n",
      "Processed row 1303\n",
      "Processed row 1304\n",
      "Processed row 1305\n",
      "Processed row 1306\n",
      "Processed row 1307\n",
      "Processed row 1308\n",
      "Processed row 1309\n",
      "Processed row 1310\n",
      "Processed row 1311\n",
      "Processed row 1312\n",
      "Processed row 1313\n",
      "Processed row 1314\n",
      "Processed row 1315\n",
      "Processed row 1316\n",
      "Processed row 1317\n",
      "Processed row 1318\n",
      "Processed row 1319\n",
      "Processed row 1320\n",
      "Processed row 1321\n",
      "Processed row 1322\n",
      "Processed row 1323\n",
      "Processed row 1324\n",
      "Processed row 1325\n",
      "Processed row 1326\n",
      "Processed row 1327\n",
      "Processed row 1328\n",
      "Processed row 1329\n",
      "Processed row 1330\n",
      "Processed row 1331\n",
      "Processed row 1332\n",
      "Processed row 1333\n",
      "Processed row 1334\n",
      "Processed row 1335\n",
      "Processed row 1336\n",
      "Processed row 1337\n",
      "Processed row 1338\n",
      "Processed row 1339\n",
      "Processed row 1340\n",
      "Processed row 1341\n",
      "Processed row 1342\n",
      "Processed row 1343\n",
      "Processed row 1344\n",
      "Processed row 1345\n",
      "Processed row 1346\n",
      "Processed row 1347\n",
      "Processed row 1348\n",
      "Processed row 1349\n",
      "Processed row 1350\n",
      "Processed row 1351\n",
      "Processed row 1352\n",
      "Processed row 1353\n",
      "Processed row 1354\n",
      "Processed row 1355\n",
      "Processed row 1356\n",
      "Processed row 1357\n",
      "Processed row 1358\n",
      "Processed row 1359\n",
      "Processed row 1360\n",
      "Processed row 1361\n",
      "Processed row 1362\n",
      "Processed row 1363\n",
      "Processed row 1364\n",
      "Processed row 1365\n",
      "Processed row 1366\n",
      "Processed row 1367\n",
      "Processed row 1368\n",
      "Processed row 1369\n",
      "Processed row 1370\n",
      "Processed row 1371\n",
      "Processed row 1372\n",
      "Processed row 1373\n",
      "Processed row 1374\n",
      "Processed row 1375\n",
      "Processed row 1376\n",
      "Processed row 1377\n",
      "Processed row 1378\n",
      "Processed row 1379\n",
      "Processed row 1380\n",
      "Processed row 1381\n",
      "Processed row 1382\n",
      "Processed row 1383\n",
      "Processed row 1384\n",
      "Processed row 1385\n",
      "Processed row 1386\n",
      "Processed row 1387\n",
      "Processed row 1388\n",
      "Processed row 1389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1390\n",
      "Processed row 1391\n",
      "Processed row 1392\n",
      "Processed row 1393\n",
      "Processed row 1394\n",
      "Processed row 1395\n",
      "Processed row 1396\n",
      "Processed row 1397\n",
      "Processed row 1398\n",
      "Processed row 1399\n",
      "Processed row 1400\n",
      "Processed row 1401\n",
      "Processed row 1402\n",
      "Processed row 1403\n",
      "Processed row 1404\n",
      "Processed row 1405\n",
      "Processed row 1406\n",
      "Processed row 1407\n",
      "Processed row 1408\n",
      "Processed row 1409\n",
      "Processed row 1410\n",
      "Processed row 1411\n",
      "Processed row 1412\n",
      "Processed row 1413\n",
      "Processed row 1414\n",
      "Processed row 1415\n",
      "Processed row 1416\n",
      "Processed row 1417\n",
      "Processed row 1418\n",
      "Processed row 1419\n",
      "Processed row 1420\n",
      "Processed row 1421\n",
      "Processed row 1422\n",
      "Processed row 1423\n",
      "Processed row 1424\n",
      "Processed row 1425\n",
      "Processed row 1426\n",
      "Processed row 1427\n",
      "Processed row 1428\n",
      "Processed row 1429\n",
      "Processed row 1430\n",
      "Processed row 1431\n",
      "Processed row 1432\n",
      "Processed row 1433\n",
      "Processed row 1434\n",
      "Processed row 1435\n",
      "Processed row 1436\n",
      "Processed row 1437\n",
      "Processed row 1438\n",
      "Processed row 1439\n",
      "Processed row 1440\n",
      "Processed row 1441\n",
      "Processed row 1442\n",
      "Processed row 1443\n",
      "Processed row 1444\n",
      "Processed row 1445\n",
      "Processed row 1446\n",
      "Processed row 1447\n",
      "Processed row 1448\n",
      "Processed row 1449\n",
      "Processed row 1450\n",
      "Processed row 1451\n",
      "Processed row 1452\n",
      "Processed row 1453\n",
      "Processed row 1454\n",
      "Processed row 1455\n",
      "Processed row 1456\n",
      "Processed row 1457\n",
      "Processed row 1458\n",
      "Processed row 1459\n",
      "Processed row 1460\n",
      "Processed row 1461\n",
      "Processed row 1462\n",
      "Processed row 1463\n",
      "Processed row 1464\n",
      "Processed row 1465\n",
      "Processed row 1466\n",
      "Processed row 1467\n",
      "Processed row 1468\n",
      "Processed row 1469\n",
      "Processed row 1470\n",
      "Processed row 1471\n",
      "Processed row 1472\n",
      "Processed row 1473\n",
      "Processed row 1474\n",
      "Processed row 1475\n",
      "Processed row 1476\n",
      "Processed row 1477\n",
      "Processed row 1478\n",
      "Processed row 1479\n",
      "Processed row 1480\n",
      "Processed row 1481\n",
      "Processed row 1482\n",
      "Processed row 1483\n",
      "Processed row 1484\n",
      "Processed row 1485\n",
      "Processed row 1486\n",
      "Processed row 1487\n",
      "Processed row 1488\n",
      "Processed row 1489\n",
      "Processed row 1490\n",
      "Processed row 1491\n",
      "Processed row 1492\n",
      "Processed row 1493\n",
      "Processed row 1494\n",
      "Processed row 1495\n",
      "Processed row 1496\n",
      "Processed row 1497\n",
      "Processed row 1498\n",
      "Processed row 1499\n",
      "Processed row 1500\n",
      "Processed row 1501\n",
      "Processed row 1502\n",
      "Processed row 1503\n",
      "Processed row 1504\n",
      "Processed row 1505\n",
      "Processed row 1506\n",
      "Processed row 1507\n",
      "Processed row 1508\n",
      "Processed row 1509\n",
      "Processed row 1510\n",
      "Processed row 1511\n",
      "Processed row 1512\n",
      "Processed row 1513\n",
      "Processed row 1514\n",
      "Processed row 1515\n",
      "Processed row 1516\n",
      "Processed row 1517\n",
      "Processed row 1518\n",
      "Processed row 1519\n",
      "Processed row 1520\n",
      "Processed row 1521\n",
      "Processed row 1522\n",
      "Processed row 1523\n",
      "Processed row 1524\n",
      "Processed row 1525\n",
      "Processed row 1526\n",
      "Processed row 1527\n",
      "Processed row 1528\n",
      "Processed row 1529\n",
      "Processed row 1530\n",
      "Processed row 1531\n",
      "Processed row 1532\n",
      "Processed row 1533\n",
      "Processed row 1534\n",
      "Processed row 1535\n",
      "Processed row 1536\n",
      "Processed row 1537\n",
      "Processed row 1538\n",
      "Processed row 1539\n",
      "Processed row 1540\n",
      "Processed row 1541\n",
      "Processed row 1542\n",
      "Processed row 1543\n",
      "Processed row 1544\n",
      "Processed row 1545\n",
      "Processed row 1546\n",
      "Processed row 1547\n",
      "Processed row 1548\n",
      "Processed row 1549\n",
      "Processed row 1550\n",
      "Processed row 1551\n",
      "Processed row 1552\n",
      "Processed row 1553\n",
      "Processed row 1554\n",
      "Processed row 1555\n",
      "Processed row 1556\n",
      "Processed row 1557\n",
      "Processed row 1558\n",
      "Processed row 1559\n",
      "Processed row 1560\n",
      "Processed row 1561\n",
      "Processed row 1562\n",
      "Processed row 1563\n",
      "Processed row 1564\n",
      "Processed row 1565\n",
      "Processed row 1566\n",
      "Processed row 1567\n",
      "Processed row 1568\n",
      "Processed row 1569\n",
      "Processed row 1570\n",
      "Processed row 1571\n",
      "Processed row 1572\n",
      "Processed row 1573\n",
      "Processed row 1574\n",
      "Processed row 1575\n",
      "Processed row 1576\n",
      "Processed row 1577\n",
      "Processed row 1578\n",
      "Processed row 1579\n",
      "Processed row 1580\n",
      "Processed row 1581\n",
      "Processed row 1582\n",
      "Processed row 1583\n",
      "Processed row 1584\n",
      "Processed row 1585\n",
      "Processed row 1586\n",
      "Processed row 1587\n",
      "Processed row 1588\n",
      "Processed row 1589\n",
      "Processed row 1590\n",
      "Processed row 1591\n",
      "Processed row 1592\n",
      "Processed row 1593\n",
      "Processed row 1594\n",
      "Processed row 1595\n",
      "Processed row 1596\n",
      "Processed row 1597\n",
      "Processed row 1598\n",
      "Processed row 1599\n",
      "Processed row 1600\n",
      "Processed row 1601\n",
      "Processed row 1602\n",
      "Processed row 1603\n",
      "Processed row 1604\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprepare_data_for_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_data/gpt-3.5-turbo_and_human_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 66\u001b[0m, in \u001b[0;36mprepare_data_for_regression\u001b[1;34m(data_file, save_file, chunk_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 66\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [calculate_perplexity(sentence\u001b[38;5;241m.\u001b[39mtext, model, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     67\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     68\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     69\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     70\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     64\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 66\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     67\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     68\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     69\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     70\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 195\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[1;34m(text, model, tokenizer)\u001b[0m\n\u001b[0;32m    192\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, :\u001b[38;5;241m512\u001b[39m]\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    197\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1100\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1114\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:466\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:377\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 377\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    379\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepare_data_for_regression(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e41b",
   "metadata": {},
   "source": [
    "Result is storred in a file called <b> data_matrix_gpt3.5-turbo.csv <b>, which I can use in a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e6a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6efff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabc1dc0",
   "metadata": {},
   "source": [
    "## Getting code for TF-IDF for synonyms of 'summarise' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfbfba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return list(set(synonyms))  # use set to remove duplicates\n",
    "\n",
    "# To get synonyms for 'conclude'\n",
    "synonyms_conclude = get_synonyms('conclude')\n",
    "\n",
    "# To get synonyms for 'summarize'\n",
    "synonyms_summarize = get_synonyms('summarize')\n",
    "\n",
    "# Concatenate the lists\n",
    "synonyms = synonyms_conclude + synonyms_summarize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
