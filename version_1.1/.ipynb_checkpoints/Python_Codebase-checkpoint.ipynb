{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaab8d0c",
   "metadata": {},
   "source": [
    "# Python Scripts for Data Generation, Pre-Processing & Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527a0e1",
   "metadata": {},
   "source": [
    "## 1. Data Generation\n",
    "\n",
    "The datasets used in this work were selected on the basis that they provide a from of 'question' and 'response', which can easily be extracted. The 'questions' were used to prompt the LLM models (GPT-3.5-turbo, GPT2, GPT-J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdcea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import datasets\n",
    "import re\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4ebb8",
   "metadata": {},
   "source": [
    "Need to download the WritingPrompts data from [here](https://www.kaggle.com/datasets/ratthachat/writing-prompts). Save the data into a directory: <b>data/writingPrompts </b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8122ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASETS = ['pubmed_qa', 'writingprompts', 'cnn_dailymail', 'gpt']\n",
    "DATA_PATH = './data/writingPrompts' #This is required to load the writingPrompts dataset, as it is not part of the 'datasets' library, \n",
    "NUM_EXAMPLES = 300 #Number of initial samples from each dataset, note below, the actual number of samples is ~825 due to filtering\n",
    "TAGS = ['[ WP ]', '[ OT ]', '[ IP ]', '[ HP ]', '[ TT ]', '[ Punch ]', '[ FF ]', '[ CW ]', '[ EU ]', '[ CC ]', '[ RF ]',\n",
    "        '[ wp ]', '[ Wp ]', '[ RF ]', '[ WP/MP ]']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c918e0",
   "metadata": {},
   "source": [
    "Defining some helper functions, see docstrings for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff5c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_newlines(text):\n",
    "    \"\"\"\n",
    "    Removes newline characters from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with newline characters removed.\n",
    "    \"\"\"\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "def replace_text(text, replacements):\n",
    "    \"\"\"\n",
    "    Performs a series of replacements in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "        replacements (dict): Dictionary mapping old substring to new substring.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with specified replacements made.\n",
    "    \"\"\"\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_whitespace_before_punctuations(text):\n",
    "    \"\"\"\n",
    "    Removes whitespace before punctuation marks in a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text string.\n",
    "\n",
    "    Returns:\n",
    "        str: Text with whitespace removed before punctuation marks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s([?.!,:;](?:\\s|$))', r'\\1', text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9423d5e",
   "metadata": {},
   "source": [
    "Functions to load the relevant dataset(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0906df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the PubMed QA dataset.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a question-answer pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('pubmed_qa', 'pqa_labeled', split=f'train[:{num_examples}]')\n",
    "    data = [(f'Question: {q} Answer: {a}', 0) for q, a in zip(data['question'], data['long_answer'])]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_gpt(file_name):\n",
    "    \"\"\"\n",
    "    Loads the GPT preprocessed dataset.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a text-label pair.\n",
    "    \"\"\"\n",
    "    if not file_name.endswith('.csv'):\n",
    "        file_name += '.csv'\n",
    "\n",
    "    if not os.path.exists(file_name):\n",
    "        raise FileNotFoundError(f\"The file '{file_name}' does not exist.\")\n",
    "\n",
    "    df = pd.read_csv(file_name)\n",
    "    data = [(row['Text'], row['Label']) for index, row in df.iterrows()]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_writingPrompts(data_path=DATA_PATH, num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the WritingPrompts dataset. Combines Prompts and Stories with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        data_path (str, optional): Path to the dataset. Defaults to DATA_PATH.\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a prompt-story pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    with open(f'{data_path}/valid.wp_source', 'r', encoding='utf-8') as f:\n",
    "        prompts = f.readlines()[:num_examples]\n",
    "    with open(f'{data_path}/valid.wp_target', 'r', encoding='utf-8') as f:\n",
    "        stories = f.readlines()[:num_examples]\n",
    "\n",
    "    prompt_replacements = {tag: '' for tag in TAGS}\n",
    "    prompts = [replace_text(prompt, prompt_replacements) for prompt in prompts]\n",
    "    prompts = [remove_whitespace_before_punctuations(prompt) for prompt in prompts]\n",
    "\n",
    "    story_replacements = {\n",
    "        ' ,': ',',\n",
    "        ' .': '.',\n",
    "        ' ?': '?',\n",
    "        ' !': '!',\n",
    "        ' ;': ';',\n",
    "        ' \\'': '\\'',\n",
    "        ' â€™ ': '\\'',\n",
    "        ' :': ':',\n",
    "        '<newline>': '\\n',\n",
    "        '`` ': '\"',\n",
    "        ' \\'\\'': '\"',\n",
    "        '\\'\\'': '\"',\n",
    "        '.. ': '... ',\n",
    "        ' )': ')',\n",
    "        '( ': '(',\n",
    "        ' n\\'t': 'n\\'t',\n",
    "        ' i ': ' I ',\n",
    "        ' i\\'': ' I\\'',\n",
    "        '\\\\\\'': '\\'',\n",
    "        '\\n ': '\\n',\n",
    "    }\n",
    "    stories = [replace_text(story, story_replacements).strip() for story in stories]\n",
    "    joined = [\"Prompt:\" + prompt + \" Story: \" + story for prompt, story in zip(prompts, stories)]\n",
    "    filtered = [story for story in joined if 'nsfw' not in story.lower()]\n",
    "    data = [(story, 0) for story in filtered]\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_cnn_daily_mail(num_examples=NUM_EXAMPLES):\n",
    "    \"\"\"\n",
    "    Loads the CNN/Daily Mail dataset. Combines article and summary with additional formatting.\n",
    "\n",
    "    Args:\n",
    "        num_examples (int, optional): Number of examples to load. Defaults to NUM_EXAMPLES.\n",
    "\n",
    "    Returns:\n",
    "        list: List of tuples where each tuple is a summary-article pair and a label (always 0).\n",
    "    \"\"\"\n",
    "    data = datasets.load_dataset('cnn_dailymail', '3.0.0', split=f'train[:{num_examples}]')\n",
    "\n",
    "    processed_data = []\n",
    "    for a, s in zip(data['article'], data['highlights']):\n",
    "        # remove the string and the '--' from the start of the articles\n",
    "        a = re.sub('^[^-]*--', '', a).strip()\n",
    "\n",
    "        # remove the string 'E-mail to a friend.' from the articles, if present\n",
    "        a = a.replace('E-mail to a friend .', '')\n",
    "        s = s.replace('NEW:', '')\n",
    "        a = a.replace(\n",
    "            'Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, '\n",
    "            'or redistributed.',\n",
    "            '')\n",
    "\n",
    "        # remove whitespace before punctuation marks in both article and summary\n",
    "        a = remove_whitespace_before_punctuations(a)\n",
    "        s = remove_whitespace_before_punctuations(s)\n",
    "\n",
    "        processed_data.append((f'Summary: {s} Article: {a}', 0))\n",
    "        data = processed_data\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_data(dataset_name, gpt_filename=None):\n",
    "    \"\"\"\n",
    "       Loads a dataset based on its name.\n",
    "\n",
    "       Args:\n",
    "           dataset_name (str): Name of the dataset to load.\n",
    "           gpt_filename (str, optional): Name of the csv file containing the GPT dataset.\n",
    "\n",
    "       Returns:\n",
    "           list: List of data from the specified dataset.\n",
    "\n",
    "       Raises:\n",
    "           ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset_name == 'pubmed_qa':\n",
    "        return load_pubmed()\n",
    "    elif dataset_name == 'writingprompts':\n",
    "        return load_writingPrompts()\n",
    "    elif dataset_name == 'cnn_dailymail':\n",
    "        return load_cnn_daily_mail()\n",
    "    elif dataset_name == 'gpt':\n",
    "        if gpt_filename is None:\n",
    "            raise ValueError(\"A filename must be provided to load the GPT dataset.\")\n",
    "        return load_gpt(gpt_filename)\n",
    "    else:\n",
    "        raise ValueError(f\"Dataset name {dataset_name} not recognized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca534d31",
   "metadata": {},
   "source": [
    "Functions to ensure the each part of the combined dataset is in the same format (no unnecessary whitespaces etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18cc99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "        Preprocesses a dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (str): Name of the dataset to preprocess.\n",
    "\n",
    "        Returns:\n",
    "            list: List of preprocessed data from the specified dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset_name is not recognized.\n",
    "    \"\"\"\n",
    "    if dataset not in DATASETS:\n",
    "        raise ValueError(f\"Dataset name {dataset} not recognized.\")\n",
    "\n",
    "    data = load_data(dataset)\n",
    "    data = list(dict.fromkeys(data))\n",
    "    data = [(strip_newlines(q).strip(), a) for q, a in data]\n",
    "\n",
    "    # Getting long-enough data, not done for PubMed due to most of the responses being fairly short.\n",
    "    # This is consistent with most research approaches concering these datasets (DetectGPT paper e.g.)\n",
    "    if dataset == 'writingprompts' or dataset == 'cnn_dailymail':\n",
    "        long_data = [(x, y) for x, y in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")  # debug\n",
    "        # print\n",
    "    else:\n",
    "        print(f\"Loaded and pre-processed {len(data)} entries from the dataset {dataset}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_and_save(gpt_dataset=None, gpt_dataset_path=None, output_folder='extracted_data'):\n",
    "    \"\"\"\n",
    "    Preprocesses the datasets, combines them, and saves the result to a .csv file.\n",
    "    Optional argument gpt_dataset allows preprocessing the GPT dataset and combining it with existing datasets.\n",
    "\n",
    "    Args:\n",
    "        gpt_dataset (str, optional): Name of the GPT dataset csv file (without the .csv extension).\n",
    "        gpt_dataset_path (str, optional): Path to the GPT dataset.\n",
    "        output_folder: folder where the extracted data will be saved\n",
    "\n",
    "    Returns:\n",
    "        None, saves the combined data to a .csv file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if gpt_dataset:\n",
    "        # Load and preprocess the GPT dataset\n",
    "        gpt_data_path = os.path.join(gpt_dataset_path, gpt_dataset)\n",
    "        gpt_data = load_data('gpt', gpt_data_path)\n",
    "        gpt_data = list(dict.fromkeys(gpt_data))\n",
    "        gpt_data = [(strip_newlines(q).strip(), a) for q, a in gpt_data]\n",
    "\n",
    "        # Load the already preprocessed data from the other datasets\n",
    "        combined_df = pd.read_csv(os.path.join(output_folder, 'combined_human_data.csv'))\n",
    "        combined_data = list(zip(combined_df['Text'], combined_df['Label']))\n",
    "\n",
    "        # Combine the data\n",
    "        combined_data += gpt_data\n",
    "\n",
    "        model_name = gpt_dataset.split('_')[0]  # Extract model name from gpt_dataset\n",
    "\n",
    "        output_file = f'{model_name}_and_human_data.csv'\n",
    "\n",
    "    else:\n",
    "        # Preprocess all the datasets\n",
    "        pubmed_data = preprocess_data('pubmed_qa')\n",
    "        writingprompts_data = preprocess_data('writingprompts')\n",
    "        cnn_daily_mail_data = preprocess_data('cnn_dailymail')\n",
    "\n",
    "        combined_data = pubmed_data + writingprompts_data + cnn_daily_mail_data\n",
    "\n",
    "        output_file = 'combined_human_data.csv'\n",
    "\n",
    "    output_file_path = os.path.join(output_folder, output_file)\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        overwrite = input(f\"'{output_file_path}' already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(f\"Not overwriting existing file '{output_file_path}'. Exiting...\")\n",
    "            return\n",
    "\n",
    "    # Save the combined data to a .csv file\n",
    "    df = pd.DataFrame(combined_data, columns=['Text', 'Label'])\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Combined dataset saved to '{output_file_path}' with {len(combined_data)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f15524da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset pubmed_qa (C:/Users/atana/.cache/huggingface/datasets/pubmed_qa/pqa_labeled/1.0.0/dd4c39f031a958c7e782595fa4dd1b1330484e8bbadd4d9212e5046f27e68924)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 300 entries from the dataset pubmed_qa\n",
      "Loaded and pre-processed 249 entries from the dataset writingprompts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (C:/Users/atana/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded and pre-processed 276 entries from the dataset cnn_dailymail\n",
      "'extracted_data\\combined_human_data.csv' already exists. Do you want to overwrite it? (y/n): y\n",
      "Combined dataset saved to 'extracted_data\\combined_human_data.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(output_folder = 'extracted_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3da4f6",
   "metadata": {},
   "source": [
    "Now we can load and pre-process <b> PubMed, WritingPrompts and CNN_DailyMail </b> the labelled 'human' data. A file called <b> combined_human_data.csv</b> will be storred in the data folder called <b>extracted_data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074feb5",
   "metadata": {},
   "source": [
    "Using the <b> combined_human_data.csv </b> the 'questions' will be extracted , to be used as prompts for the LLMs.\n",
    "Note: <b> Because the answers from PubMed are essentially abstracts from papers the prompt has been altered </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d1c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompts_and_save(file_folder_path):\n",
    "    \"\"\"\n",
    "    Extracts prompts from the combined dataset and saves them to a .csv file.\n",
    "\n",
    "    Args:\n",
    "        file_folder_path (str): The path to the folder where the combined_source_data.csv file is located.\n",
    "\n",
    "    Returns:\n",
    "        None, saves the prompts to a .csv file.\n",
    "    \"\"\"\n",
    "    # Load the combined dataset\n",
    "    combined_data_file = os.path.join(file_folder_path, 'combined_human_data.csv')\n",
    "    df = pd.read_csv(combined_data_file)\n",
    "    combined_data = list(zip(df['Text'], df['Label']))\n",
    "\n",
    "    # Extract prompts from the combined data\n",
    "    prompts = []\n",
    "    for i, (full_text, _) in enumerate(combined_data):\n",
    "        if i < 300:\n",
    "            prompt = full_text.replace('Answer:', 'Write an abstract for a scientific paper that answers the Question:')\n",
    "            prompt = prompt.split('Write an abstract for a scientific paper that answers the Question:')[0] + \\\n",
    "                     'Write an abstract for a scientific paper that answers the Question:'\n",
    "            prompts.append(prompt.strip())\n",
    "        elif 'Summary:' in full_text and 'Article:' in full_text:\n",
    "            prompts.append('Write a news article based on the following summary: ' +\n",
    "                           full_text.split('Summary:')[1].split('Article:')[0].strip())\n",
    "        elif 'Prompt:' in full_text and 'Story:' in full_text:\n",
    "            prompts.append(full_text.replace('Prompt:', '').split('Story:')[0].strip() + ' Continue the story:')\n",
    "        else:\n",
    "            print(f\"Could not determine dataset for the entry: {full_text}\")\n",
    "\n",
    "    # Save the prompts to a new CSV file\n",
    "    df_prompts = pd.DataFrame(prompts, columns=['Prompt'])\n",
    "    df_prompts.to_csv(os.path.join(file_folder_path, 'prompts.csv'), index=False)\n",
    "    print(f\"Prompts extracted and saved to '{os.path.join(file_folder_path, 'prompts.csv')}' with {len(df_prompts)}\"\n",
    "          f\" entries.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8770c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts extracted and saved to 'extracted_data\\prompts.csv' with 825 entries.\n"
     ]
    }
   ],
   "source": [
    "extract_prompts_and_save(\"extracted_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044dfdb3",
   "metadata": {},
   "source": [
    "Creates a file called <b>prompts.csv</b> that contains all prompts to be used in the LLMs. 825 entries, consistent with before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad2026",
   "metadata": {},
   "source": [
    "## Generating GPT-3.5/LLMs responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7308415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import openai\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import time \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54f33313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 10  # Define the batch size\n",
    "openai.api_key = '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' #Insert your API key here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "980b20f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt3_responses(prompt_csv_path, response_folder_path, model=\"gpt-3.5-turbo\", temperature=1):\n",
    "    \"\"\"\n",
    "    Generate GPT-3 responses for a list of prompts saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model (str, optional): The ID of the model to use. Defaults to \"gpt-3.5-turbo\".\n",
    "        temperature (float, optional): Determines the randomness of the AI's output. Defaults to 1, as per OpenAI docs.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Initialize the starting point\n",
    "    start = 0\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # If so, get the number of completed prompts from the file\n",
    "        with open(response_csv_path, \"r\", newline=\"\", encoding='utf-8') as file:\n",
    "            start = sum(1 for row in csv.reader(file)) - 1  # Subtract 1 for the header\n",
    "\n",
    "    while start < len(prompts):\n",
    "        try:\n",
    "            # Process the remaining prompts in batches\n",
    "            for i in range(start, len(prompts), BATCH_SIZE):\n",
    "                batch = prompts[i:i + BATCH_SIZE]\n",
    "                responses = []\n",
    "\n",
    "                for prompt in batch:\n",
    "                    # Generate the response\n",
    "                    response = openai.ChatCompletion.create(\n",
    "                        model=model,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                        ],\n",
    "                        temperature=temperature\n",
    "                    )\n",
    "\n",
    "                    # Append the response to the list\n",
    "                    responses.append('<<RESP>> ' + response['choices'][0]['message']['content'].strip())\n",
    "\n",
    "                # Save the responses to a new DataFrame\n",
    "                response_df = pd.DataFrame({\n",
    "                    'Prompt': batch,\n",
    "                    'Response': responses\n",
    "                })\n",
    "\n",
    "                # Write the DataFrame to the CSV file, appending if it already exists\n",
    "                if os.path.exists(response_csv_path):\n",
    "                    response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "                else:\n",
    "                    response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "                print(f\"Batch {i // BATCH_SIZE + 1} completed\")\n",
    "                start = i + BATCH_SIZE\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Sleeping for 10 seconds before retrying...\")\n",
    "            time.sleep(10)  # wait for 10 seconds before retrying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbdf6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 completed\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_gpt3_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted_data/prompts.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mextracted_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[56], line 40\u001b[0m, in \u001b[0;36mgenerate_gpt3_responses\u001b[1;34m(prompt_csv_path, response_folder_path, model, temperature)\u001b[0m\n\u001b[0;32m     36\u001b[0m responses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Generate the response\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Append the response to the list\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<<RESP>> \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 288\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupplied_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[1;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    594\u001b[0m     _thread_context\u001b[38;5;241m.\u001b[39msession_create_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 596\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mabs_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mTIMEOUT_SECS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_thread_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mTimeout(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest timed out: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m         resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m            \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m            \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[0;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1373\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1274\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1272\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1273\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\ssl.py:1130\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_gpt3_responses('extracted_data/prompts.csv', 'extracted_data', temperature=1) #temeprature is arbirtary this is the default value as per OpenAI docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79885a8c",
   "metadata": {},
   "source": [
    "Responses will be saved in <b>got-3.5-turbo_responses.csv </b> file. However they must be further processed to ensure compatability with existing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41271afd",
   "metadata": {},
   "source": [
    "Doing the same thing for gpt-2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc7cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gpt2_responses(prompt_csv_path, response_folder_path, model_name):\n",
    "    \"\"\"\n",
    "    Generate responses for a list of prompts saved in a csv file using a GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prompt_csv_path (str): Path to the csv file containing the prompts.\n",
    "        response_folder_path (str): Path to the folder where the responses will be saved.\n",
    "        model_name (str): Name of the GPT-2 model to use (for example, \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\").\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the responses.\n",
    "    \"\"\"\n",
    "    # Define acceptable models\n",
    "    acceptable_models = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "\n",
    "    if model_name not in acceptable_models:\n",
    "        raise ValueError(f\"Invalid model name. Acceptable models are: {', '.join(acceptable_models)}\")\n",
    "\n",
    "    # Load the GPT-2 model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Load the prompts\n",
    "    df = pd.read_csv(prompt_csv_path)\n",
    "    prompts = df['Prompt'].tolist()\n",
    "\n",
    "    # Construct the response file path\n",
    "    response_csv_path = os.path.join(response_folder_path, f\"{model_name}_responses.csv\")\n",
    "\n",
    "    # Check if the response file already exists\n",
    "    if os.path.exists(response_csv_path):\n",
    "        # Load the existing responses\n",
    "        existing_responses_df = pd.read_csv(response_csv_path)\n",
    "\n",
    "        # Determine the starting point based on the number of existing responses\n",
    "        start = len(existing_responses_df)\n",
    "    else:\n",
    "        start = 0\n",
    "\n",
    "    for i in range(start, len(prompts)):\n",
    "        # Encode the prompt\n",
    "        input_ids = tokenizer.encode(prompts[i], return_tensors=\"pt\")\n",
    "\n",
    "        # Generate a response\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "            pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "            do_sample=True,\n",
    "            max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "        )\n",
    "\n",
    "        # Calculate the number of tokens in the prompt\n",
    "        prompt_length = input_ids.shape[-1]\n",
    "\n",
    "        # Decode only the response, excluding the prompt\n",
    "        response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "        # Save the prompt and response to a DataFrame\n",
    "        response_df = pd.DataFrame({\n",
    "            'Prompt': [prompts[i]],\n",
    "            'Response': [response]\n",
    "        })\n",
    "\n",
    "        # Append the DataFrame to the CSV file\n",
    "        if os.path.exists(response_csv_path):\n",
    "            response_df.to_csv(response_csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            response_df.to_csv(response_csv_path, mode='w', index=False)\n",
    "\n",
    "        print(f\"Prompt {i + 1} of {len(prompts)} processed\")\n",
    "\n",
    "    print(f\"All prompts processed. Responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99484560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gpt2_responses(\"extracted_data/prompts.csv\", \"extracted_data\",model_name='gpt2-large')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e7597",
   "metadata": {},
   "source": [
    "Function above sometimes generates empty responses, hence a function to check an re-generate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abfbef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regenerate_responses(response_csv_path):\n",
    "    \"\"\"\n",
    "    Check the csv file containing generated responses for any NaN values.\n",
    "    If any are found, regenerate the responses using the provided model.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, updates the csv file with the regenerated responses.\n",
    "    \"\"\"\n",
    "    # Extract the model name from the filename\n",
    "    model_name = os.path.basename(response_csv_path).split('_')[0]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    print(f\"Loaded model {model_name}\")\n",
    "\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Iterate over the DataFrame\n",
    "    for i, row in df.iterrows():\n",
    "        if pd.isnull(row['Response']):\n",
    "            # Encode the prompt\n",
    "            input_ids = tokenizer.encode(row['Prompt'], return_tensors=\"pt\")\n",
    "\n",
    "            # Generate a response\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                attention_mask=torch.ones_like(input_ids),  # Set all positions to 1 (i.e., no padding)\n",
    "                pad_token_id=tokenizer.eos_token_id,  # Use the EOS token as the PAD token\n",
    "                do_sample=True,\n",
    "                max_length=1024,  # Use GPT-2's maximum sequence length\n",
    "            )\n",
    "\n",
    "            # Calculate the number of tokens in the prompt\n",
    "            prompt_length = input_ids.shape[-1]\n",
    "\n",
    "            # Decode only the response, excluding the prompt\n",
    "            response = tokenizer.decode(output[0, prompt_length:], skip_special_tokens=True)\n",
    "\n",
    "            # Replace the NaN response with the new one\n",
    "            df.at[i, 'Response'] = response\n",
    "\n",
    "            # Save the DataFrame back to the CSV file\n",
    "            df.to_csv(response_csv_path, index=False)\n",
    "\n",
    "            print(\n",
    "                f\"Regenerated response for prompt {i + 1} of {len(df)}. Updated responses saved to {response_csv_path}.\")\n",
    "\n",
    "    print(f\"All NaN responses regenerated. Updated responses saved to {response_csv_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422a421",
   "metadata": {},
   "source": [
    "Now we will have a file <b> gpt2-large_responses.csv </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6cf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if no NaN responses:\n",
    "# regenerate_responses('extracted_data/gpt2-large_responses.csv')\n",
    "\n",
    "\n",
    "#\n",
    "# df = pd.read_csv(\"extracted_data/gpt2-large_responses.csv\")\n",
    "# nan_rows = df[df.isna().any(axis=1)]\n",
    "# print(nan_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d61e8d",
   "metadata": {},
   "source": [
    "Now , formatting the GPT-3.5-turbo responses data so it is the same format as our human data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "485b19f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_combine(response_csv_path):\n",
    "    \"\"\"\n",
    "    Load 'Prompt' and 'Response' from the generated responses csv file, remove the '<<RESP>>' string,\n",
    "    adjust the format to match the original datasets, add a label 1 to every instance,\n",
    "    and save to a new csv file.\n",
    "\n",
    "    Args:\n",
    "        response_csv_path (str): Path to the csv file containing the generated responses.\n",
    "\n",
    "    Returns:\n",
    "        None, generates a csv file with the combined text and labels.\n",
    "    \"\"\"\n",
    "    # Load the responses\n",
    "    df = pd.read_csv(response_csv_path)\n",
    "\n",
    "    # Remove the '<<RESP>>' string from each response\n",
    "    df['Response'] = df['Response'].str.replace('<<RESP>> ', '')\n",
    "\n",
    "    # Replace the specific string in the prompt\n",
    "    df['Prompt'] = df['Prompt'].str.replace(\n",
    "        'Write an abstract for a scientific paper that answers the Question:', 'Answer:')\n",
    "\n",
    "    # Combine the prompt and the response in a new column 'Text' with adjustments for specific prompts\n",
    "    df['Text'] = df.apply(\n",
    "        lambda row: (\n",
    "            'Prompt: ' + row['Prompt'].replace(' Continue the story:', '') + ' Story: ' + row['Response']\n",
    "            if row['Prompt'].endswith('Continue the story:')\n",
    "            else (\n",
    "                'Summary: ' + row['Prompt'].replace('Write a news article based on the following summary: ',\n",
    "                                                    '') + ' Article: ' + row['Response']\n",
    "                if row['Prompt'].startswith('Write a news article based on the following summary:')\n",
    "                else row['Prompt'] + ' ' + row['Response']\n",
    "            )\n",
    "        ), axis=1\n",
    "    )\n",
    "\n",
    "    # Remove 'Title:' and/or 'Abstract:' if they appear after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer: (Title:|Abstract:)', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears after 'Answer:'\n",
    "    df['Text'] = df['Text'].str.replace(r'Answer:.*Abstract:', 'Answer:', regex=True)\n",
    "    \n",
    "    # Remove 'Abstract:' if it appears in the text\n",
    "    df['Text'] = df['Text'].str.replace('Abstract:', '', regex=False)\n",
    "\n",
    "    # Add a new column 'Label' with value 1 to each instance\n",
    "    df['Label'] = 1\n",
    "\n",
    "    # Keep only the 'Text' and 'Label' columns\n",
    "    df = df[['Text', 'Label']]\n",
    "    \n",
    "    # Print the number of entries pre-processed\n",
    "    num_entries = len(df)\n",
    "    print(f\"Number of entries pre-processed: {num_entries}\")\n",
    "\n",
    "    # Construct the output file path based on the response file path\n",
    "    base_path, extension = os.path.splitext(response_csv_path)\n",
    "    output_csv_path = f\"{base_path}_preprocessed{extension}\"\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if os.path.isfile(output_csv_path):\n",
    "        overwrite = input(f\"{output_csv_path} already exists. Do you want to overwrite it? (y/n): \")\n",
    "        if overwrite.lower() != 'y':\n",
    "            print(\"Operation cancelled.\")\n",
    "            return\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5e0317b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries pre-processed: 825\n",
      "extracted_data/gpt-3.5-turbo_responses_preprocessed.csv already exists. Do you want to overwrite it? (y/n): y\n"
     ]
    }
   ],
   "source": [
    "extract_and_combine(\"extracted_data/gpt-3.5-turbo_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db17595",
   "metadata": {},
   "source": [
    "Calling preprocess_and_save once again with additional argument the <b> 'gpt' dataset </b> , which will perform additional preprocessing to the gpt responses and append them to the human-labelled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe9da800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to 'extracted_data\\gpt-3.5-turbo_and_human_data.csv' with 1650 entries.\n"
     ]
    }
   ],
   "source": [
    "preprocess_and_save(gpt_dataset='gpt-3.5-turbo_responses_preprocessed.csv', gpt_dataset_path='extracted_data',\n",
    "                     output_folder='extracted_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71069358",
   "metadata": {},
   "source": [
    "825 human + 825 gpt = 1650 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfb312",
   "metadata": {},
   "source": [
    "## Now there exists <b> gpt-3.5-turbo_and_human_data.csv </b> which contains our observations and labels for gpt-generated and human-generated data. The below is feature extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef19280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from statistics import mean\n",
    "import textstat\n",
    "from sklearn.preprocessing import normalize\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd27dcdf",
   "metadata": {},
   "source": [
    "Note: need to run:\n",
    "\n",
    "\n",
    "pip install -U pip setuptools wheel\n",
    "\n",
    "\n",
    "pip install -U spacy\n",
    "\n",
    "\n",
    "python -m spacy download en_core_web_sm\n",
    "\n",
    "\n",
    "\n",
    "to get the below to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0ebec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "FUNCTION_WORDS = {'a', 'in', 'of', 'the'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b2453",
   "metadata": {},
   "source": [
    "The below are functions to extract features from the combined data, please refer to the docstrings for explanations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a9c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(data):\n",
    "    \"\"\"\n",
    "    This function removes a predefined prefix from each text in a given dataset.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "    is the text and the second element is its label.\n",
    "\n",
    "    Returns:\n",
    "    texts (list): The list of texts after the prefix has been removed.\n",
    "    labels (list): The list of labels corresponding to the texts.\n",
    "    \"\"\"\n",
    "\n",
    "    texts, labels = zip(*data)\n",
    "\n",
    "    prefixes = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        texts = [text.split(prefix, 1)[1].strip() if prefix in text else text for text in texts]\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "def count_pos_tags_and_special_elements(text):\n",
    "    \n",
    "    \"\"\"\n",
    "      This function counts the frequency of POS (Part of Speech) tags, punctuation marks, and function words in a given text.\n",
    "      It uses the SpaCy library for POS tagging.\n",
    "\n",
    "      Args:\n",
    "      text (str): The text for which to count POS tags and special elements.\n",
    "\n",
    "      Returns:\n",
    "      pos_counts (dict): A dictionary where keys are POS tags and values are their corresponding count.\n",
    "      punctuation_counts (dict): A dictionary where keys are punctuation marks and values are their corresponding count.\n",
    "      function_word_counts (dict): A dictionary where keys are function words and values are their corresponding count.\n",
    "\n",
    "    \"\"\"\n",
    "    # Use SpaCy to parse the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create a counter of POS tags\n",
    "    pos_counts = Counter(token.pos_ for token in doc)\n",
    "\n",
    "    # Create a counter of punctuation marks\n",
    "    punctuation_counts = Counter(token.text for token in doc if token.pos_ == 'PUNCT')\n",
    "\n",
    "    # Create a counter of function words\n",
    "    function_word_counts = Counter(token.text for token in doc if token.lower_ in FUNCTION_WORDS)\n",
    "\n",
    "    return dict(pos_counts), dict(punctuation_counts), dict(function_word_counts)\n",
    "\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \"\"\"\n",
    "    This function calculates the Flesch Reading Ease and Flesch-Kincaid Grade Level of a text using the textstat library.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to score.\n",
    "\n",
    "    Returns:\n",
    "    flesch_reading_ease (float): The Flesch Reading Ease score of the text.\n",
    "    flesch_kincaid_grade_level (float): The Flesch-Kincaid Grade Level of the text.\n",
    "\n",
    "    \"\"\"\n",
    "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
    "    flesch_kincaid_grade_level = textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "    return flesch_reading_ease, flesch_kincaid_grade_level\n",
    "\n",
    "\n",
    "def load_and_count(dataset_name, data):\n",
    "    \"\"\"\n",
    "       This function loads the texts from the dataset and calculates the frequency of POS tags, punctuation marks,\n",
    "       and function words.\n",
    "\n",
    "       Args:\n",
    "       dataset_name (str): The name of the dataset.\n",
    "       data (list of tuples): The data from the dataset. Each element of the list is a tuple, where the first element\n",
    "       is the text and the second element is its label.\n",
    "\n",
    "       Returns:\n",
    "       overall_pos_counts (Counter): A Counter object of POS tag frequencies.\n",
    "       overall_punctuation_counts (Counter): A Counter object of punctuation mark frequencies.\n",
    "       overall_function_word_counts (Counter): A Counter object of function word frequencies.\n",
    "    \"\"\"\n",
    "\n",
    "    # CHECKED\n",
    "    # Extract texts\n",
    "    texts, labels = remove_prefix(dataset_name, data)\n",
    "\n",
    "    # Calculate POS tag frequencies for the texts\n",
    "    pos_frequencies, punctuation_frequencies, function_word_frequencies = zip(\n",
    "        *[count_pos_tags_and_special_elements(text) for text in texts])\n",
    "\n",
    "    # Then, sum the dictionaries to get the overall frequencies\n",
    "    overall_pos_counts = Counter()\n",
    "    for pos_freq in pos_frequencies:\n",
    "        overall_pos_counts += Counter(pos_freq)\n",
    "\n",
    "    overall_punctuation_counts = Counter()\n",
    "    for punct_freq in punctuation_frequencies:\n",
    "        overall_punctuation_counts += Counter(punct_freq)\n",
    "\n",
    "    overall_function_word_counts = Counter()\n",
    "    for function_word_freq in function_word_frequencies:\n",
    "        overall_function_word_counts += Counter(function_word_freq)\n",
    "\n",
    "    return overall_pos_counts, overall_punctuation_counts, overall_function_word_counts\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    # CHECKED\n",
    "    \"\"\"\n",
    "      This function loads a pre-trained model and its corresponding tokenizer from the Hugging Face model hub.\n",
    "\n",
    "      Returns:\n",
    "      model: The loaded model.\n",
    "      tokenizer: The tokenizer corresponding to the model.\n",
    "\n",
    "    \"\"\"\n",
    "    # model_name = 'allenai/scibert_scivocab_uncased'\n",
    "    # model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    model_name = 'roberta-base'\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "    model = RobertaForMaskedLM.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def calculate_average_word_length(texts):\n",
    "    \"\"\"\n",
    "     This function calculates the average word length of a list of texts using the SpaCy library.\n",
    "\n",
    "     Args:\n",
    "     texts (list): The list of texts.\n",
    "\n",
    "     Returns:\n",
    "     (float): The average word length.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    word_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:  # ignore punctuation\n",
    "                word_lengths.append(len(token.text))\n",
    "\n",
    "    return mean(word_lengths)\n",
    "\n",
    "\n",
    "def calculate_average_sentence_length(texts):\n",
    "    \"\"\"\n",
    "    This function calculates the average sentence length of a list of texts using the SpaCy library.\n",
    "\n",
    "    Args:\n",
    "    texts (list): The list of texts.\n",
    "\n",
    "    Returns:\n",
    "    avg_sentence_length (float): The average sentence length.\n",
    "    \"\"\"\n",
    "    sentence_lengths = []\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentence_lengths.append(len(sent))\n",
    "\n",
    "    return mean(sentence_lengths)\n",
    "\n",
    "\n",
    "def calculate_perplexity(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of a text using a language model and tokenizer.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which perplexity will be calculated.\n",
    "    model: The language model used to calculate perplexity.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    perplexity (float or None): The calculated perplexity of the text, or None if the text is too long.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "        # Truncate the text to the first 512 tokens\n",
    "        # this step has the extra effect of removing examples with low-quality/garbage content (DetectGPT)\n",
    "        input_ids = input_ids[:, :512]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            perplexity = torch.exp(loss)\n",
    "        return perplexity.item()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in calculate_perplexity: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarity between two texts.\n",
    "\n",
    "    Args:\n",
    "    text1 (str): The first text.\n",
    "    text2 (str): The second text.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarity (float): The cosine similarity between the word embeddings of the two texts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the texts\n",
    "    input_ids1 = tokenizer.encode(text1, return_tensors=\"pt\")\n",
    "    input_ids2 = tokenizer.encode(text2, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate word embeddings for the texts\n",
    "    embeddings1 = model.roberta(input_ids1)[0].mean(dim=1).squeeze().detach()\n",
    "    embeddings2 = model.roberta(input_ids2)[0].mean(dim=1).squeeze().detach()\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    embeddings1_np = embeddings1.numpy()\n",
    "    embeddings2_np = embeddings2.numpy()\n",
    "\n",
    "    # Apply L2 normalization to the embeddings\n",
    "    normalized_embeddings1 = normalize(embeddings1_np.reshape(1, -1)).squeeze()\n",
    "    normalized_embeddings2 = normalize(embeddings2_np.reshape(1, -1)).squeeze()\n",
    "\n",
    "    # Convert back to torch tensors\n",
    "    normalized_embeddings1 = torch.from_numpy(normalized_embeddings1)\n",
    "    normalized_embeddings2 = torch.from_numpy(normalized_embeddings2)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarity = 1 - cosine(normalized_embeddings1.numpy(), normalized_embeddings2.numpy())\n",
    "\n",
    "    return cosine_similarity\n",
    "\n",
    "\n",
    "def extract_prompts_and_texts(data):\n",
    "    \"\"\"\n",
    "    This function extracts prompts and texts from the data.\n",
    "\n",
    "    Args:\n",
    "    data (list of tuples): The data. Each tuple consists of a text (including prompt) and a label.\n",
    "\n",
    "    Returns:\n",
    "    prompts_and_texts (list of tuples): The list of tuples where each tuple contains a prompt and a text.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = []\n",
    "\n",
    "    full_texts, _ = zip(*data)\n",
    "    texts, labels = remove_prefix(data)\n",
    "\n",
    "    starting_points = [\"Question:\", \"Prompt:\", \"Summary:\"]\n",
    "    end_points = [\"Answer:\", \"Story:\", \"Article:\"]\n",
    "\n",
    "    for full_text, text in zip(full_texts, texts):\n",
    "        full_text = full_text.strip()  # Remove leading/trailing white spaces\n",
    "        text = text.strip()\n",
    "        prompt = None\n",
    "        for start, end in zip(starting_points, end_points):\n",
    "            start = start.strip()\n",
    "            end = end.strip()\n",
    "            if start in full_text and end in full_text:\n",
    "                _, temp_prompt = full_text.split(start, 1)\n",
    "                if end in temp_prompt: # Check if end is present in temp_prompt before splitting\n",
    "                    prompt, _ = temp_prompt.split(end, 1)\n",
    "                    prompt = prompt.strip()\n",
    "                else:\n",
    "                    print(f\"WARNING: Unable to find the end string '{end}' in temp_prompt for full text: {full_text} and text: {text}\")\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            print(f\"WARNING: No prompt extracted for full text: {full_text} and text: {text}\")\n",
    "            prompt = \"\"  # use an empty string if no prompt is found\n",
    "\n",
    "        prompts_and_texts.append((prompt, text))  # append the prompt and text to the list\n",
    "\n",
    "    return prompts_and_texts\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_dataset(model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all (prompt, text) pairs in a dataset.\n",
    "\n",
    "    Args:\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for prompt, text in prompts_and_texts:\n",
    "        cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def calculate_cosine_similarities_for_sentences_in_text(text, model, tokenizer):\n",
    "    \"\"\"\n",
    "    This function calculates cosine similarities for all consecutive pairs of sentences in a single text.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text for which to calculate cosine similarities.\n",
    "    model: The language model used to generate word embeddings.\n",
    "    tokenizer: The tokenizer used to tokenize the text.\n",
    "\n",
    "    Returns:\n",
    "    cosine_similarities (list of floats): The list of cosine similarities.\n",
    "    \"\"\"\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    cosine_similarities = []\n",
    "\n",
    "    for i in range(len(sentences) - 1):\n",
    "        cosine_similarity = calculate_cosine_similarity(sentences[i], sentences[i + 1], model, tokenizer)\n",
    "        cosine_similarities.append(cosine_similarity)\n",
    "\n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f8ffe",
   "metadata": {},
   "source": [
    "## Now a function to create a data-matrix which has columns with each feature and each row is an observation from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd28f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "\n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features = {\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                    'label': label\n",
    "                }\n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4de5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row 1\n",
      "Processed row 2\n",
      "WARNING: No sentence cosine similarities calculated for text: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n",
      "Processed row 3\n",
      "Processed row 4\n",
      "Processed row 5\n",
      "WARNING: No sentence cosine similarities calculated for text: DBE appears to be equally safe and effective when performed in the community setting as compared to a tertiary referral center with a comparable yield, efficacy, and complication rate.\n",
      "Processed row 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprepare_data_for_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_data/gpt-3.5-turbo_and_human_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m, in \u001b[0;36mprepare_data_for_regression\u001b[1;34m(data_file, save_file, chunk_size)\u001b[0m\n\u001b[0;32m     63\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 65\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [calculate_perplexity(sentence\u001b[38;5;241m.\u001b[39mtext, model, tokenizer) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     66\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     67\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     68\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     69\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 65\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     64\u001b[0m text_perplexity \u001b[38;5;241m=\u001b[39m calculate_perplexity(text, model, tokenizer)\n\u001b[1;32m---> 65\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [\u001b[43mcalculate_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m     66\u001b[0m                          nlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     67\u001b[0m sentence_perplexities \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m sentence_perplexities \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m     68\u001b[0m avg_sentence_perplexity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(sentence_perplexities) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[0;32m     69\u001b[0m     sentence_perplexities) \u001b[38;5;28;01mif\u001b[39;00m sentence_perplexities \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 195\u001b[0m, in \u001b[0;36mcalculate_perplexity\u001b[1;34m(text, model, tokenizer)\u001b[0m\n\u001b[0;32m    192\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m input_ids[:, :\u001b[38;5;241m512\u001b[39m]\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 195\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m    197\u001b[0m     perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1100\u001b[0m, in \u001b[0;36mRobertaForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;124;03m    Used to hide legacy arguments that have been deprecated.\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1114\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:453\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    450\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    451\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 453\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    458\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:465\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 465\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:363\u001b[0m, in \u001b[0;36mRobertaIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 363\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepare_data_for_regression(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48e41b",
   "metadata": {},
   "source": [
    "Result is storred in a file called <b> data_matrix_gpt3.5-turbo.csv <b>, which I can use in a ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e6a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf6efff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aabc1dc0",
   "metadata": {},
   "source": [
    "## Getting code for TF-IDF for synonyms of 'summarise' etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3abdaa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import wordnet\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "147584f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_tfidf(text):\n",
    "    \"\"\"\n",
    "    This function pre-processes the text by lowercasing all words, removing punctuation,\n",
    "    removing stop words and lemmatizing the words.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be pre-processed.\n",
    "\n",
    "    Returns:\n",
    "    text (str): The pre-processed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    text = [word for word in word_tokens if word not in stop_words]\n",
    "    \n",
    "    # lemmatize words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "    # join words back into a single string\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb15c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_top_tfidf_words(data_file, n_top_words=10):\n",
    "    \"\"\"\n",
    "    This function reads the input data, focuses only on the texts (responses),\n",
    "    and computes the top n words with highest TF-IDF score for human-labelled text \n",
    "    and top n words with highest TF-IDF score for AI-generated text.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    n_top_words (int): The number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    top_human_words (DataFrame): DataFrame with top n words and their TF-IDF scores for human-labelled text.\n",
    "    top_ai_words (DataFrame): DataFrame with top n words and their TF-IDF scores for AI-generated text.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_file)\n",
    "    data_tuples = [tuple(x) for x in data.values]\n",
    "    _, texts = zip(*extract_prompts_and_texts(data_tuples))\n",
    "    \n",
    "    # preprocess texts\n",
    "    texts = [prepare_for_tfidf(text) for text in texts]\n",
    "\n",
    "    # split data into human and AI generated\n",
    "    human_texts = [text for text, label in zip(texts, data['Label']) if label == 0]\n",
    "    ai_texts = [text for text, label in zip(texts, data['Label']) if label == 1]\n",
    "\n",
    "    # create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # compute TF-IDF for human texts\n",
    "    human_tfidf = vectorizer.fit_transform(human_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    human_words_tfidf = dict(zip(feature_names, human_tfidf.sum(axis=0).tolist()[0]))\n",
    "    top_human_words = pd.DataFrame(human_words_tfidf.items(), columns=['word', 'tfidf']).nlargest(n_top_words, 'tfidf')\n",
    "\n",
    "    # compute TF-IDF for AI texts\n",
    "    ai_tfidf = vectorizer.fit_transform(ai_texts)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    ai_words_tfidf = dict(zip(feature_names, ai_tfidf.sum(axis=0).tolist()[0]))\n",
    "    top_ai_words = pd.DataFrame(ai_words_tfidf.items(), columns=['word', 'tfidf']).nlargest(n_top_words, 'tfidf')\n",
    "\n",
    "    return top_human_words, top_ai_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e782dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b805d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = compute_top_tfidf_words(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88103742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word      tfidf\n",
      "17160     said  30.462541\n",
      "14022      one  13.766436\n",
      "14537  patient  12.940363\n",
      "22021    would  12.438088\n",
      "20090     time  12.075804\n",
      "4993     could  12.044899\n",
      "11786     like  11.611112\n",
      "14667   people  10.598461\n",
      "12481      may  10.120055\n",
      "2218      back  10.019047\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1354d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word      tfidf\n",
      "8582   patient  17.061123\n",
      "11359    study  16.247412\n",
      "8650    people  11.980189\n",
      "6954      life  11.728132\n",
      "11894     time  10.955582\n",
      "8237       one  10.669970\n",
      "2285     child  10.570240\n",
      "12999    world  10.470353\n",
      "3226       day   9.902697\n",
      "8356   outcome   9.505522\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6c30c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_difference_tfidf_words(data_file, n_top_words=10):\n",
    "    \"\"\"\n",
    "    This function reads the input data, focuses only on the texts (responses),\n",
    "    and computes the top n words with the largest average difference in TF-IDF scores\n",
    "    between human-labelled text and AI-generated text.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    n_top_words (int): The number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    diff_words (DataFrame): DataFrame with top n words and their average difference in TF-IDF scores.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_csv(data_file)\n",
    "    data_tuples = [tuple(x) for x in data.values]\n",
    "    _, texts = zip(*extract_prompts_and_texts(data_tuples))\n",
    "    \n",
    "    # preprocess texts\n",
    "    texts = [prepare_for_tfidf(text) for text in texts]\n",
    "\n",
    "    # split data into human and AI generated\n",
    "    human_texts = [text for text, label in zip(texts, data['Label']) if label == 0]\n",
    "    ai_texts = [text for text, label in zip(texts, data['Label']) if label == 1]\n",
    "\n",
    "    # create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # compute TF-IDF for human texts\n",
    "    human_tfidf = vectorizer.fit_transform(human_texts)\n",
    "    feature_names_human = vectorizer.get_feature_names_out()\n",
    "    human_words_tfidf = dict(zip(feature_names_human, human_tfidf.sum(axis=0).tolist()[0]))\n",
    "\n",
    "    # compute TF-IDF for AI texts\n",
    "    ai_tfidf = vectorizer.fit_transform(ai_texts)\n",
    "    feature_names_ai = vectorizer.get_feature_names_out()\n",
    "    ai_words_tfidf = dict(zip(feature_names_ai, ai_tfidf.sum(axis=0).tolist()[0]))\n",
    "\n",
    "    # compute the difference in TF-IDF scores\n",
    "    diff_words_tfidf = {word: human_words_tfidf.get(word, 0) - ai_words_tfidf.get(word, 0) \n",
    "                        for word in set(feature_names_human).union(feature_names_ai)}\n",
    "    \n",
    "    diff_words = pd.DataFrame(diff_words_tfidf.items(), columns=['word', 'tfidf_difference']).nlargest(n_top_words, 'tfidf_difference')\n",
    "\n",
    "    return diff_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f1b4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compute_difference_tfidf_words(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da574115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'like', 'im', 'get', 'told', 'dont', 'say', 'know', 'think', 'look']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07fa3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        word  tfidf_difference\n",
      "4698    said         24.882167\n",
      "16335   like          5.886250\n",
      "23244     im          5.855467\n",
      "9331     get          5.713346\n",
      "396     told          5.692998\n",
      "23125   dont          5.527132\n",
      "20552    say          5.410044\n",
      "4007    know          4.858790\n",
      "2469   think          4.582883\n",
      "10160   look          4.307568\n"
     ]
    }
   ],
   "source": [
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e92704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff_tfidf_words(data_file, n_top_words=10):\n",
    "    \"\"\"\n",
    "    This function reads the input data, computes the top n words with the largest average difference in \n",
    "    TF-IDF scores between human-labelled text and AI-generated text, and plots the results.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    n_top_words (int): The number of top words to return. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # get the top words with the largest difference in tf-idf\n",
    "    diff_words = compute_difference_tfidf_words(data_file, n_top_words)\n",
    "\n",
    "    # plot the results using seaborn\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    palette = sns.color_palette(\"coolwarm\", n_top_words)  # using coolwarm palette, you can use any palette of your liking\n",
    "    sns.barplot(x=diff_words['tfidf_difference'][::-1], y=diff_words['word'][::-1], palette=palette)  # reverse order to have largest bar at top\n",
    "    plt.xlabel('TF-IDF Difference')\n",
    "    plt.ylabel('Words')\n",
    "    plt.title('Top {} Words with Largest Average Difference in TF-IDF Scores between Human and AI-Generated Text'.format(n_top_words))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28f2c9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_diff_tfidf_words(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7e956d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_regression(data_file, save_file='data_matrix.csv', chunk_size=5):\n",
    "    \"\"\"\n",
    "    This function prepares the data for regression analysis by extracting features and labels from the data.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the full_data.csv file.\n",
    "    save_file (str): The path to the file where the processed data will be saved.\n",
    "    chunk_size (int): The number of rows to process at a time.\n",
    "\n",
    "    Returns:\n",
    "    data_matrix (DataFrame): A DataFrame where each row represents a text, each column represents a feature,\n",
    "                            and the last column is the label.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the model name from the data_file\n",
    "    file_name = data_file.split('/')[-1]  # split the input file string at the slash and take the last part (filename)\n",
    "    model_name = file_name.split('_')[0]  # split the filename at the underscore and take the first part (model name)\n",
    "    save_file = f'data_matrix_{model_name}.csv'  # create save_file name based on the model_name\n",
    "\n",
    "    \n",
    "    # Load the model and tokenizer\n",
    "    model, tokenizer = load_model()\n",
    "\n",
    "    # Load saved data if it exists\n",
    "    if os.path.exists(save_file):\n",
    "        saved_data = pd.read_csv(save_file)\n",
    "        processed_rows = len(saved_data)\n",
    "    else:\n",
    "        saved_data = pd.DataFrame()\n",
    "        processed_rows = 0\n",
    "        \n",
    "    # Calculate the top 10 words with the highest difference in TF-IDF scores and the vectorizer\n",
    "    diff_words = compute_difference_tfidf_words(data_file, n_top_words=10)\n",
    "    top_words = diff_words['word'].tolist()\n",
    "    print(top_words)\n",
    "    synonyms = ['conclusion','overall','summarise','summarize','finale','overall','sum','end','summary','conclude']\n",
    "    # Combine top_words and synonyms into one list\n",
    "    all_words =list(set(top_words + synonyms))\n",
    "    \n",
    "    # Create a TF-IDF vectorizer with the top 10 words as vocabulary\n",
    "    vectorizer = TfidfVectorizer(vocabulary=all_words)\n",
    "\n",
    "    total_rows_processed = 0  # total rows processed in this session\n",
    "\n",
    "    for chunk in pd.read_csv(data_file, chunksize=chunk_size):\n",
    "        feature_list = []\n",
    "\n",
    "        # Skip chunks that have already been processed\n",
    "        if total_rows_processed < processed_rows:\n",
    "            total_rows_processed += len(chunk)\n",
    "            continue\n",
    "\n",
    "        data = list(chunk.itertuples(index=False, name=None))\n",
    "        texts, labels = remove_prefix(data)\n",
    "        prompts_and_texts = extract_prompts_and_texts(data)\n",
    "\n",
    "        for i, ((prompt, text), label) in enumerate(zip(prompts_and_texts, labels)):\n",
    "            try:\n",
    "                features = {}  # Initialize the features dictionary here\n",
    "                \n",
    "                \n",
    "                # Count POS tags in the text\n",
    "                pos_counts, punctuation_counts, function_word_counts = count_pos_tags_and_special_elements(text)\n",
    "\n",
    "                # Calculate the Flesch Reading Ease and Flesch-Kincaid Grade Level\n",
    "                flesch_reading_ease, flesch_kincaid_grade_level = calculate_readability_scores(text)\n",
    "\n",
    "                # Calculate the average word length\n",
    "                avg_word_length = calculate_average_word_length([text])\n",
    "\n",
    "                # Calculate the average sentence length\n",
    "                avg_sentence_length = calculate_average_sentence_length([text])\n",
    "                \n",
    "                # Transform the text into TF-IDF scores\n",
    "                tfidf_scores = vectorizer.fit_transform([text]).toarray()\n",
    "                    \n",
    "                # Calculate the perplexity of the text and average sentence perplexity\n",
    "                text_encoded = tokenizer.encode(text, truncation=True, max_length=510)\n",
    "                text = tokenizer.decode(text_encoded)\n",
    "                text = text.replace('<s>', '').replace('</s>', '')\n",
    "                text_perplexity = calculate_perplexity(text, model, tokenizer)\n",
    "                sentence_perplexities = [calculate_perplexity(sentence.text, model, tokenizer) for sentence in\n",
    "                                         nlp(text).sents]\n",
    "                sentence_perplexities = [p for p in sentence_perplexities if p is not None]\n",
    "                avg_sentence_perplexity = sum(sentence_perplexities) / len(\n",
    "                    sentence_perplexities) if sentence_perplexities else None\n",
    "\n",
    "                # Calculate the frequency of uppercase letters\n",
    "                uppercase_freq = sum(1 for char in text if char.isupper()) / len(text)\n",
    "\n",
    "                # Calculate the cosine similarity for the prompt and text\n",
    "                prompt_text_cosine_similarity = calculate_cosine_similarity(prompt, text, model, tokenizer)\n",
    "\n",
    "                # Calculate the average cosine similarity for sentences in the text\n",
    "                sentence_cosine_similarities = calculate_cosine_similarities_for_sentences_in_text(text, model,\n",
    "                                                                                                   tokenizer)\n",
    "                avg_sentence_cosine_similarity = None\n",
    "                if sentence_cosine_similarities:\n",
    "                    avg_sentence_cosine_similarity = sum(sentence_cosine_similarities) / len(\n",
    "                        sentence_cosine_similarities)\n",
    "                else:\n",
    "                    print(\"WARNING: No sentence cosine similarities calculated for text:\", text)\n",
    "\n",
    "                # Prepare a dictionary to append to the feature list\n",
    "                features.update({\n",
    "                    'ADJ': pos_counts.get('ADJ', 0),\n",
    "                    'ADV': pos_counts.get('ADV', 0),\n",
    "                    'CONJ': pos_counts.get('CCONJ', 0),\n",
    "                    'NOUN': pos_counts.get('NOUN', 0),\n",
    "                    'NUM': pos_counts.get('NUM', 0),\n",
    "                    'VERB': pos_counts.get('VERB', 0),\n",
    "                    'COMMA': punctuation_counts.get(',', 0),\n",
    "                    'FULLSTOP': punctuation_counts.get('.', 0),\n",
    "                    'SPECIAL-': punctuation_counts.get('-', 0),\n",
    "                    'FUNCTION-A': function_word_counts.get('a', 0),\n",
    "                    'FUNCTION-IN': function_word_counts.get('in', 0),\n",
    "                    'FUNCTION-OF': function_word_counts.get('of', 0),\n",
    "                    'FUNCTION-THE': function_word_counts.get('the', 0),\n",
    "                    'uppercase_freq': uppercase_freq,\n",
    "                    'flesch_reading_ease': flesch_reading_ease,\n",
    "                    'flesch_kincaid_grade_level': flesch_kincaid_grade_level,\n",
    "                    'avg_word_length': avg_word_length,\n",
    "                    'avg_sentence_length': avg_sentence_length,\n",
    "                    'text_perplexity': text_perplexity,\n",
    "                    'avg_sentence_perplexity': avg_sentence_perplexity,\n",
    "                    'prompt_text_cosine_similarity': prompt_text_cosine_similarity,\n",
    "                    'avg_sentence_cosine_similarity': avg_sentence_cosine_similarity,\n",
    "                })\n",
    "                \n",
    "                # If the TF-IDF scores array is not empty, zip the scores with the words to create a dictionary\n",
    "                # and update the features dictionary with this new dictionary\n",
    "                if tfidf_scores.size > 0:\n",
    "                    word_scores = {f\"tf_idfs_{word}\": score for word, score in zip(all_words, tfidf_scores[0])}\n",
    "                    features.update(word_scores)\n",
    "                else:  # If the TF-IDF scores array is empty, assign 0 to each word's score\n",
    "                    word_scores = {f\"tf_idfs_{word}\": 0 for word in all_words}\n",
    "                    features.update(word_scores)\n",
    "                    \n",
    "                    \n",
    "                features['label'] = label\n",
    "\n",
    "           \n",
    "\n",
    "                # Add the feature dictionary to the feature list\n",
    "                feature_list.append(features)\n",
    "\n",
    "                # Print progress\n",
    "                print(f\"Processed row {total_rows_processed + 1}\")\n",
    "                total_rows_processed += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {total_rows_processed + 1}: {e}\")\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # Convert the list of dictionaries into a DataFrame\n",
    "            new_data = pd.DataFrame(feature_list).fillna(0)\n",
    "\n",
    "            # Append new data to saved data and save\n",
    "            saved_data = pd.concat([saved_data, new_data])\n",
    "            saved_data.to_csv(save_file, index=False)\n",
    "\n",
    "            # Clear the feature list for the next batch\n",
    "            feature_list.clear()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc1e2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'like', 'im', 'get', 'told', 'dont', 'say', 'know', 'think', 'look']\n",
      "Processed row 1\n",
      "Processed row 2\n",
      "WARNING: No sentence cosine similarities calculated for text: \"Aquagenic maladies\" could be a pediatric form of the aquagenic urticaria.\n",
      "Processed row 3\n",
      "Processed row 4\n",
      "Processed row 5\n",
      "WARNING: No sentence cosine similarities calculated for text: DBE appears to be equally safe and effective when performed in the community setting as compared to a tertiary referral center with a comparable yield, efficacy, and complication rate.\n",
      "Processed row 6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprepare_data_for_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextracted_data/gpt-3.5-turbo_and_human_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 92\u001b[0m, in \u001b[0;36mprepare_data_for_regression\u001b[1;34m(data_file, save_file, chunk_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m uppercase_freq \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m char\u001b[38;5;241m.\u001b[39misupper()) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(text)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Calculate the cosine similarity for the prompt and text\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m prompt_text_cosine_similarity \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_cosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Calculate the average cosine similarity for sentences in the text\u001b[39;00m\n\u001b[0;32m     95\u001b[0m sentence_cosine_similarities \u001b[38;5;241m=\u001b[39m calculate_cosine_similarities_for_sentences_in_text(text, model,\n\u001b[0;32m     96\u001b[0m                                                                                    tokenizer)\n",
      "Cell \u001b[1;32mIn[1], line 224\u001b[0m, in \u001b[0;36mcalculate_cosine_similarity\u001b[1;34m(text1, text2, model, tokenizer)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Generate word embeddings for the texts\u001b[39;00m\n\u001b[0;32m    223\u001b[0m embeddings1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mroberta(input_ids1)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m--> 224\u001b[0m embeddings2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids2\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Convert embeddings to numpy arrays\u001b[39;00m\n\u001b[0;32m    227\u001b[0m embeddings1_np \u001b[38;5;241m=\u001b[39m embeddings1\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:852\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    843\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    845\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    846\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    847\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    850\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    851\u001b[0m )\n\u001b[1;32m--> 852\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    864\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    865\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:527\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    518\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    519\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    520\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    525\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:411\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    401\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 411\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:347\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    330\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    338\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    339\u001b[0m         hidden_states,\n\u001b[0;32m    340\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m         output_attentions,\n\u001b[0;32m    346\u001b[0m     )\n\u001b[1;32m--> 347\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mself_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:296\u001b[0m, in \u001b[0;36mRobertaSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 296\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    298\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\MScProject\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prepare_data_for_regression(\"extracted_data/gpt-3.5-turbo_and_human_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc6e63d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d691d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2408cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b22177b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word_list):\n",
    "    # This set will contain all the unique words\n",
    "    all_words = set(word_list)\n",
    "    \n",
    "    for word in word_list:\n",
    "        # Get all the synonyms for each word\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                # Add each synonym to the set of all_words\n",
    "                all_words.add(lemma.name())\n",
    "    \n",
    "    return list(all_words)  # Convert the set back to a list before returning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41065416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['last', 'boilersuit', 'finale', 'termination', 'sum_up', 'determination', 'overall', 'stopping_point', 'close', 'boilers_suit', 'conclusion', 'sum', 'end', 'summarize', 'finis', 'summarise', 'ending', 'finish', 'closing', 'decision', 'resume', 'ratiocination']\n"
     ]
    }
   ],
   "source": [
    "words = ['conclusion', 'overall', 'summarise']\n",
    "synonyms = get_synonyms(words)\n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eca8e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14b4b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_important_tfidf_scores(data_file, vocabulary, save_file='tfidf_scores.csv'):\n",
    "    \"\"\"\n",
    "    This function reads the input data, focuses only on the texts (responses),\n",
    "    and computes the TF-IDF scores for the specified vocabulary in both\n",
    "    human-labelled text and AI-generated text. The results are saved in a csv file.\n",
    "\n",
    "    Args:\n",
    "    data_file (str): The path to the .csv file which contains the texts and labels.\n",
    "    vocabulary (list): The list of words to compute the TF-IDF scores for.\n",
    "    save_file (str): The path to the csv file where the results will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_file)\n",
    "    data_tuples = [tuple(x) for x in data.values]\n",
    "    _, texts = zip(*extract_prompts_and_texts(data_tuples))\n",
    "    \n",
    "    # preprocess texts\n",
    "    texts = [prepare_for_tfidf(text) for text in texts]\n",
    "\n",
    "    # split data into human and AI generated\n",
    "    human_texts = [text for text, label in zip(texts, data['Label']) if label == 0]\n",
    "    ai_texts = [text for text, label in zip(texts, data['Label']) if label == 1]\n",
    "\n",
    "    # create TF-IDF vectorizer with specific vocabulary\n",
    "    vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
    "\n",
    "    # initialize DataFrame with zeros\n",
    "    tfidf_scores = pd.DataFrame(columns=['tf_idfs_' + word for word in vocabulary])\n",
    "    \n",
    "    # compute TF-IDF for human and AI texts\n",
    "    for text_list in [human_texts, ai_texts]:\n",
    "        tfidf_matrix = vectorizer.fit_transform(text_list).toarray()\n",
    "        tfidf_scores_temp = pd.DataFrame(tfidf_matrix, columns=['tf_idfs_' + word for word in vocabulary])\n",
    "        tfidf_scores = pd.concat([tfidf_scores, tfidf_scores_temp], axis=0)\n",
    "    \n",
    "    # save the DataFrame to a csv file\n",
    "    tfidf_scores.to_csv(save_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "608b43a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['said', 'like', 'im', 'get', 'told', 'dont', 'say', 'know', 'think', 'look', 'conclusion','summarise','summarize','finale','overall','sum','end','summary','conclude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3989b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_important_tfidf_scores('extracted_data/gpt-3.5-turbo_and_human_data.csv',vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dca778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
